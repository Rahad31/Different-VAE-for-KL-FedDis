# ğŸš€ Enhancing KL-FedDis with Variational Autoencoder Variants

This repository contains the code, experiments, and results from our thesis project:  
**"Enhancing KL-FedDis with Variational Autoencoder Variants for Robust Federated Learning"**.

The work extends the [KL-FedDis](https://doi.org/10.1016/j.neuri.2024.100182) method by evaluating how different Variational Autoencoder (VAE) architectures affect federated learning performance under Non-IID settings.

---

## ğŸ§  Project Overview

KL-FedDis is a federated learning approach that shares distribution information using **KL divergence** to handle **non-IID data** more effectively.

This project introduces and compares multiple VAE variants used for latent distribution modeling, including:

- âœ… Standard VAE  
- ğŸ”„ Beta-VAE  
- ğŸ”£ Conditional VAE (CVAE)  
- ğŸ§± Vector Quantized VAE (VQ-VAE)  
- ğŸ” FactorVAE *(optional, in progress)*

---



