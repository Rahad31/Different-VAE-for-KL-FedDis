# 🚀 Enhancing KL-FedDis with Variational Autoencoder Variants

This repository contains the code, experiments, and results from our thesis project:  
**"Enhancing KL-FedDis with Variational Autoencoder Variants for Robust Federated Learning"**.

The work extends the [KL-FedDis](https://doi.org/10.1016/j.neuri.2024.100182) method by evaluating how different Variational Autoencoder (VAE) architectures affect federated learning performance under Non-IID settings.

---

## 🧠 Project Overview

KL-FedDis is a federated learning approach that shares distribution information using **KL divergence** to handle **non-IID data** more effectively.

This project introduces and compares multiple VAE variants used for latent distribution modeling, including:

- ✅ Standard VAE  
- 🔄 Beta-VAE  
- 🔣 Conditional VAE (CVAE)  
- 🧱 Vector Quantized VAE (VQ-VAE)  
- 🔍 FactorVAE *(optional, in progress)*

---



