{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahad31/Different-VAE-for-KL-FedDis/blob/main/Beta_Uniform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-JKdQmmHeQ0"
      },
      "outputs": [],
      "source": [
        "Beta=.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUv6TBaAHdg2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwAgjpcnHZmc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpKCzIbdm4gQ",
        "outputId": "be62ed61-978b-403d-fac2-0b2505ac0f4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 62.6MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Images per Class: [6133 6011 5952 5876 6017 6045 5885 5899 6057 6125]\n",
            "Epoch [1/10], Training Loss: 2.304, Validation Accuracy: 10.60%\n",
            "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 10.85%\n",
            "Epoch [3/10], Training Loss: 2.302, Validation Accuracy: 11.19%\n",
            "Epoch [4/10], Training Loss: 2.301, Validation Accuracy: 11.61%\n",
            "Epoch [5/10], Training Loss: 2.299, Validation Accuracy: 12.33%\n",
            "Epoch [6/10], Training Loss: 2.298, Validation Accuracy: 12.64%\n",
            "Epoch [7/10], Training Loss: 2.296, Validation Accuracy: 12.40%\n",
            "Epoch [8/10], Training Loss: 2.294, Validation Accuracy: 11.93%\n",
            "Epoch [9/10], Training Loss: 2.291, Validation Accuracy: 12.03%\n",
            "Epoch [10/10], Training Loss: 2.288, Validation Accuracy: 12.06%\n",
            "Epoch [1/10], Training Loss: 2.285, Validation Accuracy: 14.94%\n",
            "Epoch [2/10], Training Loss: 2.278, Validation Accuracy: 15.58%\n",
            "Epoch [3/10], Training Loss: 2.270, Validation Accuracy: 15.01%\n",
            "Epoch [4/10], Training Loss: 2.260, Validation Accuracy: 15.75%\n",
            "Epoch [5/10], Training Loss: 2.247, Validation Accuracy: 16.49%\n",
            "Epoch [6/10], Training Loss: 2.230, Validation Accuracy: 17.92%\n",
            "Epoch [7/10], Training Loss: 2.208, Validation Accuracy: 19.75%\n",
            "Epoch [8/10], Training Loss: 2.180, Validation Accuracy: 20.91%\n",
            "Epoch [9/10], Training Loss: 2.147, Validation Accuracy: 21.34%\n",
            "Epoch [10/10], Training Loss: 2.116, Validation Accuracy: 22.32%\n",
            "Epoch [1/10], Training Loss: 2.091, Validation Accuracy: 23.84%\n",
            "Epoch [2/10], Training Loss: 2.067, Validation Accuracy: 24.82%\n",
            "Epoch [3/10], Training Loss: 2.043, Validation Accuracy: 26.79%\n",
            "Epoch [4/10], Training Loss: 2.020, Validation Accuracy: 26.92%\n",
            "Epoch [5/10], Training Loss: 2.001, Validation Accuracy: 27.37%\n",
            "Epoch [6/10], Training Loss: 1.984, Validation Accuracy: 28.06%\n",
            "Epoch [7/10], Training Loss: 1.967, Validation Accuracy: 28.93%\n",
            "Epoch [8/10], Training Loss: 1.955, Validation Accuracy: 28.58%\n",
            "Epoch [9/10], Training Loss: 1.943, Validation Accuracy: 29.37%\n",
            "Epoch [10/10], Training Loss: 1.928, Validation Accuracy: 30.33%\n",
            "Epoch [1/10], Training Loss: 1.914, Validation Accuracy: 30.60%\n",
            "Epoch [2/10], Training Loss: 1.902, Validation Accuracy: 31.03%\n",
            "Epoch [3/10], Training Loss: 1.889, Validation Accuracy: 30.90%\n",
            "Epoch [4/10], Training Loss: 1.874, Validation Accuracy: 31.85%\n",
            "Epoch [5/10], Training Loss: 1.866, Validation Accuracy: 32.74%\n",
            "Epoch [6/10], Training Loss: 1.844, Validation Accuracy: 32.79%\n",
            "Epoch [7/10], Training Loss: 1.832, Validation Accuracy: 33.43%\n",
            "Epoch [8/10], Training Loss: 1.815, Validation Accuracy: 33.82%\n",
            "Epoch [9/10], Training Loss: 1.803, Validation Accuracy: 34.69%\n",
            "Epoch [10/10], Training Loss: 1.784, Validation Accuracy: 34.81%\n",
            "Epoch [1/10], Training Loss: 1.801, Validation Accuracy: 35.89%\n",
            "Epoch [2/10], Training Loss: 1.782, Validation Accuracy: 36.39%\n",
            "Epoch [3/10], Training Loss: 1.764, Validation Accuracy: 37.16%\n",
            "Epoch [4/10], Training Loss: 1.753, Validation Accuracy: 37.27%\n",
            "Epoch [5/10], Training Loss: 1.733, Validation Accuracy: 37.93%\n",
            "Epoch [6/10], Training Loss: 1.722, Validation Accuracy: 37.63%\n",
            "Epoch [7/10], Training Loss: 1.708, Validation Accuracy: 38.47%\n",
            "Epoch [8/10], Training Loss: 1.696, Validation Accuracy: 38.79%\n",
            "Epoch [9/10], Training Loss: 1.684, Validation Accuracy: 39.09%\n",
            "Epoch [10/10], Training Loss: 1.673, Validation Accuracy: 39.52%\n",
            "Epoch [1/10], Training Loss: 1.649, Validation Accuracy: 40.06%\n",
            "Epoch [2/10], Training Loss: 1.640, Validation Accuracy: 39.65%\n",
            "Epoch [3/10], Training Loss: 1.631, Validation Accuracy: 40.89%\n",
            "Epoch [4/10], Training Loss: 1.606, Validation Accuracy: 40.97%\n",
            "Epoch [5/10], Training Loss: 1.596, Validation Accuracy: 41.38%\n",
            "Epoch [6/10], Training Loss: 1.588, Validation Accuracy: 41.09%\n",
            "Epoch [7/10], Training Loss: 1.579, Validation Accuracy: 41.46%\n",
            "Epoch [8/10], Training Loss: 1.564, Validation Accuracy: 42.03%\n",
            "Epoch [9/10], Training Loss: 1.551, Validation Accuracy: 42.39%\n",
            "Epoch [10/10], Training Loss: 1.542, Validation Accuracy: 42.60%\n",
            "Epoch [1/10], Training Loss: 1.591, Validation Accuracy: 42.68%\n",
            "Epoch [2/10], Training Loss: 1.577, Validation Accuracy: 43.17%\n",
            "Epoch [3/10], Training Loss: 1.566, Validation Accuracy: 43.84%\n",
            "Epoch [4/10], Training Loss: 1.556, Validation Accuracy: 42.81%\n",
            "Epoch [5/10], Training Loss: 1.545, Validation Accuracy: 43.83%\n",
            "Epoch [6/10], Training Loss: 1.538, Validation Accuracy: 43.93%\n",
            "Epoch [7/10], Training Loss: 1.530, Validation Accuracy: 43.98%\n",
            "Epoch [8/10], Training Loss: 1.529, Validation Accuracy: 44.39%\n",
            "Epoch [9/10], Training Loss: 1.514, Validation Accuracy: 43.98%\n",
            "Epoch [10/10], Training Loss: 1.506, Validation Accuracy: 44.14%\n",
            "Epoch [1/10], Training Loss: 1.546, Validation Accuracy: 44.49%\n",
            "Epoch [2/10], Training Loss: 1.527, Validation Accuracy: 45.15%\n",
            "Epoch [3/10], Training Loss: 1.517, Validation Accuracy: 44.46%\n",
            "Epoch [4/10], Training Loss: 1.523, Validation Accuracy: 45.33%\n",
            "Epoch [5/10], Training Loss: 1.506, Validation Accuracy: 44.92%\n",
            "Epoch [6/10], Training Loss: 1.495, Validation Accuracy: 44.46%\n",
            "Epoch [7/10], Training Loss: 1.491, Validation Accuracy: 45.98%\n",
            "Epoch [8/10], Training Loss: 1.480, Validation Accuracy: 46.08%\n",
            "Epoch [9/10], Training Loss: 1.475, Validation Accuracy: 46.95%\n",
            "Epoch [10/10], Training Loss: 1.468, Validation Accuracy: 46.33%\n",
            "Epoch [1/10], Training Loss: 1.477, Validation Accuracy: 46.19%\n",
            "Epoch [2/10], Training Loss: 1.462, Validation Accuracy: 46.46%\n",
            "Epoch [3/10], Training Loss: 1.449, Validation Accuracy: 46.69%\n",
            "Epoch [4/10], Training Loss: 1.447, Validation Accuracy: 46.52%\n",
            "Epoch [5/10], Training Loss: 1.429, Validation Accuracy: 47.13%\n",
            "Epoch [6/10], Training Loss: 1.429, Validation Accuracy: 47.20%\n",
            "Epoch [7/10], Training Loss: 1.411, Validation Accuracy: 46.43%\n",
            "Epoch [8/10], Training Loss: 1.405, Validation Accuracy: 48.05%\n",
            "Epoch [9/10], Training Loss: 1.394, Validation Accuracy: 47.76%\n",
            "Epoch [10/10], Training Loss: 1.392, Validation Accuracy: 47.58%\n",
            "Epoch [1/10], Training Loss: 1.451, Validation Accuracy: 47.75%\n",
            "Epoch [2/10], Training Loss: 1.431, Validation Accuracy: 48.09%\n",
            "Epoch [3/10], Training Loss: 1.420, Validation Accuracy: 48.63%\n",
            "Epoch [4/10], Training Loss: 1.413, Validation Accuracy: 49.39%\n",
            "Epoch [5/10], Training Loss: 1.400, Validation Accuracy: 48.90%\n",
            "Epoch [6/10], Training Loss: 1.387, Validation Accuracy: 49.22%\n",
            "Epoch [7/10], Training Loss: 1.387, Validation Accuracy: 49.17%\n",
            "Epoch [8/10], Training Loss: 1.368, Validation Accuracy: 49.35%\n",
            "Epoch [9/10], Training Loss: 1.365, Validation Accuracy: 49.17%\n",
            "Epoch [10/10], Training Loss: 1.353, Validation Accuracy: 50.00%\n",
            "Epoch [1/10], Training Loss: 1.369, Validation Accuracy: 48.99%\n",
            "Epoch [2/10], Training Loss: 1.351, Validation Accuracy: 49.81%\n",
            "Epoch [3/10], Training Loss: 1.343, Validation Accuracy: 49.44%\n",
            "Epoch [4/10], Training Loss: 1.332, Validation Accuracy: 50.15%\n",
            "Epoch [5/10], Training Loss: 1.317, Validation Accuracy: 50.21%\n",
            "Epoch [6/10], Training Loss: 1.312, Validation Accuracy: 50.31%\n",
            "Epoch [7/10], Training Loss: 1.306, Validation Accuracy: 48.95%\n",
            "Epoch [8/10], Training Loss: 1.302, Validation Accuracy: 49.56%\n",
            "Epoch [9/10], Training Loss: 1.290, Validation Accuracy: 50.48%\n",
            "Epoch [10/10], Training Loss: 1.286, Validation Accuracy: 51.09%\n",
            "Epoch [1/10], Training Loss: 1.366, Validation Accuracy: 51.16%\n",
            "Epoch [2/10], Training Loss: 1.348, Validation Accuracy: 51.69%\n",
            "Epoch [3/10], Training Loss: 1.336, Validation Accuracy: 50.67%\n",
            "Epoch [4/10], Training Loss: 1.320, Validation Accuracy: 51.02%\n",
            "Epoch [5/10], Training Loss: 1.315, Validation Accuracy: 51.25%\n",
            "Epoch [6/10], Training Loss: 1.307, Validation Accuracy: 51.65%\n",
            "Epoch [7/10], Training Loss: 1.303, Validation Accuracy: 51.15%\n",
            "Epoch [8/10], Training Loss: 1.289, Validation Accuracy: 51.92%\n",
            "Epoch [9/10], Training Loss: 1.288, Validation Accuracy: 50.90%\n",
            "Epoch [10/10], Training Loss: 1.275, Validation Accuracy: 50.86%\n",
            "Epoch [1/10], Training Loss: 1.344, Validation Accuracy: 52.13%\n",
            "Epoch [2/10], Training Loss: 1.323, Validation Accuracy: 51.14%\n",
            "Epoch [3/10], Training Loss: 1.316, Validation Accuracy: 51.72%\n",
            "Epoch [4/10], Training Loss: 1.300, Validation Accuracy: 51.83%\n",
            "Epoch [5/10], Training Loss: 1.298, Validation Accuracy: 52.24%\n",
            "Epoch [6/10], Training Loss: 1.281, Validation Accuracy: 52.14%\n",
            "Epoch [7/10], Training Loss: 1.269, Validation Accuracy: 52.99%\n",
            "Epoch [8/10], Training Loss: 1.261, Validation Accuracy: 53.19%\n",
            "Epoch [9/10], Training Loss: 1.255, Validation Accuracy: 52.50%\n",
            "Epoch [10/10], Training Loss: 1.245, Validation Accuracy: 53.12%\n",
            "Epoch [1/10], Training Loss: 1.287, Validation Accuracy: 53.46%\n",
            "Epoch [2/10], Training Loss: 1.265, Validation Accuracy: 52.67%\n",
            "Epoch [3/10], Training Loss: 1.241, Validation Accuracy: 53.18%\n",
            "Epoch [4/10], Training Loss: 1.229, Validation Accuracy: 52.92%\n",
            "Epoch [5/10], Training Loss: 1.218, Validation Accuracy: 53.76%\n",
            "Epoch [6/10], Training Loss: 1.213, Validation Accuracy: 53.10%\n",
            "Epoch [7/10], Training Loss: 1.202, Validation Accuracy: 53.13%\n",
            "Epoch [8/10], Training Loss: 1.183, Validation Accuracy: 53.59%\n",
            "Epoch [9/10], Training Loss: 1.175, Validation Accuracy: 53.31%\n",
            "Epoch [10/10], Training Loss: 1.165, Validation Accuracy: 53.91%\n",
            "Epoch [1/10], Training Loss: 1.269, Validation Accuracy: 53.90%\n",
            "Epoch [2/10], Training Loss: 1.244, Validation Accuracy: 54.45%\n",
            "Epoch [3/10], Training Loss: 1.223, Validation Accuracy: 54.30%\n",
            "Epoch [4/10], Training Loss: 1.221, Validation Accuracy: 54.61%\n",
            "Epoch [5/10], Training Loss: 1.206, Validation Accuracy: 54.05%\n",
            "Epoch [6/10], Training Loss: 1.198, Validation Accuracy: 54.73%\n",
            "Epoch [7/10], Training Loss: 1.183, Validation Accuracy: 54.80%\n",
            "Epoch [8/10], Training Loss: 1.177, Validation Accuracy: 54.27%\n",
            "Epoch [9/10], Training Loss: 1.168, Validation Accuracy: 54.67%\n",
            "Epoch [10/10], Training Loss: 1.157, Validation Accuracy: 54.59%\n",
            "Epoch [1/10], Training Loss: 1.218, Validation Accuracy: 53.89%\n",
            "Epoch [2/10], Training Loss: 1.190, Validation Accuracy: 55.28%\n",
            "Epoch [3/10], Training Loss: 1.165, Validation Accuracy: 55.59%\n",
            "Epoch [4/10], Training Loss: 1.148, Validation Accuracy: 54.31%\n",
            "Epoch [5/10], Training Loss: 1.148, Validation Accuracy: 54.81%\n",
            "Epoch [6/10], Training Loss: 1.130, Validation Accuracy: 55.32%\n",
            "Epoch [7/10], Training Loss: 1.120, Validation Accuracy: 55.48%\n",
            "Epoch [8/10], Training Loss: 1.117, Validation Accuracy: 55.24%\n",
            "Epoch [9/10], Training Loss: 1.105, Validation Accuracy: 55.36%\n",
            "Epoch [10/10], Training Loss: 1.095, Validation Accuracy: 55.25%\n",
            "Epoch [1/10], Training Loss: 1.221, Validation Accuracy: 55.61%\n",
            "Epoch [2/10], Training Loss: 1.191, Validation Accuracy: 55.85%\n",
            "Epoch [3/10], Training Loss: 1.164, Validation Accuracy: 56.30%\n",
            "Epoch [4/10], Training Loss: 1.158, Validation Accuracy: 55.77%\n",
            "Epoch [5/10], Training Loss: 1.141, Validation Accuracy: 56.06%\n",
            "Epoch [6/10], Training Loss: 1.126, Validation Accuracy: 56.47%\n",
            "Epoch [7/10], Training Loss: 1.113, Validation Accuracy: 56.53%\n",
            "Epoch [8/10], Training Loss: 1.107, Validation Accuracy: 56.38%\n",
            "Epoch [9/10], Training Loss: 1.101, Validation Accuracy: 55.98%\n",
            "Epoch [10/10], Training Loss: 1.086, Validation Accuracy: 56.18%\n",
            "Epoch [1/10], Training Loss: 1.204, Validation Accuracy: 56.10%\n",
            "Epoch [2/10], Training Loss: 1.173, Validation Accuracy: 56.34%\n",
            "Epoch [3/10], Training Loss: 1.157, Validation Accuracy: 56.37%\n",
            "Epoch [4/10], Training Loss: 1.143, Validation Accuracy: 55.74%\n",
            "Epoch [5/10], Training Loss: 1.124, Validation Accuracy: 56.28%\n",
            "Epoch [6/10], Training Loss: 1.122, Validation Accuracy: 55.68%\n",
            "Epoch [7/10], Training Loss: 1.099, Validation Accuracy: 56.54%\n",
            "Epoch [8/10], Training Loss: 1.101, Validation Accuracy: 55.90%\n",
            "Epoch [9/10], Training Loss: 1.087, Validation Accuracy: 56.37%\n",
            "Epoch [10/10], Training Loss: 1.081, Validation Accuracy: 56.64%\n",
            "Epoch [1/10], Training Loss: 1.140, Validation Accuracy: 56.43%\n",
            "Epoch [2/10], Training Loss: 1.111, Validation Accuracy: 56.33%\n",
            "Epoch [3/10], Training Loss: 1.091, Validation Accuracy: 57.46%\n",
            "Epoch [4/10], Training Loss: 1.075, Validation Accuracy: 56.91%\n",
            "Epoch [5/10], Training Loss: 1.055, Validation Accuracy: 57.18%\n",
            "Epoch [6/10], Training Loss: 1.044, Validation Accuracy: 56.04%\n",
            "Epoch [7/10], Training Loss: 1.037, Validation Accuracy: 55.85%\n",
            "Epoch [8/10], Training Loss: 1.030, Validation Accuracy: 56.94%\n",
            "Epoch [9/10], Training Loss: 1.014, Validation Accuracy: 56.40%\n",
            "Epoch [10/10], Training Loss: 0.999, Validation Accuracy: 57.57%\n",
            "Epoch [1/10], Training Loss: 1.146, Validation Accuracy: 56.29%\n",
            "Epoch [2/10], Training Loss: 1.126, Validation Accuracy: 58.31%\n",
            "Epoch [3/10], Training Loss: 1.109, Validation Accuracy: 57.30%\n",
            "Epoch [4/10], Training Loss: 1.087, Validation Accuracy: 57.14%\n",
            "Epoch [5/10], Training Loss: 1.069, Validation Accuracy: 57.97%\n",
            "Epoch [6/10], Training Loss: 1.056, Validation Accuracy: 58.31%\n",
            "Epoch [7/10], Training Loss: 1.048, Validation Accuracy: 56.85%\n",
            "Epoch [8/10], Training Loss: 1.035, Validation Accuracy: 58.20%\n",
            "Epoch [9/10], Training Loss: 1.018, Validation Accuracy: 57.64%\n",
            "Epoch [10/10], Training Loss: 1.014, Validation Accuracy: 56.96%\n",
            "Epoch [1/10], Training Loss: 1.095, Validation Accuracy: 58.33%\n",
            "Epoch [2/10], Training Loss: 1.061, Validation Accuracy: 58.22%\n",
            "Epoch [3/10], Training Loss: 1.040, Validation Accuracy: 58.36%\n",
            "Epoch [4/10], Training Loss: 1.025, Validation Accuracy: 56.52%\n",
            "Epoch [5/10], Training Loss: 1.012, Validation Accuracy: 58.36%\n",
            "Epoch [6/10], Training Loss: 0.991, Validation Accuracy: 58.32%\n",
            "Epoch [7/10], Training Loss: 0.989, Validation Accuracy: 58.53%\n",
            "Epoch [8/10], Training Loss: 0.971, Validation Accuracy: 58.46%\n",
            "Epoch [9/10], Training Loss: 0.957, Validation Accuracy: 58.19%\n",
            "Epoch [10/10], Training Loss: 0.949, Validation Accuracy: 58.23%\n",
            "Epoch [1/10], Training Loss: 1.114, Validation Accuracy: 58.16%\n",
            "Epoch [2/10], Training Loss: 1.074, Validation Accuracy: 58.50%\n",
            "Epoch [3/10], Training Loss: 1.050, Validation Accuracy: 58.66%\n",
            "Epoch [4/10], Training Loss: 1.034, Validation Accuracy: 58.19%\n",
            "Epoch [5/10], Training Loss: 1.014, Validation Accuracy: 58.51%\n",
            "Epoch [6/10], Training Loss: 1.001, Validation Accuracy: 58.53%\n",
            "Epoch [7/10], Training Loss: 0.981, Validation Accuracy: 57.48%\n",
            "Epoch [8/10], Training Loss: 0.967, Validation Accuracy: 58.04%\n",
            "Epoch [9/10], Training Loss: 0.958, Validation Accuracy: 58.41%\n",
            "Epoch [10/10], Training Loss: 0.953, Validation Accuracy: 58.20%\n",
            "Epoch [1/10], Training Loss: 1.116, Validation Accuracy: 58.72%\n",
            "Epoch [2/10], Training Loss: 1.080, Validation Accuracy: 58.09%\n",
            "Epoch [3/10], Training Loss: 1.047, Validation Accuracy: 58.90%\n",
            "Epoch [4/10], Training Loss: 1.029, Validation Accuracy: 58.23%\n",
            "Epoch [5/10], Training Loss: 1.010, Validation Accuracy: 58.73%\n",
            "Epoch [6/10], Training Loss: 0.997, Validation Accuracy: 59.45%\n",
            "Epoch [7/10], Training Loss: 0.989, Validation Accuracy: 59.55%\n",
            "Epoch [8/10], Training Loss: 0.973, Validation Accuracy: 59.03%\n",
            "Epoch [9/10], Training Loss: 0.967, Validation Accuracy: 59.19%\n",
            "Epoch [10/10], Training Loss: 0.946, Validation Accuracy: 58.92%\n",
            "Epoch [1/10], Training Loss: 1.047, Validation Accuracy: 58.15%\n",
            "Epoch [2/10], Training Loss: 1.009, Validation Accuracy: 58.63%\n",
            "Epoch [3/10], Training Loss: 0.990, Validation Accuracy: 59.10%\n",
            "Epoch [4/10], Training Loss: 0.959, Validation Accuracy: 59.35%\n",
            "Epoch [5/10], Training Loss: 0.936, Validation Accuracy: 58.53%\n",
            "Epoch [6/10], Training Loss: 0.933, Validation Accuracy: 59.53%\n",
            "Epoch [7/10], Training Loss: 0.917, Validation Accuracy: 58.98%\n",
            "Epoch [8/10], Training Loss: 0.906, Validation Accuracy: 58.45%\n",
            "Epoch [9/10], Training Loss: 0.890, Validation Accuracy: 59.35%\n",
            "Epoch [10/10], Training Loss: 0.870, Validation Accuracy: 57.26%\n",
            "Epoch [1/10], Training Loss: 1.077, Validation Accuracy: 59.09%\n",
            "Epoch [2/10], Training Loss: 1.029, Validation Accuracy: 58.90%\n",
            "Epoch [3/10], Training Loss: 0.996, Validation Accuracy: 60.01%\n",
            "Epoch [4/10], Training Loss: 0.973, Validation Accuracy: 58.93%\n",
            "Epoch [5/10], Training Loss: 0.963, Validation Accuracy: 59.89%\n",
            "Epoch [6/10], Training Loss: 0.952, Validation Accuracy: 60.10%\n",
            "Epoch [7/10], Training Loss: 0.937, Validation Accuracy: 59.78%\n",
            "Epoch [8/10], Training Loss: 0.922, Validation Accuracy: 59.75%\n",
            "Epoch [9/10], Training Loss: 0.907, Validation Accuracy: 58.87%\n",
            "Epoch [10/10], Training Loss: 0.889, Validation Accuracy: 59.71%\n",
            "Epoch [1/10], Training Loss: 1.018, Validation Accuracy: 59.58%\n",
            "Epoch [2/10], Training Loss: 0.978, Validation Accuracy: 59.97%\n",
            "Epoch [3/10], Training Loss: 0.936, Validation Accuracy: 60.48%\n",
            "Epoch [4/10], Training Loss: 0.913, Validation Accuracy: 59.29%\n",
            "Epoch [5/10], Training Loss: 0.898, Validation Accuracy: 59.70%\n",
            "Epoch [6/10], Training Loss: 0.889, Validation Accuracy: 59.96%\n",
            "Epoch [7/10], Training Loss: 0.878, Validation Accuracy: 60.00%\n",
            "Epoch [8/10], Training Loss: 0.862, Validation Accuracy: 59.44%\n",
            "Epoch [9/10], Training Loss: 0.843, Validation Accuracy: 59.61%\n",
            "Epoch [10/10], Training Loss: 0.838, Validation Accuracy: 59.90%\n",
            "Epoch [1/10], Training Loss: 1.035, Validation Accuracy: 59.95%\n",
            "Epoch [2/10], Training Loss: 0.987, Validation Accuracy: 60.16%\n",
            "Epoch [3/10], Training Loss: 0.955, Validation Accuracy: 59.76%\n",
            "Epoch [4/10], Training Loss: 0.943, Validation Accuracy: 60.31%\n",
            "Epoch [5/10], Training Loss: 0.915, Validation Accuracy: 59.87%\n",
            "Epoch [6/10], Training Loss: 0.895, Validation Accuracy: 59.56%\n",
            "Epoch [7/10], Training Loss: 0.883, Validation Accuracy: 59.93%\n",
            "Epoch [8/10], Training Loss: 0.860, Validation Accuracy: 60.02%\n",
            "Epoch [9/10], Training Loss: 0.848, Validation Accuracy: 60.01%\n",
            "Epoch [10/10], Training Loss: 0.839, Validation Accuracy: 60.02%\n",
            "Epoch [1/10], Training Loss: 1.040, Validation Accuracy: 60.04%\n",
            "Epoch [2/10], Training Loss: 0.994, Validation Accuracy: 59.90%\n",
            "Epoch [3/10], Training Loss: 0.963, Validation Accuracy: 60.63%\n",
            "Epoch [4/10], Training Loss: 0.946, Validation Accuracy: 60.42%\n",
            "Epoch [5/10], Training Loss: 0.916, Validation Accuracy: 60.85%\n",
            "Epoch [6/10], Training Loss: 0.895, Validation Accuracy: 59.79%\n",
            "Epoch [7/10], Training Loss: 0.881, Validation Accuracy: 59.96%\n",
            "Epoch [8/10], Training Loss: 0.878, Validation Accuracy: 58.89%\n",
            "Epoch [9/10], Training Loss: 0.852, Validation Accuracy: 59.89%\n",
            "Epoch [10/10], Training Loss: 0.844, Validation Accuracy: 59.56%\n",
            "Epoch [1/10], Training Loss: 0.979, Validation Accuracy: 60.37%\n",
            "Epoch [2/10], Training Loss: 0.937, Validation Accuracy: 60.32%\n",
            "Epoch [3/10], Training Loss: 0.894, Validation Accuracy: 60.21%\n",
            "Epoch [4/10], Training Loss: 0.872, Validation Accuracy: 59.67%\n",
            "Epoch [5/10], Training Loss: 0.851, Validation Accuracy: 60.35%\n",
            "Epoch [6/10], Training Loss: 0.841, Validation Accuracy: 59.60%\n",
            "Epoch [7/10], Training Loss: 0.821, Validation Accuracy: 60.20%\n",
            "Epoch [8/10], Training Loss: 0.806, Validation Accuracy: 59.68%\n",
            "Epoch [9/10], Training Loss: 0.786, Validation Accuracy: 60.14%\n",
            "Epoch [10/10], Training Loss: 0.767, Validation Accuracy: 60.27%\n",
            "Epoch [1/10], Training Loss: 1.002, Validation Accuracy: 61.10%\n",
            "Epoch [2/10], Training Loss: 0.953, Validation Accuracy: 60.77%\n",
            "Epoch [3/10], Training Loss: 0.924, Validation Accuracy: 60.97%\n",
            "Epoch [4/10], Training Loss: 0.887, Validation Accuracy: 60.12%\n",
            "Epoch [5/10], Training Loss: 0.877, Validation Accuracy: 60.65%\n",
            "Epoch [6/10], Training Loss: 0.849, Validation Accuracy: 60.91%\n",
            "Epoch [7/10], Training Loss: 0.834, Validation Accuracy: 61.00%\n",
            "Epoch [8/10], Training Loss: 0.823, Validation Accuracy: 60.88%\n",
            "Epoch [9/10], Training Loss: 0.803, Validation Accuracy: 61.08%\n",
            "Epoch [10/10], Training Loss: 0.789, Validation Accuracy: 61.05%\n",
            "Epoch [1/10], Training Loss: 0.948, Validation Accuracy: 60.29%\n",
            "Epoch [2/10], Training Loss: 0.898, Validation Accuracy: 61.21%\n",
            "Epoch [3/10], Training Loss: 0.876, Validation Accuracy: 60.91%\n",
            "Epoch [4/10], Training Loss: 0.845, Validation Accuracy: 61.26%\n",
            "Epoch [5/10], Training Loss: 0.818, Validation Accuracy: 60.20%\n",
            "Epoch [6/10], Training Loss: 0.801, Validation Accuracy: 61.25%\n",
            "Epoch [7/10], Training Loss: 0.780, Validation Accuracy: 60.09%\n",
            "Epoch [8/10], Training Loss: 0.764, Validation Accuracy: 61.19%\n",
            "Epoch [9/10], Training Loss: 0.749, Validation Accuracy: 60.62%\n",
            "Epoch [10/10], Training Loss: 0.739, Validation Accuracy: 60.77%\n",
            "Epoch [1/10], Training Loss: 0.967, Validation Accuracy: 60.50%\n",
            "Epoch [2/10], Training Loss: 0.903, Validation Accuracy: 60.64%\n",
            "Epoch [3/10], Training Loss: 0.880, Validation Accuracy: 60.52%\n",
            "Epoch [4/10], Training Loss: 0.850, Validation Accuracy: 60.51%\n",
            "Epoch [5/10], Training Loss: 0.817, Validation Accuracy: 60.74%\n",
            "Epoch [6/10], Training Loss: 0.803, Validation Accuracy: 60.03%\n",
            "Epoch [7/10], Training Loss: 0.778, Validation Accuracy: 60.55%\n",
            "Epoch [8/10], Training Loss: 0.764, Validation Accuracy: 60.38%\n",
            "Epoch [9/10], Training Loss: 0.749, Validation Accuracy: 60.51%\n",
            "Epoch [10/10], Training Loss: 0.735, Validation Accuracy: 60.34%\n",
            "Epoch [1/10], Training Loss: 0.978, Validation Accuracy: 60.12%\n",
            "Epoch [2/10], Training Loss: 0.910, Validation Accuracy: 60.28%\n",
            "Epoch [3/10], Training Loss: 0.875, Validation Accuracy: 60.99%\n",
            "Epoch [4/10], Training Loss: 0.848, Validation Accuracy: 60.98%\n",
            "Epoch [5/10], Training Loss: 0.827, Validation Accuracy: 59.70%\n",
            "Epoch [6/10], Training Loss: 0.809, Validation Accuracy: 60.88%\n",
            "Epoch [7/10], Training Loss: 0.785, Validation Accuracy: 61.00%\n",
            "Epoch [8/10], Training Loss: 0.769, Validation Accuracy: 60.53%\n",
            "Epoch [9/10], Training Loss: 0.756, Validation Accuracy: 60.67%\n",
            "Epoch [10/10], Training Loss: 0.741, Validation Accuracy: 60.44%\n",
            "Epoch [1/10], Training Loss: 0.927, Validation Accuracy: 60.48%\n",
            "Epoch [2/10], Training Loss: 0.849, Validation Accuracy: 61.15%\n",
            "Epoch [3/10], Training Loss: 0.824, Validation Accuracy: 61.32%\n",
            "Epoch [4/10], Training Loss: 0.789, Validation Accuracy: 60.55%\n",
            "Epoch [5/10], Training Loss: 0.775, Validation Accuracy: 61.37%\n",
            "Epoch [6/10], Training Loss: 0.746, Validation Accuracy: 61.01%\n",
            "Epoch [7/10], Training Loss: 0.728, Validation Accuracy: 60.73%\n",
            "Epoch [8/10], Training Loss: 0.712, Validation Accuracy: 60.96%\n",
            "Epoch [9/10], Training Loss: 0.693, Validation Accuracy: 60.05%\n",
            "Epoch [10/10], Training Loss: 0.687, Validation Accuracy: 61.58%\n",
            "Epoch [1/10], Training Loss: 0.962, Validation Accuracy: 60.66%\n",
            "Epoch [2/10], Training Loss: 0.888, Validation Accuracy: 61.11%\n",
            "Epoch [3/10], Training Loss: 0.852, Validation Accuracy: 61.33%\n",
            "Epoch [4/10], Training Loss: 0.825, Validation Accuracy: 61.50%\n",
            "Epoch [5/10], Training Loss: 0.799, Validation Accuracy: 61.35%\n",
            "Epoch [6/10], Training Loss: 0.774, Validation Accuracy: 60.97%\n",
            "Epoch [7/10], Training Loss: 0.757, Validation Accuracy: 61.31%\n",
            "Epoch [8/10], Training Loss: 0.735, Validation Accuracy: 60.80%\n",
            "Epoch [9/10], Training Loss: 0.723, Validation Accuracy: 60.61%\n",
            "Epoch [10/10], Training Loss: 0.703, Validation Accuracy: 60.69%\n",
            "Epoch [1/10], Training Loss: 0.901, Validation Accuracy: 61.17%\n",
            "Epoch [2/10], Training Loss: 0.837, Validation Accuracy: 61.71%\n",
            "Epoch [3/10], Training Loss: 0.801, Validation Accuracy: 61.40%\n",
            "Epoch [4/10], Training Loss: 0.764, Validation Accuracy: 61.64%\n",
            "Epoch [5/10], Training Loss: 0.738, Validation Accuracy: 61.63%\n",
            "Epoch [6/10], Training Loss: 0.719, Validation Accuracy: 61.48%\n",
            "Epoch [7/10], Training Loss: 0.702, Validation Accuracy: 61.44%\n",
            "Epoch [8/10], Training Loss: 0.685, Validation Accuracy: 60.21%\n",
            "Epoch [9/10], Training Loss: 0.659, Validation Accuracy: 60.89%\n",
            "Epoch [10/10], Training Loss: 0.651, Validation Accuracy: 61.12%\n",
            "Epoch [1/10], Training Loss: 0.919, Validation Accuracy: 60.59%\n",
            "Epoch [2/10], Training Loss: 0.839, Validation Accuracy: 60.24%\n",
            "Epoch [3/10], Training Loss: 0.802, Validation Accuracy: 61.17%\n",
            "Epoch [4/10], Training Loss: 0.765, Validation Accuracy: 60.76%\n",
            "Epoch [5/10], Training Loss: 0.746, Validation Accuracy: 60.41%\n",
            "Epoch [6/10], Training Loss: 0.724, Validation Accuracy: 60.74%\n",
            "Epoch [7/10], Training Loss: 0.699, Validation Accuracy: 60.56%\n",
            "Epoch [8/10], Training Loss: 0.672, Validation Accuracy: 60.44%\n",
            "Epoch [9/10], Training Loss: 0.661, Validation Accuracy: 60.00%\n",
            "Epoch [10/10], Training Loss: 0.651, Validation Accuracy: 60.32%\n",
            "Epoch [1/10], Training Loss: 0.930, Validation Accuracy: 61.21%\n",
            "Epoch [2/10], Training Loss: 0.846, Validation Accuracy: 60.51%\n",
            "Epoch [3/10], Training Loss: 0.812, Validation Accuracy: 60.32%\n",
            "Epoch [4/10], Training Loss: 0.778, Validation Accuracy: 61.18%\n",
            "Epoch [5/10], Training Loss: 0.745, Validation Accuracy: 60.93%\n",
            "Epoch [6/10], Training Loss: 0.726, Validation Accuracy: 59.61%\n",
            "Epoch [7/10], Training Loss: 0.703, Validation Accuracy: 61.06%\n",
            "Epoch [8/10], Training Loss: 0.680, Validation Accuracy: 60.82%\n",
            "Epoch [9/10], Training Loss: 0.662, Validation Accuracy: 60.37%\n",
            "Epoch [10/10], Training Loss: 0.646, Validation Accuracy: 60.46%\n",
            "Epoch [1/10], Training Loss: 0.882, Validation Accuracy: 60.92%\n",
            "Epoch [2/10], Training Loss: 0.791, Validation Accuracy: 60.97%\n",
            "Epoch [3/10], Training Loss: 0.749, Validation Accuracy: 62.10%\n",
            "Epoch [4/10], Training Loss: 0.711, Validation Accuracy: 61.00%\n",
            "Epoch [5/10], Training Loss: 0.703, Validation Accuracy: 60.75%\n",
            "Epoch [6/10], Training Loss: 0.666, Validation Accuracy: 61.08%\n",
            "Epoch [7/10], Training Loss: 0.641, Validation Accuracy: 61.32%\n",
            "Epoch [8/10], Training Loss: 0.624, Validation Accuracy: 61.33%\n",
            "Epoch [9/10], Training Loss: 0.603, Validation Accuracy: 61.34%\n",
            "Epoch [10/10], Training Loss: 0.584, Validation Accuracy: 61.16%\n",
            "Epoch [1/10], Training Loss: 0.911, Validation Accuracy: 61.29%\n",
            "Epoch [2/10], Training Loss: 0.832, Validation Accuracy: 61.87%\n",
            "Epoch [3/10], Training Loss: 0.777, Validation Accuracy: 61.99%\n",
            "Epoch [4/10], Training Loss: 0.739, Validation Accuracy: 60.85%\n",
            "Epoch [5/10], Training Loss: 0.722, Validation Accuracy: 61.67%\n",
            "Epoch [6/10], Training Loss: 0.689, Validation Accuracy: 60.88%\n",
            "Epoch [7/10], Training Loss: 0.673, Validation Accuracy: 60.83%\n",
            "Epoch [8/10], Training Loss: 0.645, Validation Accuracy: 60.75%\n",
            "Epoch [9/10], Training Loss: 0.636, Validation Accuracy: 61.00%\n",
            "Epoch [10/10], Training Loss: 0.616, Validation Accuracy: 60.48%\n",
            "Epoch [1/10], Training Loss: 0.858, Validation Accuracy: 61.27%\n",
            "Epoch [2/10], Training Loss: 0.781, Validation Accuracy: 61.18%\n",
            "Epoch [3/10], Training Loss: 0.736, Validation Accuracy: 61.43%\n",
            "Epoch [4/10], Training Loss: 0.692, Validation Accuracy: 61.70%\n",
            "Epoch [5/10], Training Loss: 0.669, Validation Accuracy: 60.83%\n",
            "Epoch [6/10], Training Loss: 0.650, Validation Accuracy: 60.95%\n",
            "Epoch [7/10], Training Loss: 0.624, Validation Accuracy: 61.40%\n",
            "Epoch [8/10], Training Loss: 0.597, Validation Accuracy: 61.15%\n",
            "Epoch [9/10], Training Loss: 0.578, Validation Accuracy: 60.76%\n",
            "Epoch [10/10], Training Loss: 0.566, Validation Accuracy: 61.34%\n",
            "Epoch [1/10], Training Loss: 0.882, Validation Accuracy: 61.42%\n",
            "Epoch [2/10], Training Loss: 0.779, Validation Accuracy: 61.16%\n",
            "Epoch [3/10], Training Loss: 0.730, Validation Accuracy: 61.36%\n",
            "Epoch [4/10], Training Loss: 0.699, Validation Accuracy: 61.49%\n",
            "Epoch [5/10], Training Loss: 0.664, Validation Accuracy: 60.96%\n",
            "Epoch [6/10], Training Loss: 0.636, Validation Accuracy: 60.80%\n",
            "Epoch [7/10], Training Loss: 0.614, Validation Accuracy: 59.65%\n",
            "Epoch [8/10], Training Loss: 0.603, Validation Accuracy: 60.58%\n",
            "Epoch [9/10], Training Loss: 0.571, Validation Accuracy: 60.03%\n",
            "Epoch [10/10], Training Loss: 0.551, Validation Accuracy: 60.00%\n",
            "Epoch [1/10], Training Loss: 0.882, Validation Accuracy: 60.52%\n",
            "Epoch [2/10], Training Loss: 0.780, Validation Accuracy: 60.89%\n",
            "Epoch [3/10], Training Loss: 0.728, Validation Accuracy: 60.59%\n",
            "Epoch [4/10], Training Loss: 0.690, Validation Accuracy: 61.01%\n",
            "Epoch [5/10], Training Loss: 0.673, Validation Accuracy: 60.21%\n",
            "Epoch [6/10], Training Loss: 0.648, Validation Accuracy: 60.88%\n",
            "Epoch [7/10], Training Loss: 0.613, Validation Accuracy: 60.39%\n",
            "Epoch [8/10], Training Loss: 0.599, Validation Accuracy: 60.73%\n",
            "Epoch [9/10], Training Loss: 0.570, Validation Accuracy: 59.66%\n",
            "Epoch [10/10], Training Loss: 0.548, Validation Accuracy: 59.82%\n",
            "Epoch [1/10], Training Loss: 0.830, Validation Accuracy: 60.56%\n",
            "Epoch [2/10], Training Loss: 0.728, Validation Accuracy: 60.98%\n",
            "Epoch [3/10], Training Loss: 0.674, Validation Accuracy: 61.16%\n",
            "Epoch [4/10], Training Loss: 0.635, Validation Accuracy: 61.55%\n",
            "Epoch [5/10], Training Loss: 0.607, Validation Accuracy: 61.58%\n",
            "Epoch [6/10], Training Loss: 0.577, Validation Accuracy: 61.59%\n",
            "Epoch [7/10], Training Loss: 0.553, Validation Accuracy: 61.22%\n",
            "Epoch [8/10], Training Loss: 0.543, Validation Accuracy: 61.35%\n",
            "Epoch [9/10], Training Loss: 0.521, Validation Accuracy: 60.89%\n",
            "Epoch [10/10], Training Loss: 0.500, Validation Accuracy: 61.09%\n",
            "Epoch [1/10], Training Loss: 0.871, Validation Accuracy: 59.74%\n",
            "Epoch [2/10], Training Loss: 0.766, Validation Accuracy: 60.70%\n",
            "Epoch [3/10], Training Loss: 0.721, Validation Accuracy: 61.66%\n",
            "Epoch [4/10], Training Loss: 0.667, Validation Accuracy: 60.94%\n",
            "Epoch [5/10], Training Loss: 0.648, Validation Accuracy: 60.48%\n",
            "Epoch [6/10], Training Loss: 0.615, Validation Accuracy: 61.00%\n",
            "Epoch [7/10], Training Loss: 0.594, Validation Accuracy: 61.10%\n",
            "Epoch [8/10], Training Loss: 0.561, Validation Accuracy: 60.54%\n",
            "Epoch [9/10], Training Loss: 0.545, Validation Accuracy: 60.68%\n",
            "Epoch [10/10], Training Loss: 0.532, Validation Accuracy: 60.85%\n",
            "Epoch [1/10], Training Loss: 0.822, Validation Accuracy: 60.96%\n",
            "Epoch [2/10], Training Loss: 0.719, Validation Accuracy: 61.19%\n",
            "Epoch [3/10], Training Loss: 0.667, Validation Accuracy: 61.50%\n",
            "Epoch [4/10], Training Loss: 0.622, Validation Accuracy: 61.02%\n",
            "Epoch [5/10], Training Loss: 0.591, Validation Accuracy: 61.03%\n",
            "Epoch [6/10], Training Loss: 0.561, Validation Accuracy: 61.47%\n",
            "Epoch [7/10], Training Loss: 0.545, Validation Accuracy: 60.31%\n",
            "Epoch [8/10], Training Loss: 0.513, Validation Accuracy: 61.31%\n",
            "Epoch [9/10], Training Loss: 0.501, Validation Accuracy: 61.46%\n",
            "Epoch [10/10], Training Loss: 0.475, Validation Accuracy: 61.15%\n",
            "Epoch [1/10], Training Loss: 0.846, Validation Accuracy: 59.88%\n",
            "Epoch [2/10], Training Loss: 0.748, Validation Accuracy: 60.48%\n",
            "Epoch [3/10], Training Loss: 0.674, Validation Accuracy: 60.94%\n",
            "Epoch [4/10], Training Loss: 0.630, Validation Accuracy: 60.94%\n",
            "Epoch [5/10], Training Loss: 0.589, Validation Accuracy: 60.18%\n",
            "Epoch [6/10], Training Loss: 0.565, Validation Accuracy: 60.16%\n",
            "Epoch [7/10], Training Loss: 0.534, Validation Accuracy: 60.15%\n",
            "Epoch [8/10], Training Loss: 0.517, Validation Accuracy: 59.98%\n",
            "Epoch [9/10], Training Loss: 0.492, Validation Accuracy: 60.28%\n",
            "Epoch [10/10], Training Loss: 0.470, Validation Accuracy: 60.31%\n",
            "Epoch [1/10], Training Loss: 0.832, Validation Accuracy: 59.42%\n",
            "Epoch [2/10], Training Loss: 0.710, Validation Accuracy: 60.56%\n",
            "Epoch [3/10], Training Loss: 0.660, Validation Accuracy: 60.95%\n",
            "Epoch [4/10], Training Loss: 0.622, Validation Accuracy: 60.41%\n",
            "Epoch [5/10], Training Loss: 0.589, Validation Accuracy: 60.54%\n",
            "Epoch [6/10], Training Loss: 0.555, Validation Accuracy: 60.42%\n",
            "Epoch [7/10], Training Loss: 0.524, Validation Accuracy: 60.40%\n",
            "Epoch [8/10], Training Loss: 0.506, Validation Accuracy: 60.48%\n",
            "Epoch [9/10], Training Loss: 0.497, Validation Accuracy: 60.37%\n",
            "Epoch [10/10], Training Loss: 0.460, Validation Accuracy: 60.55%\n",
            "Epoch [1/10], Training Loss: 0.806, Validation Accuracy: 60.48%\n",
            "Epoch [2/10], Training Loss: 0.677, Validation Accuracy: 61.19%\n",
            "Epoch [3/10], Training Loss: 0.610, Validation Accuracy: 61.08%\n",
            "Epoch [4/10], Training Loss: 0.571, Validation Accuracy: 60.98%\n",
            "Epoch [5/10], Training Loss: 0.531, Validation Accuracy: 61.18%\n",
            "Epoch [6/10], Training Loss: 0.501, Validation Accuracy: 61.15%\n",
            "Epoch [7/10], Training Loss: 0.478, Validation Accuracy: 61.22%\n",
            "Epoch [8/10], Training Loss: 0.458, Validation Accuracy: 60.87%\n",
            "Epoch [9/10], Training Loss: 0.435, Validation Accuracy: 60.93%\n",
            "Epoch [10/10], Training Loss: 0.416, Validation Accuracy: 60.88%\n",
            "Epoch [1/10], Training Loss: 0.844, Validation Accuracy: 60.58%\n",
            "Epoch [2/10], Training Loss: 0.716, Validation Accuracy: 61.21%\n",
            "Epoch [3/10], Training Loss: 0.642, Validation Accuracy: 60.64%\n",
            "Epoch [4/10], Training Loss: 0.605, Validation Accuracy: 61.37%\n",
            "Epoch [5/10], Training Loss: 0.558, Validation Accuracy: 60.85%\n",
            "Epoch [6/10], Training Loss: 0.525, Validation Accuracy: 60.97%\n",
            "Epoch [7/10], Training Loss: 0.504, Validation Accuracy: 61.10%\n",
            "Epoch [8/10], Training Loss: 0.478, Validation Accuracy: 60.15%\n",
            "Epoch [9/10], Training Loss: 0.451, Validation Accuracy: 60.32%\n",
            "Epoch [10/10], Training Loss: 0.439, Validation Accuracy: 60.09%\n",
            "Confusion Matrix:\n",
            "[[628  26  72  16  50  13  18  19  95  63]\n",
            " [ 35 715  17  14   8  10  15  12  44 130]\n",
            " [ 43   9 505  50 140  90  75  58  21   9]\n",
            " [ 27  14  91 320  93 241  95  69  20  30]\n",
            " [ 20   4 101  49 541  72  78 113  11  11]\n",
            " [ 17   9  87 120  90 531  44  87   3  12]\n",
            " [  8   7  60  44  66  52 729  17  10   7]\n",
            " [  7   5  39  27  98  70  13 722   4  15]\n",
            " [ 82  44  27  10  27   8  10  10 729  53]\n",
            " [ 37 112  18  15  12  14  17  30  43 702]]\n",
            "Test Accuracy: 61.22%\n",
            "True Positives (TP): [628 715 505 320 541 531 729 722 729 702]\n",
            "False Positives (FP): [276 230 512 345 584 570 365 415 251 330]\n",
            "True Negatives (TN): [8724 8770 8488 8655 8416 8430 8635 8585 8749 8670]\n",
            "False Negatives (FN): [372 285 495 680 459 469 271 278 271 298]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.69469027 0.75661376 0.49655851 0.48120301 0.48088889 0.48228883\n",
            " 0.66636197 0.6350044  0.74387755 0.68023256]\n",
            "Recall: [0.628 0.715 0.505 0.32  0.541 0.531 0.729 0.722 0.729 0.702]\n",
            "F1 Score: [0.65966387 0.73521851 0.50074368 0.38438438 0.50917647 0.50547358\n",
            " 0.69627507 0.67571362 0.73636364 0.69094488]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 0.1):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=0.1):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_uniform: Dict ) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "    mean = distribution_info_uniform[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info_uniform[\"std\"].mean().item()  # Convert numpy array to float\n",
        "\n",
        "\n",
        "\n",
        "     # Generate augmented data using Uniform distribution\n",
        "    augmented_data_uniform = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = augmented_data_uniform\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=0.1)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"uniform\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipNzJJik9CB9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNHGWoDu9C4K"
      },
      "source": [
        "Beta=.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjZR19at9FUy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tR9lskb9Fqc",
        "outputId": "9f979976-fa88-4e17-8f79-94fbc3a9ea6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Images per Class: [5957 5896 5860 6115 6083 5995 5956 5954 6073 6111]\n",
            "Epoch [1/10], Training Loss: 2.304, Validation Accuracy: 9.87%\n",
            "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 9.87%\n",
            "Epoch [3/10], Training Loss: 2.301, Validation Accuracy: 9.87%\n",
            "Epoch [4/10], Training Loss: 2.299, Validation Accuracy: 9.87%\n",
            "Epoch [5/10], Training Loss: 2.297, Validation Accuracy: 9.95%\n",
            "Epoch [6/10], Training Loss: 2.295, Validation Accuracy: 10.27%\n",
            "Epoch [7/10], Training Loss: 2.292, Validation Accuracy: 12.49%\n",
            "Epoch [8/10], Training Loss: 2.287, Validation Accuracy: 15.41%\n",
            "Epoch [9/10], Training Loss: 2.281, Validation Accuracy: 17.36%\n",
            "Epoch [10/10], Training Loss: 2.270, Validation Accuracy: 18.15%\n",
            "Epoch [1/10], Training Loss: 2.259, Validation Accuracy: 17.99%\n",
            "Epoch [2/10], Training Loss: 2.235, Validation Accuracy: 18.25%\n",
            "Epoch [3/10], Training Loss: 2.200, Validation Accuracy: 21.72%\n",
            "Epoch [4/10], Training Loss: 2.161, Validation Accuracy: 22.18%\n",
            "Epoch [5/10], Training Loss: 2.129, Validation Accuracy: 22.98%\n",
            "Epoch [6/10], Training Loss: 2.103, Validation Accuracy: 24.14%\n",
            "Epoch [7/10], Training Loss: 2.076, Validation Accuracy: 25.06%\n",
            "Epoch [8/10], Training Loss: 2.053, Validation Accuracy: 25.99%\n",
            "Epoch [9/10], Training Loss: 2.031, Validation Accuracy: 26.79%\n",
            "Epoch [10/10], Training Loss: 2.006, Validation Accuracy: 27.46%\n",
            "Epoch [1/10], Training Loss: 1.988, Validation Accuracy: 27.75%\n",
            "Epoch [2/10], Training Loss: 1.966, Validation Accuracy: 28.61%\n",
            "Epoch [3/10], Training Loss: 1.946, Validation Accuracy: 28.76%\n",
            "Epoch [4/10], Training Loss: 1.926, Validation Accuracy: 30.34%\n",
            "Epoch [5/10], Training Loss: 1.908, Validation Accuracy: 30.42%\n",
            "Epoch [6/10], Training Loss: 1.890, Validation Accuracy: 31.32%\n",
            "Epoch [7/10], Training Loss: 1.873, Validation Accuracy: 31.82%\n",
            "Epoch [8/10], Training Loss: 1.856, Validation Accuracy: 31.70%\n",
            "Epoch [9/10], Training Loss: 1.833, Validation Accuracy: 33.58%\n",
            "Epoch [10/10], Training Loss: 1.811, Validation Accuracy: 34.19%\n",
            "Epoch [1/10], Training Loss: 1.813, Validation Accuracy: 34.72%\n",
            "Epoch [2/10], Training Loss: 1.789, Validation Accuracy: 35.43%\n",
            "Epoch [3/10], Training Loss: 1.767, Validation Accuracy: 36.38%\n",
            "Epoch [4/10], Training Loss: 1.742, Validation Accuracy: 36.62%\n",
            "Epoch [5/10], Training Loss: 1.723, Validation Accuracy: 37.02%\n",
            "Epoch [6/10], Training Loss: 1.705, Validation Accuracy: 37.68%\n",
            "Epoch [7/10], Training Loss: 1.690, Validation Accuracy: 37.63%\n",
            "Epoch [8/10], Training Loss: 1.674, Validation Accuracy: 38.13%\n",
            "Epoch [9/10], Training Loss: 1.666, Validation Accuracy: 38.25%\n",
            "Epoch [10/10], Training Loss: 1.646, Validation Accuracy: 38.79%\n",
            "Epoch [1/10], Training Loss: 1.652, Validation Accuracy: 38.60%\n",
            "Epoch [2/10], Training Loss: 1.641, Validation Accuracy: 38.35%\n",
            "Epoch [3/10], Training Loss: 1.632, Validation Accuracy: 39.05%\n",
            "Epoch [4/10], Training Loss: 1.612, Validation Accuracy: 40.25%\n",
            "Epoch [5/10], Training Loss: 1.601, Validation Accuracy: 40.28%\n",
            "Epoch [6/10], Training Loss: 1.589, Validation Accuracy: 41.03%\n",
            "Epoch [7/10], Training Loss: 1.575, Validation Accuracy: 41.66%\n",
            "Epoch [8/10], Training Loss: 1.563, Validation Accuracy: 41.59%\n",
            "Epoch [9/10], Training Loss: 1.556, Validation Accuracy: 42.43%\n",
            "Epoch [10/10], Training Loss: 1.546, Validation Accuracy: 42.17%\n",
            "Epoch [1/10], Training Loss: 1.574, Validation Accuracy: 42.75%\n",
            "Epoch [2/10], Training Loss: 1.563, Validation Accuracy: 42.94%\n",
            "Epoch [3/10], Training Loss: 1.555, Validation Accuracy: 43.34%\n",
            "Epoch [4/10], Training Loss: 1.549, Validation Accuracy: 42.93%\n",
            "Epoch [5/10], Training Loss: 1.533, Validation Accuracy: 43.26%\n",
            "Epoch [6/10], Training Loss: 1.524, Validation Accuracy: 42.97%\n",
            "Epoch [7/10], Training Loss: 1.514, Validation Accuracy: 44.30%\n",
            "Epoch [8/10], Training Loss: 1.508, Validation Accuracy: 44.32%\n",
            "Epoch [9/10], Training Loss: 1.495, Validation Accuracy: 43.76%\n",
            "Epoch [10/10], Training Loss: 1.489, Validation Accuracy: 45.02%\n",
            "Epoch [1/10], Training Loss: 1.543, Validation Accuracy: 44.66%\n",
            "Epoch [2/10], Training Loss: 1.524, Validation Accuracy: 45.31%\n",
            "Epoch [3/10], Training Loss: 1.514, Validation Accuracy: 45.54%\n",
            "Epoch [4/10], Training Loss: 1.513, Validation Accuracy: 45.36%\n",
            "Epoch [5/10], Training Loss: 1.487, Validation Accuracy: 45.90%\n",
            "Epoch [6/10], Training Loss: 1.480, Validation Accuracy: 46.46%\n",
            "Epoch [7/10], Training Loss: 1.472, Validation Accuracy: 46.20%\n",
            "Epoch [8/10], Training Loss: 1.464, Validation Accuracy: 45.95%\n",
            "Epoch [9/10], Training Loss: 1.454, Validation Accuracy: 45.77%\n",
            "Epoch [10/10], Training Loss: 1.448, Validation Accuracy: 45.46%\n",
            "Epoch [1/10], Training Loss: 1.473, Validation Accuracy: 46.76%\n",
            "Epoch [2/10], Training Loss: 1.455, Validation Accuracy: 46.82%\n",
            "Epoch [3/10], Training Loss: 1.447, Validation Accuracy: 46.86%\n",
            "Epoch [4/10], Training Loss: 1.435, Validation Accuracy: 46.77%\n",
            "Epoch [5/10], Training Loss: 1.424, Validation Accuracy: 46.98%\n",
            "Epoch [6/10], Training Loss: 1.409, Validation Accuracy: 46.90%\n",
            "Epoch [7/10], Training Loss: 1.401, Validation Accuracy: 48.23%\n",
            "Epoch [8/10], Training Loss: 1.396, Validation Accuracy: 47.64%\n",
            "Epoch [9/10], Training Loss: 1.383, Validation Accuracy: 47.68%\n",
            "Epoch [10/10], Training Loss: 1.376, Validation Accuracy: 48.24%\n",
            "Epoch [1/10], Training Loss: 1.411, Validation Accuracy: 48.07%\n",
            "Epoch [2/10], Training Loss: 1.393, Validation Accuracy: 48.17%\n",
            "Epoch [3/10], Training Loss: 1.388, Validation Accuracy: 48.13%\n",
            "Epoch [4/10], Training Loss: 1.375, Validation Accuracy: 48.39%\n",
            "Epoch [5/10], Training Loss: 1.359, Validation Accuracy: 48.80%\n",
            "Epoch [6/10], Training Loss: 1.348, Validation Accuracy: 48.92%\n",
            "Epoch [7/10], Training Loss: 1.339, Validation Accuracy: 48.91%\n",
            "Epoch [8/10], Training Loss: 1.328, Validation Accuracy: 49.23%\n",
            "Epoch [9/10], Training Loss: 1.323, Validation Accuracy: 48.43%\n",
            "Epoch [10/10], Training Loss: 1.316, Validation Accuracy: 48.97%\n",
            "Epoch [1/10], Training Loss: 1.394, Validation Accuracy: 49.59%\n",
            "Epoch [2/10], Training Loss: 1.378, Validation Accuracy: 49.73%\n",
            "Epoch [3/10], Training Loss: 1.364, Validation Accuracy: 49.65%\n",
            "Epoch [4/10], Training Loss: 1.351, Validation Accuracy: 50.08%\n",
            "Epoch [5/10], Training Loss: 1.346, Validation Accuracy: 50.35%\n",
            "Epoch [6/10], Training Loss: 1.333, Validation Accuracy: 49.25%\n",
            "Epoch [7/10], Training Loss: 1.323, Validation Accuracy: 49.62%\n",
            "Epoch [8/10], Training Loss: 1.323, Validation Accuracy: 50.30%\n",
            "Epoch [9/10], Training Loss: 1.316, Validation Accuracy: 50.17%\n",
            "Epoch [10/10], Training Loss: 1.309, Validation Accuracy: 50.50%\n",
            "Epoch [1/10], Training Loss: 1.365, Validation Accuracy: 51.15%\n",
            "Epoch [2/10], Training Loss: 1.352, Validation Accuracy: 51.24%\n",
            "Epoch [3/10], Training Loss: 1.331, Validation Accuracy: 50.94%\n",
            "Epoch [4/10], Training Loss: 1.319, Validation Accuracy: 50.42%\n",
            "Epoch [5/10], Training Loss: 1.313, Validation Accuracy: 50.12%\n",
            "Epoch [6/10], Training Loss: 1.299, Validation Accuracy: 51.51%\n",
            "Epoch [7/10], Training Loss: 1.292, Validation Accuracy: 49.44%\n",
            "Epoch [8/10], Training Loss: 1.281, Validation Accuracy: 51.43%\n",
            "Epoch [9/10], Training Loss: 1.275, Validation Accuracy: 51.74%\n",
            "Epoch [10/10], Training Loss: 1.274, Validation Accuracy: 52.03%\n",
            "Epoch [1/10], Training Loss: 1.347, Validation Accuracy: 51.86%\n",
            "Epoch [2/10], Training Loss: 1.327, Validation Accuracy: 51.66%\n",
            "Epoch [3/10], Training Loss: 1.304, Validation Accuracy: 51.99%\n",
            "Epoch [4/10], Training Loss: 1.302, Validation Accuracy: 51.44%\n",
            "Epoch [5/10], Training Loss: 1.288, Validation Accuracy: 52.24%\n",
            "Epoch [6/10], Training Loss: 1.275, Validation Accuracy: 52.60%\n",
            "Epoch [7/10], Training Loss: 1.264, Validation Accuracy: 52.50%\n",
            "Epoch [8/10], Training Loss: 1.261, Validation Accuracy: 51.86%\n",
            "Epoch [9/10], Training Loss: 1.239, Validation Accuracy: 52.06%\n",
            "Epoch [10/10], Training Loss: 1.234, Validation Accuracy: 52.27%\n",
            "Epoch [1/10], Training Loss: 1.318, Validation Accuracy: 52.94%\n",
            "Epoch [2/10], Training Loss: 1.289, Validation Accuracy: 52.90%\n",
            "Epoch [3/10], Training Loss: 1.266, Validation Accuracy: 52.23%\n",
            "Epoch [4/10], Training Loss: 1.256, Validation Accuracy: 53.39%\n",
            "Epoch [5/10], Training Loss: 1.248, Validation Accuracy: 53.30%\n",
            "Epoch [6/10], Training Loss: 1.241, Validation Accuracy: 53.28%\n",
            "Epoch [7/10], Training Loss: 1.217, Validation Accuracy: 53.37%\n",
            "Epoch [8/10], Training Loss: 1.211, Validation Accuracy: 53.42%\n",
            "Epoch [9/10], Training Loss: 1.202, Validation Accuracy: 53.22%\n",
            "Epoch [10/10], Training Loss: 1.193, Validation Accuracy: 51.86%\n",
            "Epoch [1/10], Training Loss: 1.259, Validation Accuracy: 54.25%\n",
            "Epoch [2/10], Training Loss: 1.226, Validation Accuracy: 53.46%\n",
            "Epoch [3/10], Training Loss: 1.213, Validation Accuracy: 53.57%\n",
            "Epoch [4/10], Training Loss: 1.203, Validation Accuracy: 53.26%\n",
            "Epoch [5/10], Training Loss: 1.198, Validation Accuracy: 54.02%\n",
            "Epoch [6/10], Training Loss: 1.179, Validation Accuracy: 53.19%\n",
            "Epoch [7/10], Training Loss: 1.166, Validation Accuracy: 54.04%\n",
            "Epoch [8/10], Training Loss: 1.157, Validation Accuracy: 52.52%\n",
            "Epoch [9/10], Training Loss: 1.151, Validation Accuracy: 53.59%\n",
            "Epoch [10/10], Training Loss: 1.131, Validation Accuracy: 54.02%\n",
            "Epoch [1/10], Training Loss: 1.259, Validation Accuracy: 54.29%\n",
            "Epoch [2/10], Training Loss: 1.229, Validation Accuracy: 54.24%\n",
            "Epoch [3/10], Training Loss: 1.213, Validation Accuracy: 54.75%\n",
            "Epoch [4/10], Training Loss: 1.203, Validation Accuracy: 54.03%\n",
            "Epoch [5/10], Training Loss: 1.185, Validation Accuracy: 54.85%\n",
            "Epoch [6/10], Training Loss: 1.173, Validation Accuracy: 55.32%\n",
            "Epoch [7/10], Training Loss: 1.171, Validation Accuracy: 54.58%\n",
            "Epoch [8/10], Training Loss: 1.161, Validation Accuracy: 55.17%\n",
            "Epoch [9/10], Training Loss: 1.144, Validation Accuracy: 54.68%\n",
            "Epoch [10/10], Training Loss: 1.138, Validation Accuracy: 54.45%\n",
            "Epoch [1/10], Training Loss: 1.240, Validation Accuracy: 54.60%\n",
            "Epoch [2/10], Training Loss: 1.220, Validation Accuracy: 55.36%\n",
            "Epoch [3/10], Training Loss: 1.190, Validation Accuracy: 55.26%\n",
            "Epoch [4/10], Training Loss: 1.172, Validation Accuracy: 55.16%\n",
            "Epoch [5/10], Training Loss: 1.165, Validation Accuracy: 55.29%\n",
            "Epoch [6/10], Training Loss: 1.151, Validation Accuracy: 55.32%\n",
            "Epoch [7/10], Training Loss: 1.135, Validation Accuracy: 55.14%\n",
            "Epoch [8/10], Training Loss: 1.131, Validation Accuracy: 55.43%\n",
            "Epoch [9/10], Training Loss: 1.121, Validation Accuracy: 55.01%\n",
            "Epoch [10/10], Training Loss: 1.111, Validation Accuracy: 55.48%\n",
            "Epoch [1/10], Training Loss: 1.219, Validation Accuracy: 56.25%\n",
            "Epoch [2/10], Training Loss: 1.194, Validation Accuracy: 56.14%\n",
            "Epoch [3/10], Training Loss: 1.173, Validation Accuracy: 56.34%\n",
            "Epoch [4/10], Training Loss: 1.148, Validation Accuracy: 55.00%\n",
            "Epoch [5/10], Training Loss: 1.143, Validation Accuracy: 56.01%\n",
            "Epoch [6/10], Training Loss: 1.127, Validation Accuracy: 55.71%\n",
            "Epoch [7/10], Training Loss: 1.128, Validation Accuracy: 55.44%\n",
            "Epoch [8/10], Training Loss: 1.100, Validation Accuracy: 55.87%\n",
            "Epoch [9/10], Training Loss: 1.087, Validation Accuracy: 56.06%\n",
            "Epoch [10/10], Training Loss: 1.083, Validation Accuracy: 55.72%\n",
            "Epoch [1/10], Training Loss: 1.202, Validation Accuracy: 56.01%\n",
            "Epoch [2/10], Training Loss: 1.161, Validation Accuracy: 56.11%\n",
            "Epoch [3/10], Training Loss: 1.142, Validation Accuracy: 55.94%\n",
            "Epoch [4/10], Training Loss: 1.120, Validation Accuracy: 56.95%\n",
            "Epoch [5/10], Training Loss: 1.107, Validation Accuracy: 56.27%\n",
            "Epoch [6/10], Training Loss: 1.091, Validation Accuracy: 56.06%\n",
            "Epoch [7/10], Training Loss: 1.082, Validation Accuracy: 56.54%\n",
            "Epoch [8/10], Training Loss: 1.072, Validation Accuracy: 56.81%\n",
            "Epoch [9/10], Training Loss: 1.055, Validation Accuracy: 56.55%\n",
            "Epoch [10/10], Training Loss: 1.049, Validation Accuracy: 55.88%\n",
            "Epoch [1/10], Training Loss: 1.162, Validation Accuracy: 57.15%\n",
            "Epoch [2/10], Training Loss: 1.120, Validation Accuracy: 56.35%\n",
            "Epoch [3/10], Training Loss: 1.092, Validation Accuracy: 56.85%\n",
            "Epoch [4/10], Training Loss: 1.077, Validation Accuracy: 56.42%\n",
            "Epoch [5/10], Training Loss: 1.065, Validation Accuracy: 56.60%\n",
            "Epoch [6/10], Training Loss: 1.046, Validation Accuracy: 57.37%\n",
            "Epoch [7/10], Training Loss: 1.030, Validation Accuracy: 55.65%\n",
            "Epoch [8/10], Training Loss: 1.027, Validation Accuracy: 56.48%\n",
            "Epoch [9/10], Training Loss: 1.017, Validation Accuracy: 56.13%\n",
            "Epoch [10/10], Training Loss: 0.997, Validation Accuracy: 57.04%\n",
            "Epoch [1/10], Training Loss: 1.164, Validation Accuracy: 56.83%\n",
            "Epoch [2/10], Training Loss: 1.112, Validation Accuracy: 57.64%\n",
            "Epoch [3/10], Training Loss: 1.091, Validation Accuracy: 57.06%\n",
            "Epoch [4/10], Training Loss: 1.066, Validation Accuracy: 57.65%\n",
            "Epoch [5/10], Training Loss: 1.051, Validation Accuracy: 57.71%\n",
            "Epoch [6/10], Training Loss: 1.036, Validation Accuracy: 56.47%\n",
            "Epoch [7/10], Training Loss: 1.023, Validation Accuracy: 57.45%\n",
            "Epoch [8/10], Training Loss: 1.010, Validation Accuracy: 57.72%\n",
            "Epoch [9/10], Training Loss: 0.998, Validation Accuracy: 57.61%\n",
            "Epoch [10/10], Training Loss: 0.991, Validation Accuracy: 57.26%\n",
            "Epoch [1/10], Training Loss: 1.153, Validation Accuracy: 57.23%\n",
            "Epoch [2/10], Training Loss: 1.111, Validation Accuracy: 57.35%\n",
            "Epoch [3/10], Training Loss: 1.082, Validation Accuracy: 58.27%\n",
            "Epoch [4/10], Training Loss: 1.056, Validation Accuracy: 57.82%\n",
            "Epoch [5/10], Training Loss: 1.046, Validation Accuracy: 57.52%\n",
            "Epoch [6/10], Training Loss: 1.035, Validation Accuracy: 57.79%\n",
            "Epoch [7/10], Training Loss: 1.009, Validation Accuracy: 58.20%\n",
            "Epoch [8/10], Training Loss: 0.996, Validation Accuracy: 58.06%\n",
            "Epoch [9/10], Training Loss: 0.988, Validation Accuracy: 58.38%\n",
            "Epoch [10/10], Training Loss: 0.974, Validation Accuracy: 57.95%\n",
            "Epoch [1/10], Training Loss: 1.128, Validation Accuracy: 57.52%\n",
            "Epoch [2/10], Training Loss: 1.089, Validation Accuracy: 58.49%\n",
            "Epoch [3/10], Training Loss: 1.054, Validation Accuracy: 57.76%\n",
            "Epoch [4/10], Training Loss: 1.033, Validation Accuracy: 58.04%\n",
            "Epoch [5/10], Training Loss: 1.025, Validation Accuracy: 58.06%\n",
            "Epoch [6/10], Training Loss: 1.009, Validation Accuracy: 57.87%\n",
            "Epoch [7/10], Training Loss: 1.000, Validation Accuracy: 57.93%\n",
            "Epoch [8/10], Training Loss: 0.976, Validation Accuracy: 57.79%\n",
            "Epoch [9/10], Training Loss: 0.968, Validation Accuracy: 57.94%\n",
            "Epoch [10/10], Training Loss: 0.955, Validation Accuracy: 57.88%\n",
            "Epoch [1/10], Training Loss: 1.108, Validation Accuracy: 57.66%\n",
            "Epoch [2/10], Training Loss: 1.068, Validation Accuracy: 58.48%\n",
            "Epoch [3/10], Training Loss: 1.031, Validation Accuracy: 57.43%\n",
            "Epoch [4/10], Training Loss: 1.017, Validation Accuracy: 58.08%\n",
            "Epoch [5/10], Training Loss: 0.994, Validation Accuracy: 58.63%\n",
            "Epoch [6/10], Training Loss: 0.981, Validation Accuracy: 57.87%\n",
            "Epoch [7/10], Training Loss: 0.974, Validation Accuracy: 58.38%\n",
            "Epoch [8/10], Training Loss: 0.951, Validation Accuracy: 58.53%\n",
            "Epoch [9/10], Training Loss: 0.935, Validation Accuracy: 58.02%\n",
            "Epoch [10/10], Training Loss: 0.921, Validation Accuracy: 58.43%\n",
            "Epoch [1/10], Training Loss: 1.055, Validation Accuracy: 58.65%\n",
            "Epoch [2/10], Training Loss: 1.028, Validation Accuracy: 58.32%\n",
            "Epoch [3/10], Training Loss: 0.990, Validation Accuracy: 58.21%\n",
            "Epoch [4/10], Training Loss: 0.974, Validation Accuracy: 57.62%\n",
            "Epoch [5/10], Training Loss: 0.950, Validation Accuracy: 58.45%\n",
            "Epoch [6/10], Training Loss: 0.931, Validation Accuracy: 57.70%\n",
            "Epoch [7/10], Training Loss: 0.919, Validation Accuracy: 58.39%\n",
            "Epoch [8/10], Training Loss: 0.917, Validation Accuracy: 58.59%\n",
            "Epoch [9/10], Training Loss: 0.894, Validation Accuracy: 57.72%\n",
            "Epoch [10/10], Training Loss: 0.882, Validation Accuracy: 58.32%\n",
            "Epoch [1/10], Training Loss: 1.067, Validation Accuracy: 59.08%\n",
            "Epoch [2/10], Training Loss: 1.016, Validation Accuracy: 58.72%\n",
            "Epoch [3/10], Training Loss: 0.988, Validation Accuracy: 58.59%\n",
            "Epoch [4/10], Training Loss: 0.964, Validation Accuracy: 58.32%\n",
            "Epoch [5/10], Training Loss: 0.942, Validation Accuracy: 59.40%\n",
            "Epoch [6/10], Training Loss: 0.927, Validation Accuracy: 58.81%\n",
            "Epoch [7/10], Training Loss: 0.903, Validation Accuracy: 59.29%\n",
            "Epoch [8/10], Training Loss: 0.893, Validation Accuracy: 58.31%\n",
            "Epoch [9/10], Training Loss: 0.876, Validation Accuracy: 58.69%\n",
            "Epoch [10/10], Training Loss: 0.872, Validation Accuracy: 58.46%\n",
            "Epoch [1/10], Training Loss: 1.078, Validation Accuracy: 59.32%\n",
            "Epoch [2/10], Training Loss: 1.019, Validation Accuracy: 59.84%\n",
            "Epoch [3/10], Training Loss: 0.982, Validation Accuracy: 58.97%\n",
            "Epoch [4/10], Training Loss: 0.961, Validation Accuracy: 59.34%\n",
            "Epoch [5/10], Training Loss: 0.934, Validation Accuracy: 59.41%\n",
            "Epoch [6/10], Training Loss: 0.921, Validation Accuracy: 59.42%\n",
            "Epoch [7/10], Training Loss: 0.896, Validation Accuracy: 59.06%\n",
            "Epoch [8/10], Training Loss: 0.890, Validation Accuracy: 59.51%\n",
            "Epoch [9/10], Training Loss: 0.881, Validation Accuracy: 58.52%\n",
            "Epoch [10/10], Training Loss: 0.865, Validation Accuracy: 59.37%\n",
            "Epoch [1/10], Training Loss: 1.064, Validation Accuracy: 59.79%\n",
            "Epoch [2/10], Training Loss: 1.000, Validation Accuracy: 59.00%\n",
            "Epoch [3/10], Training Loss: 0.980, Validation Accuracy: 59.52%\n",
            "Epoch [4/10], Training Loss: 0.952, Validation Accuracy: 59.27%\n",
            "Epoch [5/10], Training Loss: 0.927, Validation Accuracy: 58.75%\n",
            "Epoch [6/10], Training Loss: 0.905, Validation Accuracy: 59.43%\n",
            "Epoch [7/10], Training Loss: 0.890, Validation Accuracy: 59.60%\n",
            "Epoch [8/10], Training Loss: 0.871, Validation Accuracy: 59.27%\n",
            "Epoch [9/10], Training Loss: 0.852, Validation Accuracy: 58.89%\n",
            "Epoch [10/10], Training Loss: 0.840, Validation Accuracy: 59.00%\n",
            "Epoch [1/10], Training Loss: 1.031, Validation Accuracy: 59.38%\n",
            "Epoch [2/10], Training Loss: 0.983, Validation Accuracy: 60.32%\n",
            "Epoch [3/10], Training Loss: 0.944, Validation Accuracy: 59.33%\n",
            "Epoch [4/10], Training Loss: 0.920, Validation Accuracy: 59.42%\n",
            "Epoch [5/10], Training Loss: 0.894, Validation Accuracy: 58.66%\n",
            "Epoch [6/10], Training Loss: 0.875, Validation Accuracy: 59.26%\n",
            "Epoch [7/10], Training Loss: 0.854, Validation Accuracy: 59.78%\n",
            "Epoch [8/10], Training Loss: 0.850, Validation Accuracy: 60.10%\n",
            "Epoch [9/10], Training Loss: 0.821, Validation Accuracy: 59.71%\n",
            "Epoch [10/10], Training Loss: 0.811, Validation Accuracy: 59.20%\n",
            "Epoch [1/10], Training Loss: 0.993, Validation Accuracy: 59.45%\n",
            "Epoch [2/10], Training Loss: 0.933, Validation Accuracy: 59.84%\n",
            "Epoch [3/10], Training Loss: 0.900, Validation Accuracy: 59.57%\n",
            "Epoch [4/10], Training Loss: 0.876, Validation Accuracy: 59.55%\n",
            "Epoch [5/10], Training Loss: 0.852, Validation Accuracy: 59.61%\n",
            "Epoch [6/10], Training Loss: 0.834, Validation Accuracy: 59.36%\n",
            "Epoch [7/10], Training Loss: 0.824, Validation Accuracy: 59.19%\n",
            "Epoch [8/10], Training Loss: 0.810, Validation Accuracy: 59.23%\n",
            "Epoch [9/10], Training Loss: 0.788, Validation Accuracy: 59.40%\n",
            "Epoch [10/10], Training Loss: 0.770, Validation Accuracy: 58.44%\n",
            "Epoch [1/10], Training Loss: 1.010, Validation Accuracy: 59.97%\n",
            "Epoch [2/10], Training Loss: 0.944, Validation Accuracy: 58.35%\n",
            "Epoch [3/10], Training Loss: 0.905, Validation Accuracy: 59.45%\n",
            "Epoch [4/10], Training Loss: 0.865, Validation Accuracy: 60.06%\n",
            "Epoch [5/10], Training Loss: 0.844, Validation Accuracy: 59.95%\n",
            "Epoch [6/10], Training Loss: 0.825, Validation Accuracy: 59.56%\n",
            "Epoch [7/10], Training Loss: 0.820, Validation Accuracy: 59.62%\n",
            "Epoch [8/10], Training Loss: 0.776, Validation Accuracy: 59.43%\n",
            "Epoch [9/10], Training Loss: 0.760, Validation Accuracy: 59.57%\n",
            "Epoch [10/10], Training Loss: 0.752, Validation Accuracy: 59.48%\n",
            "Epoch [1/10], Training Loss: 1.016, Validation Accuracy: 58.81%\n",
            "Epoch [2/10], Training Loss: 0.944, Validation Accuracy: 60.63%\n",
            "Epoch [3/10], Training Loss: 0.898, Validation Accuracy: 59.65%\n",
            "Epoch [4/10], Training Loss: 0.865, Validation Accuracy: 59.10%\n",
            "Epoch [5/10], Training Loss: 0.843, Validation Accuracy: 59.24%\n",
            "Epoch [6/10], Training Loss: 0.818, Validation Accuracy: 59.71%\n",
            "Epoch [7/10], Training Loss: 0.801, Validation Accuracy: 60.10%\n",
            "Epoch [8/10], Training Loss: 0.778, Validation Accuracy: 59.77%\n",
            "Epoch [9/10], Training Loss: 0.770, Validation Accuracy: 60.02%\n",
            "Epoch [10/10], Training Loss: 0.747, Validation Accuracy: 59.31%\n",
            "Epoch [1/10], Training Loss: 1.006, Validation Accuracy: 59.54%\n",
            "Epoch [2/10], Training Loss: 0.935, Validation Accuracy: 60.02%\n",
            "Epoch [3/10], Training Loss: 0.893, Validation Accuracy: 59.58%\n",
            "Epoch [4/10], Training Loss: 0.863, Validation Accuracy: 60.11%\n",
            "Epoch [5/10], Training Loss: 0.829, Validation Accuracy: 59.92%\n",
            "Epoch [6/10], Training Loss: 0.816, Validation Accuracy: 59.58%\n",
            "Epoch [7/10], Training Loss: 0.787, Validation Accuracy: 59.23%\n",
            "Epoch [8/10], Training Loss: 0.767, Validation Accuracy: 59.85%\n",
            "Epoch [9/10], Training Loss: 0.754, Validation Accuracy: 59.06%\n",
            "Epoch [10/10], Training Loss: 0.743, Validation Accuracy: 59.33%\n",
            "Epoch [1/10], Training Loss: 0.985, Validation Accuracy: 60.14%\n",
            "Epoch [2/10], Training Loss: 0.906, Validation Accuracy: 60.02%\n",
            "Epoch [3/10], Training Loss: 0.861, Validation Accuracy: 60.13%\n",
            "Epoch [4/10], Training Loss: 0.831, Validation Accuracy: 60.28%\n",
            "Epoch [5/10], Training Loss: 0.801, Validation Accuracy: 60.43%\n",
            "Epoch [6/10], Training Loss: 0.785, Validation Accuracy: 59.21%\n",
            "Epoch [7/10], Training Loss: 0.769, Validation Accuracy: 60.27%\n",
            "Epoch [8/10], Training Loss: 0.735, Validation Accuracy: 60.30%\n",
            "Epoch [9/10], Training Loss: 0.720, Validation Accuracy: 60.17%\n",
            "Epoch [10/10], Training Loss: 0.708, Validation Accuracy: 60.60%\n",
            "Epoch [1/10], Training Loss: 0.950, Validation Accuracy: 59.68%\n",
            "Epoch [2/10], Training Loss: 0.874, Validation Accuracy: 59.67%\n",
            "Epoch [3/10], Training Loss: 0.828, Validation Accuracy: 60.47%\n",
            "Epoch [4/10], Training Loss: 0.799, Validation Accuracy: 59.35%\n",
            "Epoch [5/10], Training Loss: 0.770, Validation Accuracy: 59.98%\n",
            "Epoch [6/10], Training Loss: 0.745, Validation Accuracy: 59.55%\n",
            "Epoch [7/10], Training Loss: 0.720, Validation Accuracy: 60.04%\n",
            "Epoch [8/10], Training Loss: 0.705, Validation Accuracy: 60.06%\n",
            "Epoch [9/10], Training Loss: 0.695, Validation Accuracy: 59.16%\n",
            "Epoch [10/10], Training Loss: 0.672, Validation Accuracy: 59.68%\n",
            "Epoch [1/10], Training Loss: 0.950, Validation Accuracy: 59.82%\n",
            "Epoch [2/10], Training Loss: 0.873, Validation Accuracy: 58.94%\n",
            "Epoch [3/10], Training Loss: 0.822, Validation Accuracy: 60.05%\n",
            "Epoch [4/10], Training Loss: 0.776, Validation Accuracy: 60.01%\n",
            "Epoch [5/10], Training Loss: 0.752, Validation Accuracy: 60.30%\n",
            "Epoch [6/10], Training Loss: 0.726, Validation Accuracy: 59.96%\n",
            "Epoch [7/10], Training Loss: 0.699, Validation Accuracy: 59.84%\n",
            "Epoch [8/10], Training Loss: 0.686, Validation Accuracy: 60.03%\n",
            "Epoch [9/10], Training Loss: 0.667, Validation Accuracy: 59.14%\n",
            "Epoch [10/10], Training Loss: 0.644, Validation Accuracy: 59.84%\n",
            "Epoch [1/10], Training Loss: 0.953, Validation Accuracy: 59.66%\n",
            "Epoch [2/10], Training Loss: 0.860, Validation Accuracy: 60.26%\n",
            "Epoch [3/10], Training Loss: 0.811, Validation Accuracy: 59.72%\n",
            "Epoch [4/10], Training Loss: 0.776, Validation Accuracy: 60.04%\n",
            "Epoch [5/10], Training Loss: 0.755, Validation Accuracy: 60.36%\n",
            "Epoch [6/10], Training Loss: 0.720, Validation Accuracy: 60.02%\n",
            "Epoch [7/10], Training Loss: 0.705, Validation Accuracy: 60.14%\n",
            "Epoch [8/10], Training Loss: 0.682, Validation Accuracy: 59.64%\n",
            "Epoch [9/10], Training Loss: 0.653, Validation Accuracy: 60.22%\n",
            "Epoch [10/10], Training Loss: 0.640, Validation Accuracy: 60.03%\n",
            "Epoch [1/10], Training Loss: 0.955, Validation Accuracy: 60.39%\n",
            "Epoch [2/10], Training Loss: 0.853, Validation Accuracy: 60.23%\n",
            "Epoch [3/10], Training Loss: 0.800, Validation Accuracy: 60.52%\n",
            "Epoch [4/10], Training Loss: 0.768, Validation Accuracy: 59.83%\n",
            "Epoch [5/10], Training Loss: 0.737, Validation Accuracy: 59.88%\n",
            "Epoch [6/10], Training Loss: 0.708, Validation Accuracy: 60.09%\n",
            "Epoch [7/10], Training Loss: 0.689, Validation Accuracy: 60.04%\n",
            "Epoch [8/10], Training Loss: 0.673, Validation Accuracy: 59.22%\n",
            "Epoch [9/10], Training Loss: 0.647, Validation Accuracy: 59.26%\n",
            "Epoch [10/10], Training Loss: 0.625, Validation Accuracy: 59.57%\n",
            "Epoch [1/10], Training Loss: 0.943, Validation Accuracy: 59.20%\n",
            "Epoch [2/10], Training Loss: 0.834, Validation Accuracy: 60.52%\n",
            "Epoch [3/10], Training Loss: 0.788, Validation Accuracy: 60.13%\n",
            "Epoch [4/10], Training Loss: 0.756, Validation Accuracy: 59.35%\n",
            "Epoch [5/10], Training Loss: 0.718, Validation Accuracy: 60.27%\n",
            "Epoch [6/10], Training Loss: 0.690, Validation Accuracy: 60.21%\n",
            "Epoch [7/10], Training Loss: 0.669, Validation Accuracy: 60.32%\n",
            "Epoch [8/10], Training Loss: 0.652, Validation Accuracy: 60.76%\n",
            "Epoch [9/10], Training Loss: 0.620, Validation Accuracy: 59.82%\n",
            "Epoch [10/10], Training Loss: 0.611, Validation Accuracy: 60.51%\n",
            "Epoch [1/10], Training Loss: 0.905, Validation Accuracy: 60.45%\n",
            "Epoch [2/10], Training Loss: 0.805, Validation Accuracy: 60.48%\n",
            "Epoch [3/10], Training Loss: 0.758, Validation Accuracy: 60.30%\n",
            "Epoch [4/10], Training Loss: 0.718, Validation Accuracy: 60.37%\n",
            "Epoch [5/10], Training Loss: 0.685, Validation Accuracy: 59.80%\n",
            "Epoch [6/10], Training Loss: 0.665, Validation Accuracy: 59.38%\n",
            "Epoch [7/10], Training Loss: 0.633, Validation Accuracy: 60.07%\n",
            "Epoch [8/10], Training Loss: 0.612, Validation Accuracy: 60.08%\n",
            "Epoch [9/10], Training Loss: 0.593, Validation Accuracy: 59.71%\n",
            "Epoch [10/10], Training Loss: 0.567, Validation Accuracy: 59.98%\n",
            "Epoch [1/10], Training Loss: 0.887, Validation Accuracy: 59.60%\n",
            "Epoch [2/10], Training Loss: 0.783, Validation Accuracy: 59.73%\n",
            "Epoch [3/10], Training Loss: 0.725, Validation Accuracy: 59.99%\n",
            "Epoch [4/10], Training Loss: 0.694, Validation Accuracy: 60.34%\n",
            "Epoch [5/10], Training Loss: 0.658, Validation Accuracy: 60.07%\n",
            "Epoch [6/10], Training Loss: 0.626, Validation Accuracy: 59.82%\n",
            "Epoch [7/10], Training Loss: 0.610, Validation Accuracy: 59.57%\n",
            "Epoch [8/10], Training Loss: 0.583, Validation Accuracy: 60.24%\n",
            "Epoch [9/10], Training Loss: 0.561, Validation Accuracy: 59.54%\n",
            "Epoch [10/10], Training Loss: 0.530, Validation Accuracy: 59.46%\n",
            "Epoch [1/10], Training Loss: 0.909, Validation Accuracy: 59.62%\n",
            "Epoch [2/10], Training Loss: 0.789, Validation Accuracy: 60.48%\n",
            "Epoch [3/10], Training Loss: 0.736, Validation Accuracy: 59.82%\n",
            "Epoch [4/10], Training Loss: 0.688, Validation Accuracy: 59.78%\n",
            "Epoch [5/10], Training Loss: 0.658, Validation Accuracy: 59.93%\n",
            "Epoch [6/10], Training Loss: 0.629, Validation Accuracy: 59.87%\n",
            "Epoch [7/10], Training Loss: 0.606, Validation Accuracy: 59.90%\n",
            "Epoch [8/10], Training Loss: 0.577, Validation Accuracy: 59.06%\n",
            "Epoch [9/10], Training Loss: 0.572, Validation Accuracy: 59.92%\n",
            "Epoch [10/10], Training Loss: 0.537, Validation Accuracy: 59.82%\n",
            "Epoch [1/10], Training Loss: 0.908, Validation Accuracy: 60.02%\n",
            "Epoch [2/10], Training Loss: 0.805, Validation Accuracy: 58.98%\n",
            "Epoch [3/10], Training Loss: 0.733, Validation Accuracy: 60.49%\n",
            "Epoch [4/10], Training Loss: 0.676, Validation Accuracy: 60.02%\n",
            "Epoch [5/10], Training Loss: 0.657, Validation Accuracy: 59.56%\n",
            "Epoch [6/10], Training Loss: 0.625, Validation Accuracy: 59.87%\n",
            "Epoch [7/10], Training Loss: 0.596, Validation Accuracy: 60.47%\n",
            "Epoch [8/10], Training Loss: 0.574, Validation Accuracy: 60.25%\n",
            "Epoch [9/10], Training Loss: 0.549, Validation Accuracy: 59.55%\n",
            "Epoch [10/10], Training Loss: 0.528, Validation Accuracy: 59.52%\n",
            "Epoch [1/10], Training Loss: 0.909, Validation Accuracy: 59.63%\n",
            "Epoch [2/10], Training Loss: 0.784, Validation Accuracy: 59.72%\n",
            "Epoch [3/10], Training Loss: 0.719, Validation Accuracy: 59.48%\n",
            "Epoch [4/10], Training Loss: 0.678, Validation Accuracy: 60.21%\n",
            "Epoch [5/10], Training Loss: 0.630, Validation Accuracy: 59.94%\n",
            "Epoch [6/10], Training Loss: 0.614, Validation Accuracy: 60.17%\n",
            "Epoch [7/10], Training Loss: 0.568, Validation Accuracy: 60.14%\n",
            "Epoch [8/10], Training Loss: 0.545, Validation Accuracy: 59.90%\n",
            "Epoch [9/10], Training Loss: 0.528, Validation Accuracy: 60.15%\n",
            "Epoch [10/10], Training Loss: 0.512, Validation Accuracy: 59.48%\n",
            "Epoch [1/10], Training Loss: 0.872, Validation Accuracy: 59.43%\n",
            "Epoch [2/10], Training Loss: 0.738, Validation Accuracy: 59.52%\n",
            "Epoch [3/10], Training Loss: 0.684, Validation Accuracy: 59.39%\n",
            "Epoch [4/10], Training Loss: 0.634, Validation Accuracy: 59.66%\n",
            "Epoch [5/10], Training Loss: 0.605, Validation Accuracy: 59.73%\n",
            "Epoch [6/10], Training Loss: 0.579, Validation Accuracy: 60.20%\n",
            "Epoch [7/10], Training Loss: 0.540, Validation Accuracy: 59.43%\n",
            "Epoch [8/10], Training Loss: 0.526, Validation Accuracy: 58.91%\n",
            "Epoch [9/10], Training Loss: 0.500, Validation Accuracy: 59.57%\n",
            "Epoch [10/10], Training Loss: 0.485, Validation Accuracy: 59.54%\n",
            "Epoch [1/10], Training Loss: 0.850, Validation Accuracy: 59.43%\n",
            "Epoch [2/10], Training Loss: 0.725, Validation Accuracy: 59.16%\n",
            "Epoch [3/10], Training Loss: 0.654, Validation Accuracy: 59.98%\n",
            "Epoch [4/10], Training Loss: 0.603, Validation Accuracy: 59.69%\n",
            "Epoch [5/10], Training Loss: 0.576, Validation Accuracy: 59.90%\n",
            "Epoch [6/10], Training Loss: 0.537, Validation Accuracy: 60.12%\n",
            "Epoch [7/10], Training Loss: 0.505, Validation Accuracy: 59.96%\n",
            "Epoch [8/10], Training Loss: 0.476, Validation Accuracy: 59.53%\n",
            "Epoch [9/10], Training Loss: 0.467, Validation Accuracy: 59.12%\n",
            "Epoch [10/10], Training Loss: 0.440, Validation Accuracy: 59.43%\n",
            "Epoch [1/10], Training Loss: 0.866, Validation Accuracy: 59.45%\n",
            "Epoch [2/10], Training Loss: 0.726, Validation Accuracy: 60.91%\n",
            "Epoch [3/10], Training Loss: 0.641, Validation Accuracy: 60.93%\n",
            "Epoch [4/10], Training Loss: 0.602, Validation Accuracy: 60.31%\n",
            "Epoch [5/10], Training Loss: 0.575, Validation Accuracy: 59.46%\n",
            "Epoch [6/10], Training Loss: 0.543, Validation Accuracy: 60.04%\n",
            "Epoch [7/10], Training Loss: 0.510, Validation Accuracy: 60.27%\n",
            "Epoch [8/10], Training Loss: 0.485, Validation Accuracy: 59.77%\n",
            "Epoch [9/10], Training Loss: 0.462, Validation Accuracy: 59.87%\n",
            "Epoch [10/10], Training Loss: 0.446, Validation Accuracy: 59.81%\n",
            "Epoch [1/10], Training Loss: 0.883, Validation Accuracy: 58.29%\n",
            "Epoch [2/10], Training Loss: 0.730, Validation Accuracy: 59.76%\n",
            "Epoch [3/10], Training Loss: 0.654, Validation Accuracy: 59.71%\n",
            "Epoch [4/10], Training Loss: 0.601, Validation Accuracy: 60.06%\n",
            "Epoch [5/10], Training Loss: 0.564, Validation Accuracy: 58.91%\n",
            "Epoch [6/10], Training Loss: 0.532, Validation Accuracy: 59.07%\n",
            "Epoch [7/10], Training Loss: 0.503, Validation Accuracy: 59.58%\n",
            "Epoch [8/10], Training Loss: 0.472, Validation Accuracy: 59.33%\n",
            "Epoch [9/10], Training Loss: 0.448, Validation Accuracy: 59.18%\n",
            "Epoch [10/10], Training Loss: 0.424, Validation Accuracy: 59.44%\n",
            "Epoch [1/10], Training Loss: 0.873, Validation Accuracy: 58.88%\n",
            "Epoch [2/10], Training Loss: 0.725, Validation Accuracy: 60.13%\n",
            "Epoch [3/10], Training Loss: 0.645, Validation Accuracy: 60.12%\n",
            "Epoch [4/10], Training Loss: 0.593, Validation Accuracy: 59.79%\n",
            "Epoch [5/10], Training Loss: 0.550, Validation Accuracy: 60.22%\n",
            "Epoch [6/10], Training Loss: 0.511, Validation Accuracy: 59.56%\n",
            "Epoch [7/10], Training Loss: 0.480, Validation Accuracy: 59.91%\n",
            "Epoch [8/10], Training Loss: 0.461, Validation Accuracy: 59.87%\n",
            "Epoch [9/10], Training Loss: 0.429, Validation Accuracy: 59.58%\n",
            "Epoch [10/10], Training Loss: 0.413, Validation Accuracy: 59.06%\n",
            "Epoch [1/10], Training Loss: 0.829, Validation Accuracy: 59.51%\n",
            "Epoch [2/10], Training Loss: 0.692, Validation Accuracy: 59.39%\n",
            "Epoch [3/10], Training Loss: 0.607, Validation Accuracy: 58.96%\n",
            "Epoch [4/10], Training Loss: 0.547, Validation Accuracy: 59.29%\n",
            "Epoch [5/10], Training Loss: 0.509, Validation Accuracy: 59.20%\n",
            "Epoch [6/10], Training Loss: 0.481, Validation Accuracy: 59.39%\n",
            "Epoch [7/10], Training Loss: 0.453, Validation Accuracy: 59.19%\n",
            "Epoch [8/10], Training Loss: 0.433, Validation Accuracy: 59.30%\n",
            "Epoch [9/10], Training Loss: 0.405, Validation Accuracy: 58.94%\n",
            "Epoch [10/10], Training Loss: 0.385, Validation Accuracy: 58.51%\n",
            "Epoch [1/10], Training Loss: 0.826, Validation Accuracy: 58.73%\n",
            "Epoch [2/10], Training Loss: 0.651, Validation Accuracy: 59.61%\n",
            "Epoch [3/10], Training Loss: 0.581, Validation Accuracy: 59.29%\n",
            "Epoch [4/10], Training Loss: 0.523, Validation Accuracy: 59.93%\n",
            "Epoch [5/10], Training Loss: 0.480, Validation Accuracy: 59.14%\n",
            "Epoch [6/10], Training Loss: 0.460, Validation Accuracy: 59.60%\n",
            "Epoch [7/10], Training Loss: 0.422, Validation Accuracy: 59.73%\n",
            "Epoch [8/10], Training Loss: 0.394, Validation Accuracy: 59.89%\n",
            "Epoch [9/10], Training Loss: 0.377, Validation Accuracy: 59.01%\n",
            "Epoch [10/10], Training Loss: 0.359, Validation Accuracy: 58.99%\n",
            "Confusion Matrix:\n",
            "[[667  20  76  27  24  12  23  13  77  61]\n",
            " [ 60 596  28  18  10  11  19  11  71 176]\n",
            " [ 81   8 481  91 108  79  62  53  27  10]\n",
            " [ 27   8 102 397  60 216  77  66  20  27]\n",
            " [ 38   3 148  55 494  78  76  88  17   3]\n",
            " [ 25   1  79 184  44 528  31  93   6   9]\n",
            " [  7   5  80  83  60  36 691  14   9  15]\n",
            " [ 19   3  43  52  83  80  19 668   6  27]\n",
            " [ 91  28  20  20  10   8   8   8 752  55]\n",
            " [ 57  81  24  19  17  16  16  41  62 667]]\n",
            "Test Accuracy: 59.41%\n",
            "True Positives (TP): [667 596 481 397 494 528 691 668 752 667]\n",
            "False Positives (FP): [405 157 600 549 416 536 331 387 295 383]\n",
            "True Negatives (TN): [8595 8843 8400 8451 8584 8464 8669 8613 8705 8617]\n",
            "False Negatives (FN): [333 404 519 603 506 472 309 332 248 333]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.62220149 0.79150066 0.44495837 0.41966173 0.54285714 0.4962406\n",
            " 0.67612524 0.63317536 0.7182426  0.6352381 ]\n",
            "Recall: [0.667 0.596 0.481 0.397 0.494 0.528 0.691 0.668 0.752 0.667]\n",
            "F1 Score: [0.64382239 0.67997718 0.46227775 0.40801644 0.51727749 0.51162791\n",
            " 0.6834817  0.65012165 0.73473376 0.65073171]\n",
            "CPU times: user 3h 33min 39s, sys: 1min 32s, total: 3h 35min 12s\n",
            "Wall time: 3h 52min 35s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 0.5):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=0.5):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_uniform: Dict ) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "    mean = distribution_info_uniform[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info_uniform[\"std\"].mean().item()  # Convert numpy array to float\n",
        "\n",
        "\n",
        "\n",
        "     # Generate augmented data using Uniform distribution\n",
        "    augmented_data_uniform = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = augmented_data_uniform\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=0.5)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"uniform\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcxCchEGRsXh"
      },
      "source": [
        "beta=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Caxr01-_Ruj0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-u54XnoRyu3",
        "outputId": "626c583e-6dc5-4a9d-caf6-6b900e668d52"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 41.8MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Images per Class: [6086 5913 6119 5916 5918 6020 6030 5955 6083 5960]\n",
            "Epoch [1/10], Training Loss: 2.305, Validation Accuracy: 9.58%\n",
            "Epoch [2/10], Training Loss: 2.304, Validation Accuracy: 9.58%\n",
            "Epoch [3/10], Training Loss: 2.303, Validation Accuracy: 9.57%\n",
            "Epoch [4/10], Training Loss: 2.303, Validation Accuracy: 9.57%\n",
            "Epoch [5/10], Training Loss: 2.302, Validation Accuracy: 9.57%\n",
            "Epoch [6/10], Training Loss: 2.301, Validation Accuracy: 9.54%\n",
            "Epoch [7/10], Training Loss: 2.300, Validation Accuracy: 9.87%\n",
            "Epoch [8/10], Training Loss: 2.299, Validation Accuracy: 10.43%\n",
            "Epoch [9/10], Training Loss: 2.298, Validation Accuracy: 11.12%\n",
            "Epoch [10/10], Training Loss: 2.297, Validation Accuracy: 11.83%\n",
            "Epoch [1/10], Training Loss: 2.296, Validation Accuracy: 11.73%\n",
            "Epoch [2/10], Training Loss: 2.294, Validation Accuracy: 12.32%\n",
            "Epoch [3/10], Training Loss: 2.291, Validation Accuracy: 13.32%\n",
            "Epoch [4/10], Training Loss: 2.287, Validation Accuracy: 13.89%\n",
            "Epoch [5/10], Training Loss: 2.282, Validation Accuracy: 14.10%\n",
            "Epoch [6/10], Training Loss: 2.273, Validation Accuracy: 15.28%\n",
            "Epoch [7/10], Training Loss: 2.261, Validation Accuracy: 17.53%\n",
            "Epoch [8/10], Training Loss: 2.243, Validation Accuracy: 18.82%\n",
            "Epoch [9/10], Training Loss: 2.221, Validation Accuracy: 19.33%\n",
            "Epoch [10/10], Training Loss: 2.196, Validation Accuracy: 20.70%\n",
            "Epoch [1/10], Training Loss: 2.166, Validation Accuracy: 22.54%\n",
            "Epoch [2/10], Training Loss: 2.132, Validation Accuracy: 23.48%\n",
            "Epoch [3/10], Training Loss: 2.094, Validation Accuracy: 24.59%\n",
            "Epoch [4/10], Training Loss: 2.058, Validation Accuracy: 25.69%\n",
            "Epoch [5/10], Training Loss: 2.026, Validation Accuracy: 26.74%\n",
            "Epoch [6/10], Training Loss: 2.000, Validation Accuracy: 27.42%\n",
            "Epoch [7/10], Training Loss: 1.980, Validation Accuracy: 27.74%\n",
            "Epoch [8/10], Training Loss: 1.960, Validation Accuracy: 28.54%\n",
            "Epoch [9/10], Training Loss: 1.942, Validation Accuracy: 29.40%\n",
            "Epoch [10/10], Training Loss: 1.924, Validation Accuracy: 29.94%\n",
            "Epoch [1/10], Training Loss: 1.918, Validation Accuracy: 30.21%\n",
            "Epoch [2/10], Training Loss: 1.901, Validation Accuracy: 31.41%\n",
            "Epoch [3/10], Training Loss: 1.884, Validation Accuracy: 31.21%\n",
            "Epoch [4/10], Training Loss: 1.868, Validation Accuracy: 32.58%\n",
            "Epoch [5/10], Training Loss: 1.849, Validation Accuracy: 33.09%\n",
            "Epoch [6/10], Training Loss: 1.830, Validation Accuracy: 33.50%\n",
            "Epoch [7/10], Training Loss: 1.812, Validation Accuracy: 34.33%\n",
            "Epoch [8/10], Training Loss: 1.791, Validation Accuracy: 34.75%\n",
            "Epoch [9/10], Training Loss: 1.774, Validation Accuracy: 34.98%\n",
            "Epoch [10/10], Training Loss: 1.756, Validation Accuracy: 35.58%\n",
            "Epoch [1/10], Training Loss: 1.754, Validation Accuracy: 36.22%\n",
            "Epoch [2/10], Training Loss: 1.731, Validation Accuracy: 36.50%\n",
            "Epoch [3/10], Training Loss: 1.714, Validation Accuracy: 36.67%\n",
            "Epoch [4/10], Training Loss: 1.707, Validation Accuracy: 37.29%\n",
            "Epoch [5/10], Training Loss: 1.687, Validation Accuracy: 36.45%\n",
            "Epoch [6/10], Training Loss: 1.682, Validation Accuracy: 37.71%\n",
            "Epoch [7/10], Training Loss: 1.661, Validation Accuracy: 37.44%\n",
            "Epoch [8/10], Training Loss: 1.654, Validation Accuracy: 38.29%\n",
            "Epoch [9/10], Training Loss: 1.644, Validation Accuracy: 38.60%\n",
            "Epoch [10/10], Training Loss: 1.638, Validation Accuracy: 38.30%\n",
            "Epoch [1/10], Training Loss: 1.654, Validation Accuracy: 39.08%\n",
            "Epoch [2/10], Training Loss: 1.638, Validation Accuracy: 38.91%\n",
            "Epoch [3/10], Training Loss: 1.633, Validation Accuracy: 39.59%\n",
            "Epoch [4/10], Training Loss: 1.618, Validation Accuracy: 39.83%\n",
            "Epoch [5/10], Training Loss: 1.605, Validation Accuracy: 40.21%\n",
            "Epoch [6/10], Training Loss: 1.595, Validation Accuracy: 40.97%\n",
            "Epoch [7/10], Training Loss: 1.586, Validation Accuracy: 40.27%\n",
            "Epoch [8/10], Training Loss: 1.578, Validation Accuracy: 41.31%\n",
            "Epoch [9/10], Training Loss: 1.574, Validation Accuracy: 41.59%\n",
            "Epoch [10/10], Training Loss: 1.567, Validation Accuracy: 40.97%\n",
            "Epoch [1/10], Training Loss: 1.592, Validation Accuracy: 41.51%\n",
            "Epoch [2/10], Training Loss: 1.589, Validation Accuracy: 41.14%\n",
            "Epoch [3/10], Training Loss: 1.568, Validation Accuracy: 41.05%\n",
            "Epoch [4/10], Training Loss: 1.557, Validation Accuracy: 42.18%\n",
            "Epoch [5/10], Training Loss: 1.554, Validation Accuracy: 42.69%\n",
            "Epoch [6/10], Training Loss: 1.544, Validation Accuracy: 42.69%\n",
            "Epoch [7/10], Training Loss: 1.539, Validation Accuracy: 42.23%\n",
            "Epoch [8/10], Training Loss: 1.528, Validation Accuracy: 42.94%\n",
            "Epoch [9/10], Training Loss: 1.524, Validation Accuracy: 42.98%\n",
            "Epoch [10/10], Training Loss: 1.512, Validation Accuracy: 43.48%\n",
            "Epoch [1/10], Training Loss: 1.541, Validation Accuracy: 43.89%\n",
            "Epoch [2/10], Training Loss: 1.529, Validation Accuracy: 44.20%\n",
            "Epoch [3/10], Training Loss: 1.518, Validation Accuracy: 44.94%\n",
            "Epoch [4/10], Training Loss: 1.508, Validation Accuracy: 44.77%\n",
            "Epoch [5/10], Training Loss: 1.499, Validation Accuracy: 44.33%\n",
            "Epoch [6/10], Training Loss: 1.490, Validation Accuracy: 44.27%\n",
            "Epoch [7/10], Training Loss: 1.487, Validation Accuracy: 45.00%\n",
            "Epoch [8/10], Training Loss: 1.473, Validation Accuracy: 45.96%\n",
            "Epoch [9/10], Training Loss: 1.462, Validation Accuracy: 45.59%\n",
            "Epoch [10/10], Training Loss: 1.457, Validation Accuracy: 45.76%\n",
            "Epoch [1/10], Training Loss: 1.492, Validation Accuracy: 46.11%\n",
            "Epoch [2/10], Training Loss: 1.478, Validation Accuracy: 46.39%\n",
            "Epoch [3/10], Training Loss: 1.469, Validation Accuracy: 46.17%\n",
            "Epoch [4/10], Training Loss: 1.463, Validation Accuracy: 46.36%\n",
            "Epoch [5/10], Training Loss: 1.452, Validation Accuracy: 46.94%\n",
            "Epoch [6/10], Training Loss: 1.439, Validation Accuracy: 46.69%\n",
            "Epoch [7/10], Training Loss: 1.433, Validation Accuracy: 46.74%\n",
            "Epoch [8/10], Training Loss: 1.433, Validation Accuracy: 47.31%\n",
            "Epoch [9/10], Training Loss: 1.425, Validation Accuracy: 47.43%\n",
            "Epoch [10/10], Training Loss: 1.413, Validation Accuracy: 47.21%\n",
            "Epoch [1/10], Training Loss: 1.450, Validation Accuracy: 47.53%\n",
            "Epoch [2/10], Training Loss: 1.426, Validation Accuracy: 47.78%\n",
            "Epoch [3/10], Training Loss: 1.417, Validation Accuracy: 47.33%\n",
            "Epoch [4/10], Training Loss: 1.411, Validation Accuracy: 46.99%\n",
            "Epoch [5/10], Training Loss: 1.400, Validation Accuracy: 48.16%\n",
            "Epoch [6/10], Training Loss: 1.392, Validation Accuracy: 47.43%\n",
            "Epoch [7/10], Training Loss: 1.381, Validation Accuracy: 48.30%\n",
            "Epoch [8/10], Training Loss: 1.373, Validation Accuracy: 47.86%\n",
            "Epoch [9/10], Training Loss: 1.362, Validation Accuracy: 47.68%\n",
            "Epoch [10/10], Training Loss: 1.359, Validation Accuracy: 48.88%\n",
            "Epoch [1/10], Training Loss: 1.420, Validation Accuracy: 49.10%\n",
            "Epoch [2/10], Training Loss: 1.398, Validation Accuracy: 48.39%\n",
            "Epoch [3/10], Training Loss: 1.386, Validation Accuracy: 48.29%\n",
            "Epoch [4/10], Training Loss: 1.388, Validation Accuracy: 49.14%\n",
            "Epoch [5/10], Training Loss: 1.363, Validation Accuracy: 48.67%\n",
            "Epoch [6/10], Training Loss: 1.359, Validation Accuracy: 49.22%\n",
            "Epoch [7/10], Training Loss: 1.348, Validation Accuracy: 49.05%\n",
            "Epoch [8/10], Training Loss: 1.346, Validation Accuracy: 49.67%\n",
            "Epoch [9/10], Training Loss: 1.330, Validation Accuracy: 49.21%\n",
            "Epoch [10/10], Training Loss: 1.322, Validation Accuracy: 48.87%\n",
            "Epoch [1/10], Training Loss: 1.389, Validation Accuracy: 49.67%\n",
            "Epoch [2/10], Training Loss: 1.367, Validation Accuracy: 50.00%\n",
            "Epoch [3/10], Training Loss: 1.355, Validation Accuracy: 49.12%\n",
            "Epoch [4/10], Training Loss: 1.340, Validation Accuracy: 50.21%\n",
            "Epoch [5/10], Training Loss: 1.327, Validation Accuracy: 50.23%\n",
            "Epoch [6/10], Training Loss: 1.323, Validation Accuracy: 49.99%\n",
            "Epoch [7/10], Training Loss: 1.309, Validation Accuracy: 49.80%\n",
            "Epoch [8/10], Training Loss: 1.301, Validation Accuracy: 50.69%\n",
            "Epoch [9/10], Training Loss: 1.290, Validation Accuracy: 50.62%\n",
            "Epoch [10/10], Training Loss: 1.283, Validation Accuracy: 50.14%\n",
            "Epoch [1/10], Training Loss: 1.354, Validation Accuracy: 51.06%\n",
            "Epoch [2/10], Training Loss: 1.332, Validation Accuracy: 50.98%\n",
            "Epoch [3/10], Training Loss: 1.318, Validation Accuracy: 51.53%\n",
            "Epoch [4/10], Training Loss: 1.307, Validation Accuracy: 50.61%\n",
            "Epoch [5/10], Training Loss: 1.294, Validation Accuracy: 51.32%\n",
            "Epoch [6/10], Training Loss: 1.291, Validation Accuracy: 51.22%\n",
            "Epoch [7/10], Training Loss: 1.273, Validation Accuracy: 51.44%\n",
            "Epoch [8/10], Training Loss: 1.266, Validation Accuracy: 51.93%\n",
            "Epoch [9/10], Training Loss: 1.253, Validation Accuracy: 52.07%\n",
            "Epoch [10/10], Training Loss: 1.247, Validation Accuracy: 51.76%\n",
            "Epoch [1/10], Training Loss: 1.329, Validation Accuracy: 52.55%\n",
            "Epoch [2/10], Training Loss: 1.304, Validation Accuracy: 52.64%\n",
            "Epoch [3/10], Training Loss: 1.292, Validation Accuracy: 52.02%\n",
            "Epoch [4/10], Training Loss: 1.280, Validation Accuracy: 51.72%\n",
            "Epoch [5/10], Training Loss: 1.265, Validation Accuracy: 52.80%\n",
            "Epoch [6/10], Training Loss: 1.260, Validation Accuracy: 52.72%\n",
            "Epoch [7/10], Training Loss: 1.245, Validation Accuracy: 52.82%\n",
            "Epoch [8/10], Training Loss: 1.244, Validation Accuracy: 52.19%\n",
            "Epoch [9/10], Training Loss: 1.235, Validation Accuracy: 52.67%\n",
            "Epoch [10/10], Training Loss: 1.211, Validation Accuracy: 53.34%\n",
            "Epoch [1/10], Training Loss: 1.284, Validation Accuracy: 53.32%\n",
            "Epoch [2/10], Training Loss: 1.264, Validation Accuracy: 53.00%\n",
            "Epoch [3/10], Training Loss: 1.243, Validation Accuracy: 52.76%\n",
            "Epoch [4/10], Training Loss: 1.235, Validation Accuracy: 53.15%\n",
            "Epoch [5/10], Training Loss: 1.226, Validation Accuracy: 52.81%\n",
            "Epoch [6/10], Training Loss: 1.208, Validation Accuracy: 54.42%\n",
            "Epoch [7/10], Training Loss: 1.196, Validation Accuracy: 53.39%\n",
            "Epoch [8/10], Training Loss: 1.188, Validation Accuracy: 53.23%\n",
            "Epoch [9/10], Training Loss: 1.174, Validation Accuracy: 54.02%\n",
            "Epoch [10/10], Training Loss: 1.163, Validation Accuracy: 53.68%\n",
            "Epoch [1/10], Training Loss: 1.265, Validation Accuracy: 54.26%\n",
            "Epoch [2/10], Training Loss: 1.238, Validation Accuracy: 54.27%\n",
            "Epoch [3/10], Training Loss: 1.221, Validation Accuracy: 54.48%\n",
            "Epoch [4/10], Training Loss: 1.196, Validation Accuracy: 54.75%\n",
            "Epoch [5/10], Training Loss: 1.185, Validation Accuracy: 54.35%\n",
            "Epoch [6/10], Training Loss: 1.176, Validation Accuracy: 54.82%\n",
            "Epoch [7/10], Training Loss: 1.165, Validation Accuracy: 54.59%\n",
            "Epoch [8/10], Training Loss: 1.150, Validation Accuracy: 54.32%\n",
            "Epoch [9/10], Training Loss: 1.143, Validation Accuracy: 54.90%\n",
            "Epoch [10/10], Training Loss: 1.135, Validation Accuracy: 55.00%\n",
            "Epoch [1/10], Training Loss: 1.238, Validation Accuracy: 55.43%\n",
            "Epoch [2/10], Training Loss: 1.206, Validation Accuracy: 55.07%\n",
            "Epoch [3/10], Training Loss: 1.191, Validation Accuracy: 55.28%\n",
            "Epoch [4/10], Training Loss: 1.166, Validation Accuracy: 55.38%\n",
            "Epoch [5/10], Training Loss: 1.160, Validation Accuracy: 55.57%\n",
            "Epoch [6/10], Training Loss: 1.142, Validation Accuracy: 54.55%\n",
            "Epoch [7/10], Training Loss: 1.131, Validation Accuracy: 54.14%\n",
            "Epoch [8/10], Training Loss: 1.127, Validation Accuracy: 55.21%\n",
            "Epoch [9/10], Training Loss: 1.118, Validation Accuracy: 54.29%\n",
            "Epoch [10/10], Training Loss: 1.100, Validation Accuracy: 55.55%\n",
            "Epoch [1/10], Training Loss: 1.206, Validation Accuracy: 56.18%\n",
            "Epoch [2/10], Training Loss: 1.175, Validation Accuracy: 55.97%\n",
            "Epoch [3/10], Training Loss: 1.165, Validation Accuracy: 55.82%\n",
            "Epoch [4/10], Training Loss: 1.144, Validation Accuracy: 56.24%\n",
            "Epoch [5/10], Training Loss: 1.129, Validation Accuracy: 56.42%\n",
            "Epoch [6/10], Training Loss: 1.109, Validation Accuracy: 56.26%\n",
            "Epoch [7/10], Training Loss: 1.100, Validation Accuracy: 56.45%\n",
            "Epoch [8/10], Training Loss: 1.100, Validation Accuracy: 55.92%\n",
            "Epoch [9/10], Training Loss: 1.077, Validation Accuracy: 56.62%\n",
            "Epoch [10/10], Training Loss: 1.069, Validation Accuracy: 55.96%\n",
            "Epoch [1/10], Training Loss: 1.203, Validation Accuracy: 55.61%\n",
            "Epoch [2/10], Training Loss: 1.169, Validation Accuracy: 56.07%\n",
            "Epoch [3/10], Training Loss: 1.145, Validation Accuracy: 56.19%\n",
            "Epoch [4/10], Training Loss: 1.136, Validation Accuracy: 56.22%\n",
            "Epoch [5/10], Training Loss: 1.115, Validation Accuracy: 56.33%\n",
            "Epoch [6/10], Training Loss: 1.106, Validation Accuracy: 56.51%\n",
            "Epoch [7/10], Training Loss: 1.105, Validation Accuracy: 56.79%\n",
            "Epoch [8/10], Training Loss: 1.078, Validation Accuracy: 56.35%\n",
            "Epoch [9/10], Training Loss: 1.063, Validation Accuracy: 56.84%\n",
            "Epoch [10/10], Training Loss: 1.064, Validation Accuracy: 57.09%\n",
            "Epoch [1/10], Training Loss: 1.155, Validation Accuracy: 57.11%\n",
            "Epoch [2/10], Training Loss: 1.124, Validation Accuracy: 56.68%\n",
            "Epoch [3/10], Training Loss: 1.108, Validation Accuracy: 56.88%\n",
            "Epoch [4/10], Training Loss: 1.082, Validation Accuracy: 57.28%\n",
            "Epoch [5/10], Training Loss: 1.074, Validation Accuracy: 57.28%\n",
            "Epoch [6/10], Training Loss: 1.057, Validation Accuracy: 57.02%\n",
            "Epoch [7/10], Training Loss: 1.057, Validation Accuracy: 56.43%\n",
            "Epoch [8/10], Training Loss: 1.036, Validation Accuracy: 57.17%\n",
            "Epoch [9/10], Training Loss: 1.022, Validation Accuracy: 57.00%\n",
            "Epoch [10/10], Training Loss: 1.013, Validation Accuracy: 56.18%\n",
            "Epoch [1/10], Training Loss: 1.149, Validation Accuracy: 56.74%\n",
            "Epoch [2/10], Training Loss: 1.113, Validation Accuracy: 57.35%\n",
            "Epoch [3/10], Training Loss: 1.098, Validation Accuracy: 57.55%\n",
            "Epoch [4/10], Training Loss: 1.075, Validation Accuracy: 57.77%\n",
            "Epoch [5/10], Training Loss: 1.056, Validation Accuracy: 57.07%\n",
            "Epoch [6/10], Training Loss: 1.043, Validation Accuracy: 57.65%\n",
            "Epoch [7/10], Training Loss: 1.027, Validation Accuracy: 57.48%\n",
            "Epoch [8/10], Training Loss: 1.017, Validation Accuracy: 57.52%\n",
            "Epoch [9/10], Training Loss: 1.006, Validation Accuracy: 57.79%\n",
            "Epoch [10/10], Training Loss: 0.987, Validation Accuracy: 57.34%\n",
            "Epoch [1/10], Training Loss: 1.128, Validation Accuracy: 57.64%\n",
            "Epoch [2/10], Training Loss: 1.084, Validation Accuracy: 58.09%\n",
            "Epoch [3/10], Training Loss: 1.063, Validation Accuracy: 57.46%\n",
            "Epoch [4/10], Training Loss: 1.047, Validation Accuracy: 58.12%\n",
            "Epoch [5/10], Training Loss: 1.030, Validation Accuracy: 57.83%\n",
            "Epoch [6/10], Training Loss: 1.017, Validation Accuracy: 57.78%\n",
            "Epoch [7/10], Training Loss: 0.996, Validation Accuracy: 57.96%\n",
            "Epoch [8/10], Training Loss: 0.989, Validation Accuracy: 57.31%\n",
            "Epoch [9/10], Training Loss: 0.968, Validation Accuracy: 58.18%\n",
            "Epoch [10/10], Training Loss: 0.958, Validation Accuracy: 57.25%\n",
            "Epoch [1/10], Training Loss: 1.108, Validation Accuracy: 58.61%\n",
            "Epoch [2/10], Training Loss: 1.064, Validation Accuracy: 57.99%\n",
            "Epoch [3/10], Training Loss: 1.038, Validation Accuracy: 58.69%\n",
            "Epoch [4/10], Training Loss: 1.023, Validation Accuracy: 58.00%\n",
            "Epoch [5/10], Training Loss: 1.007, Validation Accuracy: 57.85%\n",
            "Epoch [6/10], Training Loss: 0.989, Validation Accuracy: 58.39%\n",
            "Epoch [7/10], Training Loss: 0.972, Validation Accuracy: 58.09%\n",
            "Epoch [8/10], Training Loss: 0.968, Validation Accuracy: 58.22%\n",
            "Epoch [9/10], Training Loss: 0.942, Validation Accuracy: 58.01%\n",
            "Epoch [10/10], Training Loss: 0.938, Validation Accuracy: 57.83%\n",
            "Epoch [1/10], Training Loss: 1.112, Validation Accuracy: 56.79%\n",
            "Epoch [2/10], Training Loss: 1.071, Validation Accuracy: 58.41%\n",
            "Epoch [3/10], Training Loss: 1.039, Validation Accuracy: 58.36%\n",
            "Epoch [4/10], Training Loss: 1.014, Validation Accuracy: 58.23%\n",
            "Epoch [5/10], Training Loss: 0.999, Validation Accuracy: 57.82%\n",
            "Epoch [6/10], Training Loss: 0.985, Validation Accuracy: 58.28%\n",
            "Epoch [7/10], Training Loss: 0.972, Validation Accuracy: 58.33%\n",
            "Epoch [8/10], Training Loss: 0.965, Validation Accuracy: 58.72%\n",
            "Epoch [9/10], Training Loss: 0.942, Validation Accuracy: 58.53%\n",
            "Epoch [10/10], Training Loss: 0.930, Validation Accuracy: 58.31%\n",
            "Epoch [1/10], Training Loss: 1.087, Validation Accuracy: 57.05%\n",
            "Epoch [2/10], Training Loss: 1.034, Validation Accuracy: 58.38%\n",
            "Epoch [3/10], Training Loss: 0.998, Validation Accuracy: 58.51%\n",
            "Epoch [4/10], Training Loss: 0.975, Validation Accuracy: 58.36%\n",
            "Epoch [5/10], Training Loss: 0.964, Validation Accuracy: 58.72%\n",
            "Epoch [6/10], Training Loss: 0.934, Validation Accuracy: 58.86%\n",
            "Epoch [7/10], Training Loss: 0.923, Validation Accuracy: 58.48%\n",
            "Epoch [8/10], Training Loss: 0.908, Validation Accuracy: 58.18%\n",
            "Epoch [9/10], Training Loss: 0.902, Validation Accuracy: 58.17%\n",
            "Epoch [10/10], Training Loss: 0.890, Validation Accuracy: 58.86%\n",
            "Epoch [1/10], Training Loss: 1.076, Validation Accuracy: 58.53%\n",
            "Epoch [2/10], Training Loss: 1.021, Validation Accuracy: 59.33%\n",
            "Epoch [3/10], Training Loss: 1.005, Validation Accuracy: 58.02%\n",
            "Epoch [4/10], Training Loss: 0.970, Validation Accuracy: 59.11%\n",
            "Epoch [5/10], Training Loss: 0.944, Validation Accuracy: 58.89%\n",
            "Epoch [6/10], Training Loss: 0.930, Validation Accuracy: 58.88%\n",
            "Epoch [7/10], Training Loss: 0.905, Validation Accuracy: 58.79%\n",
            "Epoch [8/10], Training Loss: 0.897, Validation Accuracy: 58.24%\n",
            "Epoch [9/10], Training Loss: 0.886, Validation Accuracy: 59.56%\n",
            "Epoch [10/10], Training Loss: 0.868, Validation Accuracy: 59.28%\n",
            "Epoch [1/10], Training Loss: 1.046, Validation Accuracy: 58.60%\n",
            "Epoch [2/10], Training Loss: 1.007, Validation Accuracy: 59.43%\n",
            "Epoch [3/10], Training Loss: 0.965, Validation Accuracy: 59.91%\n",
            "Epoch [4/10], Training Loss: 0.944, Validation Accuracy: 59.57%\n",
            "Epoch [5/10], Training Loss: 0.927, Validation Accuracy: 59.74%\n",
            "Epoch [6/10], Training Loss: 0.903, Validation Accuracy: 59.07%\n",
            "Epoch [7/10], Training Loss: 0.886, Validation Accuracy: 59.51%\n",
            "Epoch [8/10], Training Loss: 0.874, Validation Accuracy: 58.41%\n",
            "Epoch [9/10], Training Loss: 0.856, Validation Accuracy: 59.78%\n",
            "Epoch [10/10], Training Loss: 0.844, Validation Accuracy: 57.24%\n",
            "Epoch [1/10], Training Loss: 1.038, Validation Accuracy: 60.21%\n",
            "Epoch [2/10], Training Loss: 0.987, Validation Accuracy: 59.27%\n",
            "Epoch [3/10], Training Loss: 0.949, Validation Accuracy: 59.66%\n",
            "Epoch [4/10], Training Loss: 0.922, Validation Accuracy: 58.15%\n",
            "Epoch [5/10], Training Loss: 0.912, Validation Accuracy: 59.66%\n",
            "Epoch [6/10], Training Loss: 0.891, Validation Accuracy: 59.93%\n",
            "Epoch [7/10], Training Loss: 0.870, Validation Accuracy: 59.63%\n",
            "Epoch [8/10], Training Loss: 0.848, Validation Accuracy: 59.75%\n",
            "Epoch [9/10], Training Loss: 0.837, Validation Accuracy: 59.32%\n",
            "Epoch [10/10], Training Loss: 0.828, Validation Accuracy: 59.11%\n",
            "Epoch [1/10], Training Loss: 1.044, Validation Accuracy: 59.64%\n",
            "Epoch [2/10], Training Loss: 0.980, Validation Accuracy: 60.08%\n",
            "Epoch [3/10], Training Loss: 0.941, Validation Accuracy: 60.03%\n",
            "Epoch [4/10], Training Loss: 0.917, Validation Accuracy: 60.37%\n",
            "Epoch [5/10], Training Loss: 0.894, Validation Accuracy: 59.85%\n",
            "Epoch [6/10], Training Loss: 0.882, Validation Accuracy: 59.90%\n",
            "Epoch [7/10], Training Loss: 0.861, Validation Accuracy: 59.63%\n",
            "Epoch [8/10], Training Loss: 0.840, Validation Accuracy: 59.52%\n",
            "Epoch [9/10], Training Loss: 0.839, Validation Accuracy: 59.82%\n",
            "Epoch [10/10], Training Loss: 0.815, Validation Accuracy: 59.43%\n",
            "Epoch [1/10], Training Loss: 0.999, Validation Accuracy: 59.65%\n",
            "Epoch [2/10], Training Loss: 0.948, Validation Accuracy: 59.97%\n",
            "Epoch [3/10], Training Loss: 0.907, Validation Accuracy: 59.42%\n",
            "Epoch [4/10], Training Loss: 0.877, Validation Accuracy: 59.85%\n",
            "Epoch [5/10], Training Loss: 0.854, Validation Accuracy: 59.85%\n",
            "Epoch [6/10], Training Loss: 0.838, Validation Accuracy: 59.67%\n",
            "Epoch [7/10], Training Loss: 0.830, Validation Accuracy: 59.95%\n",
            "Epoch [8/10], Training Loss: 0.801, Validation Accuracy: 59.75%\n",
            "Epoch [9/10], Training Loss: 0.790, Validation Accuracy: 59.57%\n",
            "Epoch [10/10], Training Loss: 0.765, Validation Accuracy: 59.55%\n",
            "Epoch [1/10], Training Loss: 1.023, Validation Accuracy: 59.21%\n",
            "Epoch [2/10], Training Loss: 0.945, Validation Accuracy: 59.51%\n",
            "Epoch [3/10], Training Loss: 0.905, Validation Accuracy: 59.25%\n",
            "Epoch [4/10], Training Loss: 0.882, Validation Accuracy: 59.85%\n",
            "Epoch [5/10], Training Loss: 0.850, Validation Accuracy: 59.91%\n",
            "Epoch [6/10], Training Loss: 0.833, Validation Accuracy: 59.65%\n",
            "Epoch [7/10], Training Loss: 0.808, Validation Accuracy: 59.87%\n",
            "Epoch [8/10], Training Loss: 0.792, Validation Accuracy: 59.49%\n",
            "Epoch [9/10], Training Loss: 0.769, Validation Accuracy: 59.50%\n",
            "Epoch [10/10], Training Loss: 0.759, Validation Accuracy: 59.85%\n",
            "Epoch [1/10], Training Loss: 0.989, Validation Accuracy: 60.84%\n",
            "Epoch [2/10], Training Loss: 0.909, Validation Accuracy: 60.53%\n",
            "Epoch [3/10], Training Loss: 0.885, Validation Accuracy: 60.66%\n",
            "Epoch [4/10], Training Loss: 0.848, Validation Accuracy: 60.62%\n",
            "Epoch [5/10], Training Loss: 0.832, Validation Accuracy: 59.66%\n",
            "Epoch [6/10], Training Loss: 0.802, Validation Accuracy: 60.77%\n",
            "Epoch [7/10], Training Loss: 0.788, Validation Accuracy: 59.78%\n",
            "Epoch [8/10], Training Loss: 0.763, Validation Accuracy: 59.83%\n",
            "Epoch [9/10], Training Loss: 0.751, Validation Accuracy: 59.52%\n",
            "Epoch [10/10], Training Loss: 0.738, Validation Accuracy: 58.96%\n",
            "Epoch [1/10], Training Loss: 0.984, Validation Accuracy: 60.45%\n",
            "Epoch [2/10], Training Loss: 0.902, Validation Accuracy: 60.68%\n",
            "Epoch [3/10], Training Loss: 0.863, Validation Accuracy: 58.94%\n",
            "Epoch [4/10], Training Loss: 0.849, Validation Accuracy: 60.35%\n",
            "Epoch [5/10], Training Loss: 0.805, Validation Accuracy: 60.56%\n",
            "Epoch [6/10], Training Loss: 0.785, Validation Accuracy: 60.35%\n",
            "Epoch [7/10], Training Loss: 0.763, Validation Accuracy: 60.54%\n",
            "Epoch [8/10], Training Loss: 0.749, Validation Accuracy: 59.86%\n",
            "Epoch [9/10], Training Loss: 0.733, Validation Accuracy: 59.92%\n",
            "Epoch [10/10], Training Loss: 0.715, Validation Accuracy: 60.34%\n",
            "Epoch [1/10], Training Loss: 0.988, Validation Accuracy: 59.96%\n",
            "Epoch [2/10], Training Loss: 0.908, Validation Accuracy: 60.58%\n",
            "Epoch [3/10], Training Loss: 0.862, Validation Accuracy: 60.38%\n",
            "Epoch [4/10], Training Loss: 0.833, Validation Accuracy: 60.29%\n",
            "Epoch [5/10], Training Loss: 0.806, Validation Accuracy: 60.33%\n",
            "Epoch [6/10], Training Loss: 0.780, Validation Accuracy: 60.84%\n",
            "Epoch [7/10], Training Loss: 0.768, Validation Accuracy: 60.74%\n",
            "Epoch [8/10], Training Loss: 0.743, Validation Accuracy: 60.74%\n",
            "Epoch [9/10], Training Loss: 0.732, Validation Accuracy: 60.46%\n",
            "Epoch [10/10], Training Loss: 0.718, Validation Accuracy: 60.23%\n",
            "Epoch [1/10], Training Loss: 0.933, Validation Accuracy: 59.37%\n",
            "Epoch [2/10], Training Loss: 0.859, Validation Accuracy: 59.97%\n",
            "Epoch [3/10], Training Loss: 0.819, Validation Accuracy: 60.50%\n",
            "Epoch [4/10], Training Loss: 0.787, Validation Accuracy: 60.15%\n",
            "Epoch [5/10], Training Loss: 0.761, Validation Accuracy: 60.59%\n",
            "Epoch [6/10], Training Loss: 0.729, Validation Accuracy: 60.42%\n",
            "Epoch [7/10], Training Loss: 0.715, Validation Accuracy: 59.61%\n",
            "Epoch [8/10], Training Loss: 0.689, Validation Accuracy: 58.90%\n",
            "Epoch [9/10], Training Loss: 0.679, Validation Accuracy: 59.60%\n",
            "Epoch [10/10], Training Loss: 0.661, Validation Accuracy: 58.93%\n",
            "Epoch [1/10], Training Loss: 0.965, Validation Accuracy: 59.49%\n",
            "Epoch [2/10], Training Loss: 0.881, Validation Accuracy: 60.65%\n",
            "Epoch [3/10], Training Loss: 0.823, Validation Accuracy: 60.33%\n",
            "Epoch [4/10], Training Loss: 0.780, Validation Accuracy: 60.74%\n",
            "Epoch [5/10], Training Loss: 0.753, Validation Accuracy: 60.39%\n",
            "Epoch [6/10], Training Loss: 0.738, Validation Accuracy: 60.06%\n",
            "Epoch [7/10], Training Loss: 0.714, Validation Accuracy: 59.01%\n",
            "Epoch [8/10], Training Loss: 0.700, Validation Accuracy: 59.65%\n",
            "Epoch [9/10], Training Loss: 0.687, Validation Accuracy: 60.16%\n",
            "Epoch [10/10], Training Loss: 0.662, Validation Accuracy: 60.10%\n",
            "Epoch [1/10], Training Loss: 0.940, Validation Accuracy: 59.72%\n",
            "Epoch [2/10], Training Loss: 0.847, Validation Accuracy: 61.28%\n",
            "Epoch [3/10], Training Loss: 0.800, Validation Accuracy: 60.86%\n",
            "Epoch [4/10], Training Loss: 0.765, Validation Accuracy: 60.71%\n",
            "Epoch [5/10], Training Loss: 0.749, Validation Accuracy: 60.13%\n",
            "Epoch [6/10], Training Loss: 0.715, Validation Accuracy: 60.97%\n",
            "Epoch [7/10], Training Loss: 0.693, Validation Accuracy: 60.46%\n",
            "Epoch [8/10], Training Loss: 0.670, Validation Accuracy: 61.06%\n",
            "Epoch [9/10], Training Loss: 0.653, Validation Accuracy: 60.88%\n",
            "Epoch [10/10], Training Loss: 0.635, Validation Accuracy: 60.57%\n",
            "Epoch [1/10], Training Loss: 0.913, Validation Accuracy: 60.93%\n",
            "Epoch [2/10], Training Loss: 0.836, Validation Accuracy: 60.20%\n",
            "Epoch [3/10], Training Loss: 0.783, Validation Accuracy: 61.19%\n",
            "Epoch [4/10], Training Loss: 0.746, Validation Accuracy: 61.20%\n",
            "Epoch [5/10], Training Loss: 0.713, Validation Accuracy: 61.24%\n",
            "Epoch [6/10], Training Loss: 0.700, Validation Accuracy: 59.98%\n",
            "Epoch [7/10], Training Loss: 0.671, Validation Accuracy: 60.48%\n",
            "Epoch [8/10], Training Loss: 0.654, Validation Accuracy: 60.47%\n",
            "Epoch [9/10], Training Loss: 0.635, Validation Accuracy: 60.04%\n",
            "Epoch [10/10], Training Loss: 0.616, Validation Accuracy: 60.36%\n",
            "Epoch [1/10], Training Loss: 0.937, Validation Accuracy: 59.52%\n",
            "Epoch [2/10], Training Loss: 0.841, Validation Accuracy: 61.08%\n",
            "Epoch [3/10], Training Loss: 0.785, Validation Accuracy: 59.80%\n",
            "Epoch [4/10], Training Loss: 0.759, Validation Accuracy: 59.82%\n",
            "Epoch [5/10], Training Loss: 0.729, Validation Accuracy: 60.91%\n",
            "Epoch [6/10], Training Loss: 0.701, Validation Accuracy: 60.31%\n",
            "Epoch [7/10], Training Loss: 0.676, Validation Accuracy: 60.36%\n",
            "Epoch [8/10], Training Loss: 0.653, Validation Accuracy: 60.55%\n",
            "Epoch [9/10], Training Loss: 0.643, Validation Accuracy: 59.64%\n",
            "Epoch [10/10], Training Loss: 0.621, Validation Accuracy: 60.59%\n",
            "Epoch [1/10], Training Loss: 0.892, Validation Accuracy: 59.54%\n",
            "Epoch [2/10], Training Loss: 0.798, Validation Accuracy: 60.70%\n",
            "Epoch [3/10], Training Loss: 0.742, Validation Accuracy: 60.54%\n",
            "Epoch [4/10], Training Loss: 0.702, Validation Accuracy: 60.34%\n",
            "Epoch [5/10], Training Loss: 0.673, Validation Accuracy: 60.27%\n",
            "Epoch [6/10], Training Loss: 0.648, Validation Accuracy: 60.19%\n",
            "Epoch [7/10], Training Loss: 0.628, Validation Accuracy: 59.78%\n",
            "Epoch [8/10], Training Loss: 0.602, Validation Accuracy: 59.23%\n",
            "Epoch [9/10], Training Loss: 0.589, Validation Accuracy: 59.97%\n",
            "Epoch [10/10], Training Loss: 0.568, Validation Accuracy: 60.09%\n",
            "Epoch [1/10], Training Loss: 0.915, Validation Accuracy: 59.99%\n",
            "Epoch [2/10], Training Loss: 0.807, Validation Accuracy: 60.76%\n",
            "Epoch [3/10], Training Loss: 0.750, Validation Accuracy: 59.94%\n",
            "Epoch [4/10], Training Loss: 0.724, Validation Accuracy: 60.59%\n",
            "Epoch [5/10], Training Loss: 0.679, Validation Accuracy: 60.67%\n",
            "Epoch [6/10], Training Loss: 0.649, Validation Accuracy: 60.79%\n",
            "Epoch [7/10], Training Loss: 0.631, Validation Accuracy: 59.88%\n",
            "Epoch [8/10], Training Loss: 0.608, Validation Accuracy: 60.52%\n",
            "Epoch [9/10], Training Loss: 0.584, Validation Accuracy: 60.93%\n",
            "Epoch [10/10], Training Loss: 0.567, Validation Accuracy: 59.07%\n",
            "Epoch [1/10], Training Loss: 0.898, Validation Accuracy: 61.13%\n",
            "Epoch [2/10], Training Loss: 0.784, Validation Accuracy: 60.58%\n",
            "Epoch [3/10], Training Loss: 0.728, Validation Accuracy: 60.93%\n",
            "Epoch [4/10], Training Loss: 0.682, Validation Accuracy: 60.99%\n",
            "Epoch [5/10], Training Loss: 0.650, Validation Accuracy: 61.14%\n",
            "Epoch [6/10], Training Loss: 0.624, Validation Accuracy: 60.33%\n",
            "Epoch [7/10], Training Loss: 0.593, Validation Accuracy: 61.13%\n",
            "Epoch [8/10], Training Loss: 0.569, Validation Accuracy: 60.94%\n",
            "Epoch [9/10], Training Loss: 0.560, Validation Accuracy: 60.62%\n",
            "Epoch [10/10], Training Loss: 0.535, Validation Accuracy: 60.22%\n",
            "Epoch [1/10], Training Loss: 0.869, Validation Accuracy: 60.22%\n",
            "Epoch [2/10], Training Loss: 0.762, Validation Accuracy: 60.67%\n",
            "Epoch [3/10], Training Loss: 0.707, Validation Accuracy: 60.42%\n",
            "Epoch [4/10], Training Loss: 0.667, Validation Accuracy: 60.53%\n",
            "Epoch [5/10], Training Loss: 0.631, Validation Accuracy: 61.30%\n",
            "Epoch [6/10], Training Loss: 0.595, Validation Accuracy: 60.95%\n",
            "Epoch [7/10], Training Loss: 0.575, Validation Accuracy: 60.48%\n",
            "Epoch [8/10], Training Loss: 0.555, Validation Accuracy: 59.40%\n",
            "Epoch [9/10], Training Loss: 0.539, Validation Accuracy: 60.58%\n",
            "Epoch [10/10], Training Loss: 0.517, Validation Accuracy: 60.58%\n",
            "Epoch [1/10], Training Loss: 0.880, Validation Accuracy: 60.10%\n",
            "Epoch [2/10], Training Loss: 0.771, Validation Accuracy: 60.72%\n",
            "Epoch [3/10], Training Loss: 0.714, Validation Accuracy: 60.28%\n",
            "Epoch [4/10], Training Loss: 0.674, Validation Accuracy: 61.04%\n",
            "Epoch [5/10], Training Loss: 0.643, Validation Accuracy: 60.85%\n",
            "Epoch [6/10], Training Loss: 0.612, Validation Accuracy: 60.77%\n",
            "Epoch [7/10], Training Loss: 0.594, Validation Accuracy: 60.15%\n",
            "Epoch [8/10], Training Loss: 0.572, Validation Accuracy: 59.76%\n",
            "Epoch [9/10], Training Loss: 0.547, Validation Accuracy: 59.62%\n",
            "Epoch [10/10], Training Loss: 0.520, Validation Accuracy: 59.77%\n",
            "Epoch [1/10], Training Loss: 0.863, Validation Accuracy: 59.71%\n",
            "Epoch [2/10], Training Loss: 0.728, Validation Accuracy: 60.56%\n",
            "Epoch [3/10], Training Loss: 0.666, Validation Accuracy: 60.59%\n",
            "Epoch [4/10], Training Loss: 0.621, Validation Accuracy: 59.82%\n",
            "Epoch [5/10], Training Loss: 0.595, Validation Accuracy: 60.34%\n",
            "Epoch [6/10], Training Loss: 0.570, Validation Accuracy: 59.57%\n",
            "Epoch [7/10], Training Loss: 0.548, Validation Accuracy: 59.50%\n",
            "Epoch [8/10], Training Loss: 0.520, Validation Accuracy: 60.40%\n",
            "Epoch [9/10], Training Loss: 0.503, Validation Accuracy: 59.73%\n",
            "Epoch [10/10], Training Loss: 0.476, Validation Accuracy: 59.31%\n",
            "Epoch [1/10], Training Loss: 0.893, Validation Accuracy: 60.04%\n",
            "Epoch [2/10], Training Loss: 0.742, Validation Accuracy: 59.78%\n",
            "Epoch [3/10], Training Loss: 0.687, Validation Accuracy: 60.92%\n",
            "Epoch [4/10], Training Loss: 0.638, Validation Accuracy: 60.08%\n",
            "Epoch [5/10], Training Loss: 0.595, Validation Accuracy: 60.11%\n",
            "Epoch [6/10], Training Loss: 0.574, Validation Accuracy: 60.24%\n",
            "Epoch [7/10], Training Loss: 0.539, Validation Accuracy: 60.00%\n",
            "Epoch [8/10], Training Loss: 0.521, Validation Accuracy: 60.05%\n",
            "Epoch [9/10], Training Loss: 0.501, Validation Accuracy: 60.51%\n",
            "Epoch [10/10], Training Loss: 0.474, Validation Accuracy: 60.09%\n",
            "Epoch [1/10], Training Loss: 0.849, Validation Accuracy: 59.59%\n",
            "Epoch [2/10], Training Loss: 0.709, Validation Accuracy: 60.89%\n",
            "Epoch [3/10], Training Loss: 0.641, Validation Accuracy: 60.74%\n",
            "Epoch [4/10], Training Loss: 0.597, Validation Accuracy: 60.68%\n",
            "Epoch [5/10], Training Loss: 0.564, Validation Accuracy: 61.03%\n",
            "Epoch [6/10], Training Loss: 0.533, Validation Accuracy: 60.32%\n",
            "Epoch [7/10], Training Loss: 0.506, Validation Accuracy: 60.27%\n",
            "Epoch [8/10], Training Loss: 0.476, Validation Accuracy: 60.72%\n",
            "Epoch [9/10], Training Loss: 0.463, Validation Accuracy: 60.56%\n",
            "Epoch [10/10], Training Loss: 0.442, Validation Accuracy: 59.87%\n",
            "Epoch [1/10], Training Loss: 0.833, Validation Accuracy: 60.45%\n",
            "Epoch [2/10], Training Loss: 0.704, Validation Accuracy: 60.66%\n",
            "Epoch [3/10], Training Loss: 0.635, Validation Accuracy: 60.31%\n",
            "Epoch [4/10], Training Loss: 0.584, Validation Accuracy: 60.78%\n",
            "Epoch [5/10], Training Loss: 0.544, Validation Accuracy: 60.20%\n",
            "Epoch [6/10], Training Loss: 0.522, Validation Accuracy: 60.53%\n",
            "Epoch [7/10], Training Loss: 0.491, Validation Accuracy: 60.71%\n",
            "Epoch [8/10], Training Loss: 0.470, Validation Accuracy: 60.63%\n",
            "Epoch [9/10], Training Loss: 0.442, Validation Accuracy: 60.08%\n",
            "Epoch [10/10], Training Loss: 0.430, Validation Accuracy: 59.87%\n",
            "Epoch [1/10], Training Loss: 0.842, Validation Accuracy: 59.70%\n",
            "Epoch [2/10], Training Loss: 0.713, Validation Accuracy: 60.42%\n",
            "Epoch [3/10], Training Loss: 0.636, Validation Accuracy: 60.10%\n",
            "Epoch [4/10], Training Loss: 0.592, Validation Accuracy: 60.60%\n",
            "Epoch [5/10], Training Loss: 0.551, Validation Accuracy: 60.41%\n",
            "Epoch [6/10], Training Loss: 0.521, Validation Accuracy: 60.31%\n",
            "Epoch [7/10], Training Loss: 0.504, Validation Accuracy: 60.55%\n",
            "Epoch [8/10], Training Loss: 0.469, Validation Accuracy: 59.95%\n",
            "Epoch [9/10], Training Loss: 0.455, Validation Accuracy: 59.88%\n",
            "Epoch [10/10], Training Loss: 0.435, Validation Accuracy: 60.12%\n",
            "Epoch [1/10], Training Loss: 0.823, Validation Accuracy: 58.66%\n",
            "Epoch [2/10], Training Loss: 0.676, Validation Accuracy: 60.38%\n",
            "Epoch [3/10], Training Loss: 0.612, Validation Accuracy: 60.15%\n",
            "Epoch [4/10], Training Loss: 0.557, Validation Accuracy: 59.76%\n",
            "Epoch [5/10], Training Loss: 0.511, Validation Accuracy: 59.90%\n",
            "Epoch [6/10], Training Loss: 0.478, Validation Accuracy: 59.79%\n",
            "Epoch [7/10], Training Loss: 0.466, Validation Accuracy: 60.15%\n",
            "Epoch [8/10], Training Loss: 0.438, Validation Accuracy: 59.80%\n",
            "Epoch [9/10], Training Loss: 0.412, Validation Accuracy: 59.45%\n",
            "Epoch [10/10], Training Loss: 0.396, Validation Accuracy: 59.12%\n",
            "Confusion Matrix:\n",
            "[[588  26  78  31  65   9  23  17 107  56]\n",
            " [ 33 664  17  20  19  11  16   9  45 166]\n",
            " [ 44  12 470  94 160  70  67  49  18  16]\n",
            " [ 19  12  67 396 120 187  85  65  15  34]\n",
            " [ 20   7  70  57 619  47  65  90   9  16]\n",
            " [ 10   4  56 219 125 445  35  91   2  13]\n",
            " [ 12  10  44  80  95  28 700  12   8  11]\n",
            " [ 10   4  39  47 137  59  16 653   3  32]\n",
            " [ 70  51  31  31  26  17   6   7 707  54]\n",
            " [ 31  93  17  39  25  11  14  23  45 702]]\n",
            "Test Accuracy: 59.44%\n",
            "True Positives (TP): [588 664 470 396 619 445 700 653 707 702]\n",
            "False Positives (FP): [249 219 419 618 772 439 327 363 252 398]\n",
            "True Negatives (TN): [8751 8781 8581 8382 8228 8561 8673 8637 8748 8602]\n",
            "False Negatives (FN): [412 336 530 604 381 555 300 347 293 298]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.70250896 0.75198188 0.52868391 0.39053254 0.44500359 0.50339367\n",
            " 0.68159688 0.64271654 0.73722628 0.63818182]\n",
            "Recall: [0.588 0.664 0.47  0.396 0.619 0.445 0.7   0.653 0.707 0.702]\n",
            "F1 Score: [0.6401742  0.70525757 0.49761779 0.39324727 0.51777499 0.47239915\n",
            " 0.69067588 0.64781746 0.72179684 0.66857143]\n",
            "CPU times: user 4h 9min 58s, sys: 1min 50s, total: 4h 11min 49s\n",
            "Wall time: 4h 33min 10s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 1):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=1):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_uniform: Dict ) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "    mean = distribution_info_uniform[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info_uniform[\"std\"].mean().item()  # Convert numpy array to float\n",
        "\n",
        "\n",
        "\n",
        "     # Generate augmented data using Uniform distribution\n",
        "    augmented_data_uniform = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = augmented_data_uniform\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=1)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"uniform\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLczw1rQRyKn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3LBlR6_jWyu"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfqntTVpjfcG"
      },
      "source": [
        "Beta=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u-EaJbAi7vN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK6l-szVi8F3",
        "outputId": "96794d8d-2bb6-4fc7-e8f5-3437ceee9104"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:01<00:00, 104MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Images per Class: [6170 6008 6061 5971 5962 5942 6078 5957 5893 5958]\n",
            "Epoch [1/10], Training Loss: 2.306, Validation Accuracy: 9.92%\n",
            "Epoch [2/10], Training Loss: 2.305, Validation Accuracy: 9.92%\n",
            "Epoch [3/10], Training Loss: 2.304, Validation Accuracy: 10.04%\n",
            "Epoch [4/10], Training Loss: 2.303, Validation Accuracy: 10.06%\n",
            "Epoch [5/10], Training Loss: 2.302, Validation Accuracy: 10.35%\n",
            "Epoch [6/10], Training Loss: 2.300, Validation Accuracy: 10.56%\n",
            "Epoch [7/10], Training Loss: 2.299, Validation Accuracy: 10.57%\n",
            "Epoch [8/10], Training Loss: 2.298, Validation Accuracy: 10.68%\n",
            "Epoch [9/10], Training Loss: 2.297, Validation Accuracy: 11.03%\n",
            "Epoch [10/10], Training Loss: 2.295, Validation Accuracy: 12.94%\n",
            "Epoch [1/10], Training Loss: 2.294, Validation Accuracy: 14.75%\n",
            "Epoch [2/10], Training Loss: 2.292, Validation Accuracy: 19.49%\n",
            "Epoch [3/10], Training Loss: 2.288, Validation Accuracy: 21.63%\n",
            "Epoch [4/10], Training Loss: 2.283, Validation Accuracy: 20.93%\n",
            "Epoch [5/10], Training Loss: 2.276, Validation Accuracy: 19.67%\n",
            "Epoch [6/10], Training Loss: 2.266, Validation Accuracy: 19.51%\n",
            "Epoch [7/10], Training Loss: 2.249, Validation Accuracy: 19.44%\n",
            "Epoch [8/10], Training Loss: 2.224, Validation Accuracy: 20.25%\n",
            "Epoch [9/10], Training Loss: 2.188, Validation Accuracy: 22.88%\n",
            "Epoch [10/10], Training Loss: 2.153, Validation Accuracy: 23.64%\n",
            "Epoch [1/10], Training Loss: 2.130, Validation Accuracy: 24.65%\n",
            "Epoch [2/10], Training Loss: 2.104, Validation Accuracy: 25.83%\n",
            "Epoch [3/10], Training Loss: 2.077, Validation Accuracy: 26.15%\n",
            "Epoch [4/10], Training Loss: 2.055, Validation Accuracy: 26.17%\n",
            "Epoch [5/10], Training Loss: 2.030, Validation Accuracy: 27.59%\n",
            "Epoch [6/10], Training Loss: 2.009, Validation Accuracy: 28.71%\n",
            "Epoch [7/10], Training Loss: 1.988, Validation Accuracy: 28.99%\n",
            "Epoch [8/10], Training Loss: 1.970, Validation Accuracy: 29.80%\n",
            "Epoch [9/10], Training Loss: 1.954, Validation Accuracy: 30.38%\n",
            "Epoch [10/10], Training Loss: 1.939, Validation Accuracy: 30.40%\n",
            "Epoch [1/10], Training Loss: 1.915, Validation Accuracy: 31.07%\n",
            "Epoch [2/10], Training Loss: 1.894, Validation Accuracy: 31.35%\n",
            "Epoch [3/10], Training Loss: 1.879, Validation Accuracy: 31.74%\n",
            "Epoch [4/10], Training Loss: 1.863, Validation Accuracy: 32.96%\n",
            "Epoch [5/10], Training Loss: 1.838, Validation Accuracy: 33.53%\n",
            "Epoch [6/10], Training Loss: 1.822, Validation Accuracy: 34.25%\n",
            "Epoch [7/10], Training Loss: 1.803, Validation Accuracy: 35.18%\n",
            "Epoch [8/10], Training Loss: 1.783, Validation Accuracy: 34.57%\n",
            "Epoch [9/10], Training Loss: 1.766, Validation Accuracy: 35.74%\n",
            "Epoch [10/10], Training Loss: 1.746, Validation Accuracy: 35.38%\n",
            "Epoch [1/10], Training Loss: 1.746, Validation Accuracy: 37.44%\n",
            "Epoch [2/10], Training Loss: 1.722, Validation Accuracy: 36.53%\n",
            "Epoch [3/10], Training Loss: 1.707, Validation Accuracy: 37.80%\n",
            "Epoch [4/10], Training Loss: 1.691, Validation Accuracy: 37.56%\n",
            "Epoch [5/10], Training Loss: 1.678, Validation Accuracy: 37.79%\n",
            "Epoch [6/10], Training Loss: 1.665, Validation Accuracy: 38.19%\n",
            "Epoch [7/10], Training Loss: 1.651, Validation Accuracy: 38.78%\n",
            "Epoch [8/10], Training Loss: 1.640, Validation Accuracy: 38.59%\n",
            "Epoch [9/10], Training Loss: 1.627, Validation Accuracy: 39.12%\n",
            "Epoch [10/10], Training Loss: 1.617, Validation Accuracy: 38.94%\n",
            "Epoch [1/10], Training Loss: 1.630, Validation Accuracy: 40.09%\n",
            "Epoch [2/10], Training Loss: 1.616, Validation Accuracy: 40.44%\n",
            "Epoch [3/10], Training Loss: 1.607, Validation Accuracy: 40.86%\n",
            "Epoch [4/10], Training Loss: 1.595, Validation Accuracy: 40.95%\n",
            "Epoch [5/10], Training Loss: 1.589, Validation Accuracy: 40.80%\n",
            "Epoch [6/10], Training Loss: 1.576, Validation Accuracy: 40.77%\n",
            "Epoch [7/10], Training Loss: 1.571, Validation Accuracy: 40.89%\n",
            "Epoch [8/10], Training Loss: 1.560, Validation Accuracy: 42.14%\n",
            "Epoch [9/10], Training Loss: 1.551, Validation Accuracy: 42.32%\n",
            "Epoch [10/10], Training Loss: 1.545, Validation Accuracy: 42.61%\n",
            "Epoch [1/10], Training Loss: 1.589, Validation Accuracy: 42.40%\n",
            "Epoch [2/10], Training Loss: 1.582, Validation Accuracy: 42.50%\n",
            "Epoch [3/10], Training Loss: 1.577, Validation Accuracy: 42.60%\n",
            "Epoch [4/10], Training Loss: 1.563, Validation Accuracy: 43.05%\n",
            "Epoch [5/10], Training Loss: 1.553, Validation Accuracy: 43.66%\n",
            "Epoch [6/10], Training Loss: 1.548, Validation Accuracy: 43.59%\n",
            "Epoch [7/10], Training Loss: 1.540, Validation Accuracy: 43.70%\n",
            "Epoch [8/10], Training Loss: 1.544, Validation Accuracy: 44.11%\n",
            "Epoch [9/10], Training Loss: 1.526, Validation Accuracy: 43.87%\n",
            "Epoch [10/10], Training Loss: 1.521, Validation Accuracy: 44.40%\n",
            "Epoch [1/10], Training Loss: 1.538, Validation Accuracy: 45.08%\n",
            "Epoch [2/10], Training Loss: 1.528, Validation Accuracy: 44.51%\n",
            "Epoch [3/10], Training Loss: 1.513, Validation Accuracy: 45.62%\n",
            "Epoch [4/10], Training Loss: 1.508, Validation Accuracy: 45.19%\n",
            "Epoch [5/10], Training Loss: 1.497, Validation Accuracy: 44.92%\n",
            "Epoch [6/10], Training Loss: 1.494, Validation Accuracy: 45.64%\n",
            "Epoch [7/10], Training Loss: 1.485, Validation Accuracy: 46.18%\n",
            "Epoch [8/10], Training Loss: 1.472, Validation Accuracy: 46.38%\n",
            "Epoch [9/10], Training Loss: 1.467, Validation Accuracy: 46.40%\n",
            "Epoch [10/10], Training Loss: 1.459, Validation Accuracy: 46.62%\n",
            "Epoch [1/10], Training Loss: 1.483, Validation Accuracy: 46.60%\n",
            "Epoch [2/10], Training Loss: 1.473, Validation Accuracy: 47.08%\n",
            "Epoch [3/10], Training Loss: 1.459, Validation Accuracy: 46.65%\n",
            "Epoch [4/10], Training Loss: 1.449, Validation Accuracy: 46.87%\n",
            "Epoch [5/10], Training Loss: 1.447, Validation Accuracy: 46.95%\n",
            "Epoch [6/10], Training Loss: 1.442, Validation Accuracy: 47.07%\n",
            "Epoch [7/10], Training Loss: 1.430, Validation Accuracy: 47.53%\n",
            "Epoch [8/10], Training Loss: 1.425, Validation Accuracy: 47.30%\n",
            "Epoch [9/10], Training Loss: 1.418, Validation Accuracy: 48.06%\n",
            "Epoch [10/10], Training Loss: 1.408, Validation Accuracy: 47.93%\n",
            "Epoch [1/10], Training Loss: 1.436, Validation Accuracy: 48.27%\n",
            "Epoch [2/10], Training Loss: 1.419, Validation Accuracy: 48.00%\n",
            "Epoch [3/10], Training Loss: 1.406, Validation Accuracy: 48.54%\n",
            "Epoch [4/10], Training Loss: 1.401, Validation Accuracy: 49.03%\n",
            "Epoch [5/10], Training Loss: 1.386, Validation Accuracy: 48.19%\n",
            "Epoch [6/10], Training Loss: 1.382, Validation Accuracy: 48.73%\n",
            "Epoch [7/10], Training Loss: 1.376, Validation Accuracy: 48.92%\n",
            "Epoch [8/10], Training Loss: 1.367, Validation Accuracy: 48.75%\n",
            "Epoch [9/10], Training Loss: 1.359, Validation Accuracy: 49.14%\n",
            "Epoch [10/10], Training Loss: 1.360, Validation Accuracy: 48.77%\n",
            "Epoch [1/10], Training Loss: 1.400, Validation Accuracy: 49.08%\n",
            "Epoch [2/10], Training Loss: 1.389, Validation Accuracy: 49.99%\n",
            "Epoch [3/10], Training Loss: 1.370, Validation Accuracy: 50.22%\n",
            "Epoch [4/10], Training Loss: 1.360, Validation Accuracy: 50.36%\n",
            "Epoch [5/10], Training Loss: 1.355, Validation Accuracy: 49.74%\n",
            "Epoch [6/10], Training Loss: 1.345, Validation Accuracy: 50.32%\n",
            "Epoch [7/10], Training Loss: 1.339, Validation Accuracy: 50.46%\n",
            "Epoch [8/10], Training Loss: 1.330, Validation Accuracy: 50.52%\n",
            "Epoch [9/10], Training Loss: 1.330, Validation Accuracy: 50.64%\n",
            "Epoch [10/10], Training Loss: 1.320, Validation Accuracy: 49.31%\n",
            "Epoch [1/10], Training Loss: 1.413, Validation Accuracy: 50.66%\n",
            "Epoch [2/10], Training Loss: 1.391, Validation Accuracy: 50.94%\n",
            "Epoch [3/10], Training Loss: 1.372, Validation Accuracy: 50.04%\n",
            "Epoch [4/10], Training Loss: 1.372, Validation Accuracy: 50.84%\n",
            "Epoch [5/10], Training Loss: 1.353, Validation Accuracy: 51.02%\n",
            "Epoch [6/10], Training Loss: 1.345, Validation Accuracy: 50.52%\n",
            "Epoch [7/10], Training Loss: 1.342, Validation Accuracy: 51.26%\n",
            "Epoch [8/10], Training Loss: 1.341, Validation Accuracy: 51.59%\n",
            "Epoch [9/10], Training Loss: 1.329, Validation Accuracy: 51.74%\n",
            "Epoch [10/10], Training Loss: 1.318, Validation Accuracy: 51.52%\n",
            "Epoch [1/10], Training Loss: 1.370, Validation Accuracy: 51.76%\n",
            "Epoch [2/10], Training Loss: 1.344, Validation Accuracy: 51.77%\n",
            "Epoch [3/10], Training Loss: 1.332, Validation Accuracy: 51.98%\n",
            "Epoch [4/10], Training Loss: 1.329, Validation Accuracy: 51.86%\n",
            "Epoch [5/10], Training Loss: 1.316, Validation Accuracy: 51.88%\n",
            "Epoch [6/10], Training Loss: 1.306, Validation Accuracy: 52.23%\n",
            "Epoch [7/10], Training Loss: 1.302, Validation Accuracy: 51.94%\n",
            "Epoch [8/10], Training Loss: 1.290, Validation Accuracy: 52.34%\n",
            "Epoch [9/10], Training Loss: 1.283, Validation Accuracy: 51.73%\n",
            "Epoch [10/10], Training Loss: 1.274, Validation Accuracy: 51.92%\n",
            "Epoch [1/10], Training Loss: 1.331, Validation Accuracy: 52.52%\n",
            "Epoch [2/10], Training Loss: 1.315, Validation Accuracy: 52.49%\n",
            "Epoch [3/10], Training Loss: 1.308, Validation Accuracy: 50.94%\n",
            "Epoch [4/10], Training Loss: 1.299, Validation Accuracy: 52.01%\n",
            "Epoch [5/10], Training Loss: 1.283, Validation Accuracy: 52.72%\n",
            "Epoch [6/10], Training Loss: 1.275, Validation Accuracy: 53.01%\n",
            "Epoch [7/10], Training Loss: 1.267, Validation Accuracy: 53.11%\n",
            "Epoch [8/10], Training Loss: 1.257, Validation Accuracy: 53.16%\n",
            "Epoch [9/10], Training Loss: 1.251, Validation Accuracy: 52.72%\n",
            "Epoch [10/10], Training Loss: 1.240, Validation Accuracy: 52.49%\n",
            "Epoch [1/10], Training Loss: 1.281, Validation Accuracy: 53.46%\n",
            "Epoch [2/10], Training Loss: 1.259, Validation Accuracy: 53.60%\n",
            "Epoch [3/10], Training Loss: 1.246, Validation Accuracy: 52.90%\n",
            "Epoch [4/10], Training Loss: 1.231, Validation Accuracy: 54.05%\n",
            "Epoch [5/10], Training Loss: 1.221, Validation Accuracy: 53.82%\n",
            "Epoch [6/10], Training Loss: 1.211, Validation Accuracy: 53.34%\n",
            "Epoch [7/10], Training Loss: 1.210, Validation Accuracy: 52.38%\n",
            "Epoch [8/10], Training Loss: 1.195, Validation Accuracy: 53.59%\n",
            "Epoch [9/10], Training Loss: 1.182, Validation Accuracy: 53.60%\n",
            "Epoch [10/10], Training Loss: 1.185, Validation Accuracy: 53.62%\n",
            "Epoch [1/10], Training Loss: 1.276, Validation Accuracy: 54.25%\n",
            "Epoch [2/10], Training Loss: 1.239, Validation Accuracy: 54.20%\n",
            "Epoch [3/10], Training Loss: 1.226, Validation Accuracy: 54.06%\n",
            "Epoch [4/10], Training Loss: 1.221, Validation Accuracy: 54.33%\n",
            "Epoch [5/10], Training Loss: 1.209, Validation Accuracy: 54.38%\n",
            "Epoch [6/10], Training Loss: 1.197, Validation Accuracy: 54.60%\n",
            "Epoch [7/10], Training Loss: 1.191, Validation Accuracy: 54.31%\n",
            "Epoch [8/10], Training Loss: 1.176, Validation Accuracy: 54.76%\n",
            "Epoch [9/10], Training Loss: 1.173, Validation Accuracy: 54.88%\n",
            "Epoch [10/10], Training Loss: 1.159, Validation Accuracy: 53.27%\n",
            "Epoch [1/10], Training Loss: 1.285, Validation Accuracy: 54.32%\n",
            "Epoch [2/10], Training Loss: 1.257, Validation Accuracy: 54.12%\n",
            "Epoch [3/10], Training Loss: 1.238, Validation Accuracy: 54.69%\n",
            "Epoch [4/10], Training Loss: 1.218, Validation Accuracy: 54.44%\n",
            "Epoch [5/10], Training Loss: 1.210, Validation Accuracy: 54.49%\n",
            "Epoch [6/10], Training Loss: 1.202, Validation Accuracy: 54.39%\n",
            "Epoch [7/10], Training Loss: 1.184, Validation Accuracy: 55.10%\n",
            "Epoch [8/10], Training Loss: 1.177, Validation Accuracy: 54.85%\n",
            "Epoch [9/10], Training Loss: 1.177, Validation Accuracy: 55.18%\n",
            "Epoch [10/10], Training Loss: 1.158, Validation Accuracy: 54.94%\n",
            "Epoch [1/10], Training Loss: 1.246, Validation Accuracy: 54.83%\n",
            "Epoch [2/10], Training Loss: 1.215, Validation Accuracy: 54.99%\n",
            "Epoch [3/10], Training Loss: 1.198, Validation Accuracy: 55.53%\n",
            "Epoch [4/10], Training Loss: 1.188, Validation Accuracy: 55.83%\n",
            "Epoch [5/10], Training Loss: 1.166, Validation Accuracy: 55.80%\n",
            "Epoch [6/10], Training Loss: 1.160, Validation Accuracy: 54.98%\n",
            "Epoch [7/10], Training Loss: 1.158, Validation Accuracy: 56.03%\n",
            "Epoch [8/10], Training Loss: 1.145, Validation Accuracy: 55.11%\n",
            "Epoch [9/10], Training Loss: 1.134, Validation Accuracy: 55.59%\n",
            "Epoch [10/10], Training Loss: 1.128, Validation Accuracy: 55.80%\n",
            "Epoch [1/10], Training Loss: 1.217, Validation Accuracy: 56.19%\n",
            "Epoch [2/10], Training Loss: 1.189, Validation Accuracy: 55.06%\n",
            "Epoch [3/10], Training Loss: 1.177, Validation Accuracy: 56.49%\n",
            "Epoch [4/10], Training Loss: 1.158, Validation Accuracy: 56.59%\n",
            "Epoch [5/10], Training Loss: 1.154, Validation Accuracy: 55.95%\n",
            "Epoch [6/10], Training Loss: 1.133, Validation Accuracy: 56.56%\n",
            "Epoch [7/10], Training Loss: 1.118, Validation Accuracy: 56.45%\n",
            "Epoch [8/10], Training Loss: 1.114, Validation Accuracy: 56.53%\n",
            "Epoch [9/10], Training Loss: 1.098, Validation Accuracy: 56.52%\n",
            "Epoch [10/10], Training Loss: 1.088, Validation Accuracy: 56.56%\n",
            "Epoch [1/10], Training Loss: 1.166, Validation Accuracy: 56.47%\n",
            "Epoch [2/10], Training Loss: 1.134, Validation Accuracy: 56.17%\n",
            "Epoch [3/10], Training Loss: 1.120, Validation Accuracy: 56.32%\n",
            "Epoch [4/10], Training Loss: 1.100, Validation Accuracy: 57.02%\n",
            "Epoch [5/10], Training Loss: 1.088, Validation Accuracy: 56.73%\n",
            "Epoch [6/10], Training Loss: 1.070, Validation Accuracy: 57.23%\n",
            "Epoch [7/10], Training Loss: 1.067, Validation Accuracy: 57.10%\n",
            "Epoch [8/10], Training Loss: 1.053, Validation Accuracy: 56.13%\n",
            "Epoch [9/10], Training Loss: 1.038, Validation Accuracy: 57.10%\n",
            "Epoch [10/10], Training Loss: 1.027, Validation Accuracy: 57.13%\n",
            "Epoch [1/10], Training Loss: 1.167, Validation Accuracy: 57.25%\n",
            "Epoch [2/10], Training Loss: 1.142, Validation Accuracy: 57.22%\n",
            "Epoch [3/10], Training Loss: 1.109, Validation Accuracy: 57.73%\n",
            "Epoch [4/10], Training Loss: 1.095, Validation Accuracy: 57.19%\n",
            "Epoch [5/10], Training Loss: 1.082, Validation Accuracy: 57.71%\n",
            "Epoch [6/10], Training Loss: 1.071, Validation Accuracy: 57.57%\n",
            "Epoch [7/10], Training Loss: 1.062, Validation Accuracy: 57.65%\n",
            "Epoch [8/10], Training Loss: 1.057, Validation Accuracy: 57.05%\n",
            "Epoch [9/10], Training Loss: 1.035, Validation Accuracy: 57.27%\n",
            "Epoch [10/10], Training Loss: 1.028, Validation Accuracy: 57.10%\n",
            "Epoch [1/10], Training Loss: 1.181, Validation Accuracy: 57.20%\n",
            "Epoch [2/10], Training Loss: 1.146, Validation Accuracy: 57.88%\n",
            "Epoch [3/10], Training Loss: 1.118, Validation Accuracy: 58.06%\n",
            "Epoch [4/10], Training Loss: 1.102, Validation Accuracy: 57.57%\n",
            "Epoch [5/10], Training Loss: 1.085, Validation Accuracy: 57.34%\n",
            "Epoch [6/10], Training Loss: 1.074, Validation Accuracy: 57.66%\n",
            "Epoch [7/10], Training Loss: 1.058, Validation Accuracy: 57.20%\n",
            "Epoch [8/10], Training Loss: 1.059, Validation Accuracy: 56.95%\n",
            "Epoch [9/10], Training Loss: 1.038, Validation Accuracy: 57.65%\n",
            "Epoch [10/10], Training Loss: 1.015, Validation Accuracy: 57.51%\n",
            "Epoch [1/10], Training Loss: 1.159, Validation Accuracy: 58.44%\n",
            "Epoch [2/10], Training Loss: 1.114, Validation Accuracy: 58.33%\n",
            "Epoch [3/10], Training Loss: 1.086, Validation Accuracy: 57.46%\n",
            "Epoch [4/10], Training Loss: 1.078, Validation Accuracy: 58.44%\n",
            "Epoch [5/10], Training Loss: 1.049, Validation Accuracy: 58.37%\n",
            "Epoch [6/10], Training Loss: 1.045, Validation Accuracy: 58.00%\n",
            "Epoch [7/10], Training Loss: 1.017, Validation Accuracy: 58.81%\n",
            "Epoch [8/10], Training Loss: 1.011, Validation Accuracy: 57.80%\n",
            "Epoch [9/10], Training Loss: 1.002, Validation Accuracy: 57.56%\n",
            "Epoch [10/10], Training Loss: 0.996, Validation Accuracy: 58.06%\n",
            "Epoch [1/10], Training Loss: 1.131, Validation Accuracy: 58.39%\n",
            "Epoch [2/10], Training Loss: 1.087, Validation Accuracy: 58.91%\n",
            "Epoch [3/10], Training Loss: 1.065, Validation Accuracy: 57.99%\n",
            "Epoch [4/10], Training Loss: 1.049, Validation Accuracy: 59.17%\n",
            "Epoch [5/10], Training Loss: 1.021, Validation Accuracy: 59.05%\n",
            "Epoch [6/10], Training Loss: 1.016, Validation Accuracy: 58.19%\n",
            "Epoch [7/10], Training Loss: 0.998, Validation Accuracy: 58.69%\n",
            "Epoch [8/10], Training Loss: 0.980, Validation Accuracy: 58.46%\n",
            "Epoch [9/10], Training Loss: 0.971, Validation Accuracy: 58.05%\n",
            "Epoch [10/10], Training Loss: 0.966, Validation Accuracy: 58.08%\n",
            "Epoch [1/10], Training Loss: 1.082, Validation Accuracy: 59.17%\n",
            "Epoch [2/10], Training Loss: 1.045, Validation Accuracy: 58.89%\n",
            "Epoch [3/10], Training Loss: 1.008, Validation Accuracy: 59.32%\n",
            "Epoch [4/10], Training Loss: 0.989, Validation Accuracy: 59.06%\n",
            "Epoch [5/10], Training Loss: 0.970, Validation Accuracy: 59.05%\n",
            "Epoch [6/10], Training Loss: 0.969, Validation Accuracy: 58.46%\n",
            "Epoch [7/10], Training Loss: 0.948, Validation Accuracy: 58.93%\n",
            "Epoch [8/10], Training Loss: 0.928, Validation Accuracy: 59.25%\n",
            "Epoch [9/10], Training Loss: 0.922, Validation Accuracy: 58.55%\n",
            "Epoch [10/10], Training Loss: 0.909, Validation Accuracy: 59.16%\n",
            "Epoch [1/10], Training Loss: 1.083, Validation Accuracy: 58.09%\n",
            "Epoch [2/10], Training Loss: 1.037, Validation Accuracy: 58.92%\n",
            "Epoch [3/10], Training Loss: 1.009, Validation Accuracy: 59.53%\n",
            "Epoch [4/10], Training Loss: 0.997, Validation Accuracy: 59.30%\n",
            "Epoch [5/10], Training Loss: 0.976, Validation Accuracy: 59.31%\n",
            "Epoch [6/10], Training Loss: 0.957, Validation Accuracy: 59.25%\n",
            "Epoch [7/10], Training Loss: 0.941, Validation Accuracy: 59.15%\n",
            "Epoch [8/10], Training Loss: 0.933, Validation Accuracy: 58.49%\n",
            "Epoch [9/10], Training Loss: 0.922, Validation Accuracy: 59.30%\n",
            "Epoch [10/10], Training Loss: 0.903, Validation Accuracy: 58.33%\n",
            "Epoch [1/10], Training Loss: 1.104, Validation Accuracy: 59.12%\n",
            "Epoch [2/10], Training Loss: 1.048, Validation Accuracy: 59.92%\n",
            "Epoch [3/10], Training Loss: 1.030, Validation Accuracy: 59.67%\n",
            "Epoch [4/10], Training Loss: 1.005, Validation Accuracy: 59.21%\n",
            "Epoch [5/10], Training Loss: 0.985, Validation Accuracy: 59.91%\n",
            "Epoch [6/10], Training Loss: 0.963, Validation Accuracy: 59.02%\n",
            "Epoch [7/10], Training Loss: 0.943, Validation Accuracy: 59.09%\n",
            "Epoch [8/10], Training Loss: 0.933, Validation Accuracy: 59.03%\n",
            "Epoch [9/10], Training Loss: 0.918, Validation Accuracy: 58.53%\n",
            "Epoch [10/10], Training Loss: 0.905, Validation Accuracy: 58.73%\n",
            "Epoch [1/10], Training Loss: 1.062, Validation Accuracy: 59.18%\n",
            "Epoch [2/10], Training Loss: 1.017, Validation Accuracy: 59.77%\n",
            "Epoch [3/10], Training Loss: 0.986, Validation Accuracy: 59.80%\n",
            "Epoch [4/10], Training Loss: 0.969, Validation Accuracy: 59.30%\n",
            "Epoch [5/10], Training Loss: 0.955, Validation Accuracy: 58.19%\n",
            "Epoch [6/10], Training Loss: 0.929, Validation Accuracy: 59.46%\n",
            "Epoch [7/10], Training Loss: 0.908, Validation Accuracy: 59.25%\n",
            "Epoch [8/10], Training Loss: 0.898, Validation Accuracy: 58.14%\n",
            "Epoch [9/10], Training Loss: 0.884, Validation Accuracy: 59.29%\n",
            "Epoch [10/10], Training Loss: 0.876, Validation Accuracy: 58.99%\n",
            "Epoch [1/10], Training Loss: 1.051, Validation Accuracy: 59.15%\n",
            "Epoch [2/10], Training Loss: 0.995, Validation Accuracy: 58.70%\n",
            "Epoch [3/10], Training Loss: 0.970, Validation Accuracy: 59.18%\n",
            "Epoch [4/10], Training Loss: 0.947, Validation Accuracy: 59.27%\n",
            "Epoch [5/10], Training Loss: 0.932, Validation Accuracy: 59.37%\n",
            "Epoch [6/10], Training Loss: 0.919, Validation Accuracy: 59.81%\n",
            "Epoch [7/10], Training Loss: 0.889, Validation Accuracy: 59.48%\n",
            "Epoch [8/10], Training Loss: 0.883, Validation Accuracy: 58.28%\n",
            "Epoch [9/10], Training Loss: 0.870, Validation Accuracy: 59.34%\n",
            "Epoch [10/10], Training Loss: 0.849, Validation Accuracy: 59.45%\n",
            "Epoch [1/10], Training Loss: 1.011, Validation Accuracy: 59.68%\n",
            "Epoch [2/10], Training Loss: 0.964, Validation Accuracy: 59.99%\n",
            "Epoch [3/10], Training Loss: 0.940, Validation Accuracy: 59.89%\n",
            "Epoch [4/10], Training Loss: 0.900, Validation Accuracy: 59.90%\n",
            "Epoch [5/10], Training Loss: 0.882, Validation Accuracy: 59.74%\n",
            "Epoch [6/10], Training Loss: 0.871, Validation Accuracy: 59.86%\n",
            "Epoch [7/10], Training Loss: 0.863, Validation Accuracy: 60.31%\n",
            "Epoch [8/10], Training Loss: 0.831, Validation Accuracy: 60.35%\n",
            "Epoch [9/10], Training Loss: 0.823, Validation Accuracy: 59.90%\n",
            "Epoch [10/10], Training Loss: 0.801, Validation Accuracy: 60.71%\n",
            "Epoch [1/10], Training Loss: 1.013, Validation Accuracy: 60.03%\n",
            "Epoch [2/10], Training Loss: 0.957, Validation Accuracy: 60.57%\n",
            "Epoch [3/10], Training Loss: 0.925, Validation Accuracy: 60.75%\n",
            "Epoch [4/10], Training Loss: 0.898, Validation Accuracy: 60.10%\n",
            "Epoch [5/10], Training Loss: 0.876, Validation Accuracy: 59.45%\n",
            "Epoch [6/10], Training Loss: 0.883, Validation Accuracy: 60.43%\n",
            "Epoch [7/10], Training Loss: 0.842, Validation Accuracy: 60.13%\n",
            "Epoch [8/10], Training Loss: 0.828, Validation Accuracy: 60.22%\n",
            "Epoch [9/10], Training Loss: 0.808, Validation Accuracy: 60.03%\n",
            "Epoch [10/10], Training Loss: 0.796, Validation Accuracy: 60.10%\n",
            "Epoch [1/10], Training Loss: 1.029, Validation Accuracy: 60.26%\n",
            "Epoch [2/10], Training Loss: 0.976, Validation Accuracy: 60.11%\n",
            "Epoch [3/10], Training Loss: 0.939, Validation Accuracy: 60.66%\n",
            "Epoch [4/10], Training Loss: 0.913, Validation Accuracy: 59.77%\n",
            "Epoch [5/10], Training Loss: 0.887, Validation Accuracy: 60.06%\n",
            "Epoch [6/10], Training Loss: 0.870, Validation Accuracy: 60.48%\n",
            "Epoch [7/10], Training Loss: 0.844, Validation Accuracy: 59.27%\n",
            "Epoch [8/10], Training Loss: 0.832, Validation Accuracy: 60.23%\n",
            "Epoch [9/10], Training Loss: 0.833, Validation Accuracy: 59.22%\n",
            "Epoch [10/10], Training Loss: 0.804, Validation Accuracy: 60.18%\n",
            "Epoch [1/10], Training Loss: 1.000, Validation Accuracy: 59.84%\n",
            "Epoch [2/10], Training Loss: 0.943, Validation Accuracy: 60.24%\n",
            "Epoch [3/10], Training Loss: 0.908, Validation Accuracy: 60.64%\n",
            "Epoch [4/10], Training Loss: 0.877, Validation Accuracy: 60.94%\n",
            "Epoch [5/10], Training Loss: 0.863, Validation Accuracy: 60.03%\n",
            "Epoch [6/10], Training Loss: 0.832, Validation Accuracy: 60.21%\n",
            "Epoch [7/10], Training Loss: 0.812, Validation Accuracy: 59.36%\n",
            "Epoch [8/10], Training Loss: 0.808, Validation Accuracy: 60.15%\n",
            "Epoch [9/10], Training Loss: 0.783, Validation Accuracy: 59.48%\n",
            "Epoch [10/10], Training Loss: 0.770, Validation Accuracy: 59.62%\n",
            "Epoch [1/10], Training Loss: 1.001, Validation Accuracy: 59.43%\n",
            "Epoch [2/10], Training Loss: 0.924, Validation Accuracy: 60.80%\n",
            "Epoch [3/10], Training Loss: 0.897, Validation Accuracy: 60.23%\n",
            "Epoch [4/10], Training Loss: 0.862, Validation Accuracy: 59.61%\n",
            "Epoch [5/10], Training Loss: 0.838, Validation Accuracy: 59.83%\n",
            "Epoch [6/10], Training Loss: 0.823, Validation Accuracy: 59.76%\n",
            "Epoch [7/10], Training Loss: 0.803, Validation Accuracy: 59.27%\n",
            "Epoch [8/10], Training Loss: 0.790, Validation Accuracy: 59.21%\n",
            "Epoch [9/10], Training Loss: 0.766, Validation Accuracy: 59.80%\n",
            "Epoch [10/10], Training Loss: 0.753, Validation Accuracy: 59.81%\n",
            "Epoch [1/10], Training Loss: 0.949, Validation Accuracy: 60.85%\n",
            "Epoch [2/10], Training Loss: 0.885, Validation Accuracy: 59.27%\n",
            "Epoch [3/10], Training Loss: 0.852, Validation Accuracy: 61.02%\n",
            "Epoch [4/10], Training Loss: 0.829, Validation Accuracy: 60.67%\n",
            "Epoch [5/10], Training Loss: 0.792, Validation Accuracy: 60.57%\n",
            "Epoch [6/10], Training Loss: 0.783, Validation Accuracy: 61.05%\n",
            "Epoch [7/10], Training Loss: 0.752, Validation Accuracy: 60.08%\n",
            "Epoch [8/10], Training Loss: 0.734, Validation Accuracy: 60.84%\n",
            "Epoch [9/10], Training Loss: 0.728, Validation Accuracy: 59.90%\n",
            "Epoch [10/10], Training Loss: 0.701, Validation Accuracy: 60.73%\n",
            "Epoch [1/10], Training Loss: 0.956, Validation Accuracy: 60.35%\n",
            "Epoch [2/10], Training Loss: 0.889, Validation Accuracy: 60.74%\n",
            "Epoch [3/10], Training Loss: 0.854, Validation Accuracy: 61.06%\n",
            "Epoch [4/10], Training Loss: 0.828, Validation Accuracy: 59.90%\n",
            "Epoch [5/10], Training Loss: 0.793, Validation Accuracy: 60.35%\n",
            "Epoch [6/10], Training Loss: 0.765, Validation Accuracy: 61.11%\n",
            "Epoch [7/10], Training Loss: 0.746, Validation Accuracy: 60.79%\n",
            "Epoch [8/10], Training Loss: 0.724, Validation Accuracy: 59.88%\n",
            "Epoch [9/10], Training Loss: 0.715, Validation Accuracy: 60.81%\n",
            "Epoch [10/10], Training Loss: 0.698, Validation Accuracy: 60.85%\n",
            "Epoch [1/10], Training Loss: 0.973, Validation Accuracy: 60.87%\n",
            "Epoch [2/10], Training Loss: 0.898, Validation Accuracy: 60.46%\n",
            "Epoch [3/10], Training Loss: 0.862, Validation Accuracy: 60.97%\n",
            "Epoch [4/10], Training Loss: 0.830, Validation Accuracy: 60.83%\n",
            "Epoch [5/10], Training Loss: 0.802, Validation Accuracy: 60.24%\n",
            "Epoch [6/10], Training Loss: 0.782, Validation Accuracy: 58.28%\n",
            "Epoch [7/10], Training Loss: 0.764, Validation Accuracy: 60.56%\n",
            "Epoch [8/10], Training Loss: 0.741, Validation Accuracy: 59.61%\n",
            "Epoch [9/10], Training Loss: 0.723, Validation Accuracy: 60.47%\n",
            "Epoch [10/10], Training Loss: 0.712, Validation Accuracy: 60.55%\n",
            "Epoch [1/10], Training Loss: 0.949, Validation Accuracy: 60.51%\n",
            "Epoch [2/10], Training Loss: 0.870, Validation Accuracy: 60.22%\n",
            "Epoch [3/10], Training Loss: 0.832, Validation Accuracy: 60.70%\n",
            "Epoch [4/10], Training Loss: 0.803, Validation Accuracy: 60.16%\n",
            "Epoch [5/10], Training Loss: 0.777, Validation Accuracy: 61.11%\n",
            "Epoch [6/10], Training Loss: 0.744, Validation Accuracy: 60.65%\n",
            "Epoch [7/10], Training Loss: 0.725, Validation Accuracy: 60.12%\n",
            "Epoch [8/10], Training Loss: 0.705, Validation Accuracy: 60.93%\n",
            "Epoch [9/10], Training Loss: 0.697, Validation Accuracy: 60.16%\n",
            "Epoch [10/10], Training Loss: 0.675, Validation Accuracy: 59.91%\n",
            "Epoch [1/10], Training Loss: 0.946, Validation Accuracy: 60.56%\n",
            "Epoch [2/10], Training Loss: 0.867, Validation Accuracy: 60.42%\n",
            "Epoch [3/10], Training Loss: 0.813, Validation Accuracy: 60.56%\n",
            "Epoch [4/10], Training Loss: 0.780, Validation Accuracy: 60.42%\n",
            "Epoch [5/10], Training Loss: 0.755, Validation Accuracy: 60.46%\n",
            "Epoch [6/10], Training Loss: 0.734, Validation Accuracy: 60.21%\n",
            "Epoch [7/10], Training Loss: 0.712, Validation Accuracy: 59.57%\n",
            "Epoch [8/10], Training Loss: 0.698, Validation Accuracy: 59.96%\n",
            "Epoch [9/10], Training Loss: 0.679, Validation Accuracy: 60.34%\n",
            "Epoch [10/10], Training Loss: 0.654, Validation Accuracy: 59.74%\n",
            "Epoch [1/10], Training Loss: 0.913, Validation Accuracy: 60.21%\n",
            "Epoch [2/10], Training Loss: 0.829, Validation Accuracy: 61.27%\n",
            "Epoch [3/10], Training Loss: 0.779, Validation Accuracy: 61.02%\n",
            "Epoch [4/10], Training Loss: 0.741, Validation Accuracy: 61.25%\n",
            "Epoch [5/10], Training Loss: 0.711, Validation Accuracy: 60.70%\n",
            "Epoch [6/10], Training Loss: 0.702, Validation Accuracy: 61.68%\n",
            "Epoch [7/10], Training Loss: 0.676, Validation Accuracy: 61.07%\n",
            "Epoch [8/10], Training Loss: 0.652, Validation Accuracy: 60.78%\n",
            "Epoch [9/10], Training Loss: 0.639, Validation Accuracy: 60.66%\n",
            "Epoch [10/10], Training Loss: 0.611, Validation Accuracy: 60.38%\n",
            "Epoch [1/10], Training Loss: 0.903, Validation Accuracy: 60.11%\n",
            "Epoch [2/10], Training Loss: 0.836, Validation Accuracy: 61.18%\n",
            "Epoch [3/10], Training Loss: 0.774, Validation Accuracy: 60.79%\n",
            "Epoch [4/10], Training Loss: 0.743, Validation Accuracy: 61.50%\n",
            "Epoch [5/10], Training Loss: 0.702, Validation Accuracy: 61.38%\n",
            "Epoch [6/10], Training Loss: 0.682, Validation Accuracy: 60.94%\n",
            "Epoch [7/10], Training Loss: 0.664, Validation Accuracy: 61.20%\n",
            "Epoch [8/10], Training Loss: 0.633, Validation Accuracy: 60.75%\n",
            "Epoch [9/10], Training Loss: 0.621, Validation Accuracy: 60.66%\n",
            "Epoch [10/10], Training Loss: 0.608, Validation Accuracy: 59.98%\n",
            "Epoch [1/10], Training Loss: 0.931, Validation Accuracy: 60.48%\n",
            "Epoch [2/10], Training Loss: 0.841, Validation Accuracy: 61.01%\n",
            "Epoch [3/10], Training Loss: 0.798, Validation Accuracy: 60.76%\n",
            "Epoch [4/10], Training Loss: 0.757, Validation Accuracy: 60.13%\n",
            "Epoch [5/10], Training Loss: 0.725, Validation Accuracy: 60.40%\n",
            "Epoch [6/10], Training Loss: 0.697, Validation Accuracy: 60.25%\n",
            "Epoch [7/10], Training Loss: 0.678, Validation Accuracy: 59.88%\n",
            "Epoch [8/10], Training Loss: 0.665, Validation Accuracy: 60.35%\n",
            "Epoch [9/10], Training Loss: 0.641, Validation Accuracy: 59.93%\n",
            "Epoch [10/10], Training Loss: 0.614, Validation Accuracy: 59.81%\n",
            "Epoch [1/10], Training Loss: 0.903, Validation Accuracy: 60.22%\n",
            "Epoch [2/10], Training Loss: 0.807, Validation Accuracy: 60.91%\n",
            "Epoch [3/10], Training Loss: 0.770, Validation Accuracy: 59.88%\n",
            "Epoch [4/10], Training Loss: 0.717, Validation Accuracy: 61.02%\n",
            "Epoch [5/10], Training Loss: 0.684, Validation Accuracy: 61.44%\n",
            "Epoch [6/10], Training Loss: 0.662, Validation Accuracy: 60.96%\n",
            "Epoch [7/10], Training Loss: 0.649, Validation Accuracy: 59.97%\n",
            "Epoch [8/10], Training Loss: 0.625, Validation Accuracy: 60.73%\n",
            "Epoch [9/10], Training Loss: 0.598, Validation Accuracy: 60.36%\n",
            "Epoch [10/10], Training Loss: 0.585, Validation Accuracy: 60.41%\n",
            "Epoch [1/10], Training Loss: 0.894, Validation Accuracy: 59.97%\n",
            "Epoch [2/10], Training Loss: 0.792, Validation Accuracy: 60.02%\n",
            "Epoch [3/10], Training Loss: 0.741, Validation Accuracy: 60.39%\n",
            "Epoch [4/10], Training Loss: 0.698, Validation Accuracy: 60.41%\n",
            "Epoch [5/10], Training Loss: 0.677, Validation Accuracy: 60.86%\n",
            "Epoch [6/10], Training Loss: 0.642, Validation Accuracy: 60.44%\n",
            "Epoch [7/10], Training Loss: 0.629, Validation Accuracy: 60.11%\n",
            "Epoch [8/10], Training Loss: 0.602, Validation Accuracy: 59.67%\n",
            "Epoch [9/10], Training Loss: 0.592, Validation Accuracy: 59.98%\n",
            "Epoch [10/10], Training Loss: 0.561, Validation Accuracy: 60.58%\n",
            "Epoch [1/10], Training Loss: 0.878, Validation Accuracy: 60.97%\n",
            "Epoch [2/10], Training Loss: 0.765, Validation Accuracy: 60.23%\n",
            "Epoch [3/10], Training Loss: 0.707, Validation Accuracy: 60.62%\n",
            "Epoch [4/10], Training Loss: 0.674, Validation Accuracy: 60.85%\n",
            "Epoch [5/10], Training Loss: 0.643, Validation Accuracy: 60.47%\n",
            "Epoch [6/10], Training Loss: 0.608, Validation Accuracy: 60.81%\n",
            "Epoch [7/10], Training Loss: 0.595, Validation Accuracy: 60.40%\n",
            "Epoch [8/10], Training Loss: 0.561, Validation Accuracy: 60.93%\n",
            "Epoch [9/10], Training Loss: 0.539, Validation Accuracy: 60.77%\n",
            "Epoch [10/10], Training Loss: 0.529, Validation Accuracy: 60.81%\n",
            "Epoch [1/10], Training Loss: 0.880, Validation Accuracy: 59.71%\n",
            "Epoch [2/10], Training Loss: 0.795, Validation Accuracy: 60.67%\n",
            "Epoch [3/10], Training Loss: 0.709, Validation Accuracy: 61.31%\n",
            "Epoch [4/10], Training Loss: 0.662, Validation Accuracy: 61.31%\n",
            "Epoch [5/10], Training Loss: 0.643, Validation Accuracy: 61.05%\n",
            "Epoch [6/10], Training Loss: 0.597, Validation Accuracy: 60.61%\n",
            "Epoch [7/10], Training Loss: 0.589, Validation Accuracy: 60.31%\n",
            "Epoch [8/10], Training Loss: 0.554, Validation Accuracy: 61.02%\n",
            "Epoch [9/10], Training Loss: 0.534, Validation Accuracy: 60.62%\n",
            "Epoch [10/10], Training Loss: 0.517, Validation Accuracy: 60.45%\n",
            "Epoch [1/10], Training Loss: 0.892, Validation Accuracy: 60.27%\n",
            "Epoch [2/10], Training Loss: 0.775, Validation Accuracy: 59.74%\n",
            "Epoch [3/10], Training Loss: 0.725, Validation Accuracy: 60.52%\n",
            "Epoch [4/10], Training Loss: 0.673, Validation Accuracy: 60.05%\n",
            "Epoch [5/10], Training Loss: 0.646, Validation Accuracy: 60.39%\n",
            "Epoch [6/10], Training Loss: 0.615, Validation Accuracy: 59.88%\n",
            "Epoch [7/10], Training Loss: 0.587, Validation Accuracy: 60.10%\n",
            "Epoch [8/10], Training Loss: 0.559, Validation Accuracy: 60.04%\n",
            "Epoch [9/10], Training Loss: 0.542, Validation Accuracy: 59.52%\n",
            "Epoch [10/10], Training Loss: 0.530, Validation Accuracy: 60.11%\n",
            "Epoch [1/10], Training Loss: 0.858, Validation Accuracy: 60.09%\n",
            "Epoch [2/10], Training Loss: 0.751, Validation Accuracy: 60.43%\n",
            "Epoch [3/10], Training Loss: 0.689, Validation Accuracy: 60.69%\n",
            "Epoch [4/10], Training Loss: 0.652, Validation Accuracy: 60.70%\n",
            "Epoch [5/10], Training Loss: 0.617, Validation Accuracy: 60.86%\n",
            "Epoch [6/10], Training Loss: 0.592, Validation Accuracy: 60.24%\n",
            "Epoch [7/10], Training Loss: 0.559, Validation Accuracy: 60.49%\n",
            "Epoch [8/10], Training Loss: 0.535, Validation Accuracy: 60.06%\n",
            "Epoch [9/10], Training Loss: 0.519, Validation Accuracy: 59.94%\n",
            "Epoch [10/10], Training Loss: 0.494, Validation Accuracy: 60.37%\n",
            "Epoch [1/10], Training Loss: 0.860, Validation Accuracy: 59.62%\n",
            "Epoch [2/10], Training Loss: 0.737, Validation Accuracy: 59.86%\n",
            "Epoch [3/10], Training Loss: 0.677, Validation Accuracy: 60.00%\n",
            "Epoch [4/10], Training Loss: 0.645, Validation Accuracy: 60.52%\n",
            "Epoch [5/10], Training Loss: 0.615, Validation Accuracy: 60.38%\n",
            "Epoch [6/10], Training Loss: 0.576, Validation Accuracy: 60.29%\n",
            "Epoch [7/10], Training Loss: 0.540, Validation Accuracy: 59.76%\n",
            "Epoch [8/10], Training Loss: 0.522, Validation Accuracy: 60.58%\n",
            "Epoch [9/10], Training Loss: 0.500, Validation Accuracy: 60.66%\n",
            "Epoch [10/10], Training Loss: 0.477, Validation Accuracy: 60.19%\n",
            "Epoch [1/10], Training Loss: 0.839, Validation Accuracy: 60.76%\n",
            "Epoch [2/10], Training Loss: 0.719, Validation Accuracy: 59.78%\n",
            "Epoch [3/10], Training Loss: 0.660, Validation Accuracy: 61.08%\n",
            "Epoch [4/10], Training Loss: 0.597, Validation Accuracy: 61.16%\n",
            "Epoch [5/10], Training Loss: 0.558, Validation Accuracy: 60.38%\n",
            "Epoch [6/10], Training Loss: 0.530, Validation Accuracy: 61.15%\n",
            "Epoch [7/10], Training Loss: 0.502, Validation Accuracy: 60.73%\n",
            "Epoch [8/10], Training Loss: 0.484, Validation Accuracy: 60.70%\n",
            "Epoch [9/10], Training Loss: 0.458, Validation Accuracy: 60.30%\n",
            "Epoch [10/10], Training Loss: 0.454, Validation Accuracy: 60.48%\n",
            "Confusion Matrix:\n",
            "[[661  32  56  25  35  18  14  18  94  47]\n",
            " [ 35 744  10  17   5  16  17  20  58  78]\n",
            " [ 73  11 451  76 115 122  76  44  24   8]\n",
            " [ 25  19  67 429  69 210  58  80  23  20]\n",
            " [ 34  15  96  61 524  70  66 114  15   5]\n",
            " [ 16   8  67 177  52 547  25  92   9   7]\n",
            " [  7  16  61  84  80  59 656  16  11  10]\n",
            " [ 11  10  36  54  64  94  12 700   4  15]\n",
            " [ 82  53  26  22  17  15  13  13 726  33]\n",
            " [ 52 177  11  38   8  34  15  43  54 568]]\n",
            "Test Accuracy: 60.06%\n",
            "True Positives (TP): [661 744 451 429 524 547 656 700 726 568]\n",
            "False Positives (FP): [335 341 430 554 445 638 296 440 292 223]\n",
            "True Negatives (TN): [8665 8659 8570 8446 8555 8362 8704 8560 8708 8777]\n",
            "False Negatives (FN): [339 256 549 571 476 453 344 300 274 432]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.66365462 0.68571429 0.51191827 0.43641913 0.54076367 0.46160338\n",
            " 0.68907563 0.61403509 0.71316306 0.71807838]\n",
            "Recall: [0.661 0.744 0.451 0.429 0.524 0.547 0.656 0.7   0.726 0.568]\n",
            "F1 Score: [0.66232465 0.71366906 0.47953216 0.43267776 0.53224987 0.5006865\n",
            " 0.67213115 0.65420561 0.71952428 0.63428252]\n",
            "CPU times: user 3h 12min 5s, sys: 1min 21s, total: 3h 13min 27s\n",
            "Wall time: 3h 28min 42s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 2):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=2):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_uniform: Dict ) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "    mean = distribution_info_uniform[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info_uniform[\"std\"].mean().item()  # Convert numpy array to float\n",
        "\n",
        "\n",
        "\n",
        "     # Generate augmented data using Uniform distribution\n",
        "    augmented_data_uniform = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = augmented_data_uniform\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=2)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"uniform\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iax40oN8Qj_a"
      },
      "source": [
        "beta=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qigq3i2NQqTl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_CRiA32QqxE",
        "outputId": "84f5e58b-5767-4bcf-cfde-46d023b2f86e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 28.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Images per Class: [5847 6077 6117 5960 5855 6092 6030 6089 5927 6006]\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 3):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=3):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_uniform: Dict ) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "    mean = distribution_info_uniform[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info_uniform[\"std\"].mean().item()  # Convert numpy array to float\n",
        "\n",
        "\n",
        "\n",
        "     # Generate augmented data using Uniform distribution\n",
        "    augmented_data_uniform = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = augmented_data_uniform\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=3)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"uniform\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5EiRDkYofAaWivXYkwiKK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}