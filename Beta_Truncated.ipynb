{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxLV5G6nG6BSrWznuir1Pj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahad31/Different-VAE-for-KL-FedDis/blob/main/Beta_Truncated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c42iSuJFslZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "beta =0.1"
      ],
      "metadata": {
        "id": "Gva6rnugFy3W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nKZlTAPBF2Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYAwS10pnHSq",
        "outputId": "12db6539-3a76-4232-f1ea-a6c568c4151c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 36.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Images per Class: [5998 5905 6051 6013 6016 6018 6072 5985 5944 5998]\n",
            "Epoch [1/10], Training Loss: 2.306, Validation Accuracy: 10.17%\n",
            "Epoch [2/10], Training Loss: 2.305, Validation Accuracy: 10.37%\n",
            "Epoch [3/10], Training Loss: 2.304, Validation Accuracy: 10.96%\n",
            "Epoch [4/10], Training Loss: 2.303, Validation Accuracy: 11.13%\n",
            "Epoch [5/10], Training Loss: 2.302, Validation Accuracy: 11.34%\n",
            "Epoch [6/10], Training Loss: 2.300, Validation Accuracy: 12.23%\n",
            "Epoch [7/10], Training Loss: 2.299, Validation Accuracy: 13.75%\n",
            "Epoch [8/10], Training Loss: 2.298, Validation Accuracy: 15.21%\n",
            "Epoch [9/10], Training Loss: 2.296, Validation Accuracy: 16.59%\n",
            "Epoch [10/10], Training Loss: 2.293, Validation Accuracy: 17.35%\n",
            "Epoch [1/10], Training Loss: 2.290, Validation Accuracy: 16.91%\n",
            "Epoch [2/10], Training Loss: 2.286, Validation Accuracy: 17.25%\n",
            "Epoch [3/10], Training Loss: 2.280, Validation Accuracy: 19.94%\n",
            "Epoch [4/10], Training Loss: 2.271, Validation Accuracy: 19.86%\n",
            "Epoch [5/10], Training Loss: 2.257, Validation Accuracy: 19.24%\n",
            "Epoch [6/10], Training Loss: 2.236, Validation Accuracy: 19.16%\n",
            "Epoch [7/10], Training Loss: 2.206, Validation Accuracy: 20.65%\n",
            "Epoch [8/10], Training Loss: 2.165, Validation Accuracy: 22.48%\n",
            "Epoch [9/10], Training Loss: 2.115, Validation Accuracy: 23.43%\n",
            "Epoch [10/10], Training Loss: 2.073, Validation Accuracy: 25.96%\n",
            "Epoch [1/10], Training Loss: 2.055, Validation Accuracy: 26.87%\n",
            "Epoch [2/10], Training Loss: 2.027, Validation Accuracy: 27.20%\n",
            "Epoch [3/10], Training Loss: 1.999, Validation Accuracy: 27.88%\n",
            "Epoch [4/10], Training Loss: 1.972, Validation Accuracy: 28.41%\n",
            "Epoch [5/10], Training Loss: 1.949, Validation Accuracy: 29.16%\n",
            "Epoch [6/10], Training Loss: 1.927, Validation Accuracy: 29.87%\n",
            "Epoch [7/10], Training Loss: 1.904, Validation Accuracy: 30.60%\n",
            "Epoch [8/10], Training Loss: 1.885, Validation Accuracy: 30.76%\n",
            "Epoch [9/10], Training Loss: 1.865, Validation Accuracy: 32.25%\n",
            "Epoch [10/10], Training Loss: 1.844, Validation Accuracy: 32.61%\n",
            "Epoch [1/10], Training Loss: 1.854, Validation Accuracy: 32.83%\n",
            "Epoch [2/10], Training Loss: 1.837, Validation Accuracy: 32.95%\n",
            "Epoch [3/10], Training Loss: 1.827, Validation Accuracy: 34.33%\n",
            "Epoch [4/10], Training Loss: 1.810, Validation Accuracy: 34.25%\n",
            "Epoch [5/10], Training Loss: 1.794, Validation Accuracy: 35.48%\n",
            "Epoch [6/10], Training Loss: 1.779, Validation Accuracy: 35.77%\n",
            "Epoch [7/10], Training Loss: 1.766, Validation Accuracy: 35.59%\n",
            "Epoch [8/10], Training Loss: 1.752, Validation Accuracy: 36.35%\n",
            "Epoch [9/10], Training Loss: 1.742, Validation Accuracy: 37.08%\n",
            "Epoch [10/10], Training Loss: 1.726, Validation Accuracy: 36.63%\n",
            "Epoch [1/10], Training Loss: 1.711, Validation Accuracy: 37.89%\n",
            "Epoch [2/10], Training Loss: 1.692, Validation Accuracy: 37.13%\n",
            "Epoch [3/10], Training Loss: 1.677, Validation Accuracy: 39.06%\n",
            "Epoch [4/10], Training Loss: 1.661, Validation Accuracy: 39.02%\n",
            "Epoch [5/10], Training Loss: 1.645, Validation Accuracy: 39.23%\n",
            "Epoch [6/10], Training Loss: 1.633, Validation Accuracy: 39.98%\n",
            "Epoch [7/10], Training Loss: 1.617, Validation Accuracy: 40.24%\n",
            "Epoch [8/10], Training Loss: 1.612, Validation Accuracy: 40.55%\n",
            "Epoch [9/10], Training Loss: 1.596, Validation Accuracy: 40.36%\n",
            "Epoch [10/10], Training Loss: 1.585, Validation Accuracy: 41.29%\n",
            "Epoch [1/10], Training Loss: 1.606, Validation Accuracy: 41.05%\n",
            "Epoch [2/10], Training Loss: 1.595, Validation Accuracy: 41.23%\n",
            "Epoch [3/10], Training Loss: 1.587, Validation Accuracy: 40.98%\n",
            "Epoch [4/10], Training Loss: 1.571, Validation Accuracy: 42.75%\n",
            "Epoch [5/10], Training Loss: 1.564, Validation Accuracy: 43.47%\n",
            "Epoch [6/10], Training Loss: 1.553, Validation Accuracy: 42.11%\n",
            "Epoch [7/10], Training Loss: 1.539, Validation Accuracy: 44.06%\n",
            "Epoch [8/10], Training Loss: 1.534, Validation Accuracy: 42.64%\n",
            "Epoch [9/10], Training Loss: 1.517, Validation Accuracy: 43.45%\n",
            "Epoch [10/10], Training Loss: 1.508, Validation Accuracy: 43.24%\n",
            "Epoch [1/10], Training Loss: 1.517, Validation Accuracy: 44.45%\n",
            "Epoch [2/10], Training Loss: 1.506, Validation Accuracy: 43.98%\n",
            "Epoch [3/10], Training Loss: 1.496, Validation Accuracy: 45.41%\n",
            "Epoch [4/10], Training Loss: 1.482, Validation Accuracy: 44.69%\n",
            "Epoch [5/10], Training Loss: 1.475, Validation Accuracy: 45.74%\n",
            "Epoch [6/10], Training Loss: 1.468, Validation Accuracy: 46.63%\n",
            "Epoch [7/10], Training Loss: 1.455, Validation Accuracy: 46.01%\n",
            "Epoch [8/10], Training Loss: 1.452, Validation Accuracy: 46.62%\n",
            "Epoch [9/10], Training Loss: 1.432, Validation Accuracy: 46.33%\n",
            "Epoch [10/10], Training Loss: 1.434, Validation Accuracy: 46.48%\n",
            "Epoch [1/10], Training Loss: 1.472, Validation Accuracy: 46.69%\n",
            "Epoch [2/10], Training Loss: 1.455, Validation Accuracy: 47.19%\n",
            "Epoch [3/10], Training Loss: 1.447, Validation Accuracy: 47.07%\n",
            "Epoch [4/10], Training Loss: 1.443, Validation Accuracy: 48.16%\n",
            "Epoch [5/10], Training Loss: 1.430, Validation Accuracy: 47.81%\n",
            "Epoch [6/10], Training Loss: 1.426, Validation Accuracy: 47.69%\n",
            "Epoch [7/10], Training Loss: 1.423, Validation Accuracy: 47.63%\n",
            "Epoch [8/10], Training Loss: 1.403, Validation Accuracy: 48.57%\n",
            "Epoch [9/10], Training Loss: 1.402, Validation Accuracy: 48.89%\n",
            "Epoch [10/10], Training Loss: 1.392, Validation Accuracy: 47.36%\n",
            "Epoch [1/10], Training Loss: 1.453, Validation Accuracy: 48.73%\n",
            "Epoch [2/10], Training Loss: 1.430, Validation Accuracy: 49.20%\n",
            "Epoch [3/10], Training Loss: 1.422, Validation Accuracy: 48.80%\n",
            "Epoch [4/10], Training Loss: 1.421, Validation Accuracy: 49.48%\n",
            "Epoch [5/10], Training Loss: 1.413, Validation Accuracy: 49.57%\n",
            "Epoch [6/10], Training Loss: 1.399, Validation Accuracy: 49.04%\n",
            "Epoch [7/10], Training Loss: 1.394, Validation Accuracy: 49.03%\n",
            "Epoch [8/10], Training Loss: 1.384, Validation Accuracy: 50.06%\n",
            "Epoch [9/10], Training Loss: 1.381, Validation Accuracy: 49.28%\n",
            "Epoch [10/10], Training Loss: 1.376, Validation Accuracy: 49.40%\n",
            "Epoch [1/10], Training Loss: 1.359, Validation Accuracy: 49.88%\n",
            "Epoch [2/10], Training Loss: 1.346, Validation Accuracy: 49.68%\n",
            "Epoch [3/10], Training Loss: 1.331, Validation Accuracy: 50.45%\n",
            "Epoch [4/10], Training Loss: 1.322, Validation Accuracy: 50.88%\n",
            "Epoch [5/10], Training Loss: 1.314, Validation Accuracy: 49.64%\n",
            "Epoch [6/10], Training Loss: 1.316, Validation Accuracy: 51.00%\n",
            "Epoch [7/10], Training Loss: 1.297, Validation Accuracy: 50.38%\n",
            "Epoch [8/10], Training Loss: 1.297, Validation Accuracy: 51.20%\n",
            "Epoch [9/10], Training Loss: 1.285, Validation Accuracy: 49.37%\n",
            "Epoch [10/10], Training Loss: 1.283, Validation Accuracy: 50.90%\n",
            "Epoch [1/10], Training Loss: 1.339, Validation Accuracy: 50.90%\n",
            "Epoch [2/10], Training Loss: 1.324, Validation Accuracy: 51.12%\n",
            "Epoch [3/10], Training Loss: 1.318, Validation Accuracy: 51.35%\n",
            "Epoch [4/10], Training Loss: 1.301, Validation Accuracy: 52.18%\n",
            "Epoch [5/10], Training Loss: 1.291, Validation Accuracy: 50.66%\n",
            "Epoch [6/10], Training Loss: 1.286, Validation Accuracy: 51.23%\n",
            "Epoch [7/10], Training Loss: 1.280, Validation Accuracy: 50.52%\n",
            "Epoch [8/10], Training Loss: 1.277, Validation Accuracy: 51.60%\n",
            "Epoch [9/10], Training Loss: 1.258, Validation Accuracy: 52.32%\n",
            "Epoch [10/10], Training Loss: 1.260, Validation Accuracy: 52.22%\n",
            "Epoch [1/10], Training Loss: 1.312, Validation Accuracy: 52.64%\n",
            "Epoch [2/10], Training Loss: 1.298, Validation Accuracy: 51.80%\n",
            "Epoch [3/10], Training Loss: 1.286, Validation Accuracy: 53.25%\n",
            "Epoch [4/10], Training Loss: 1.273, Validation Accuracy: 53.28%\n",
            "Epoch [5/10], Training Loss: 1.263, Validation Accuracy: 53.28%\n",
            "Epoch [6/10], Training Loss: 1.246, Validation Accuracy: 53.49%\n",
            "Epoch [7/10], Training Loss: 1.245, Validation Accuracy: 52.72%\n",
            "Epoch [8/10], Training Loss: 1.238, Validation Accuracy: 52.70%\n",
            "Epoch [9/10], Training Loss: 1.228, Validation Accuracy: 53.63%\n",
            "Epoch [10/10], Training Loss: 1.221, Validation Accuracy: 53.62%\n",
            "Epoch [1/10], Training Loss: 1.286, Validation Accuracy: 53.82%\n",
            "Epoch [2/10], Training Loss: 1.277, Validation Accuracy: 53.83%\n",
            "Epoch [3/10], Training Loss: 1.255, Validation Accuracy: 54.08%\n",
            "Epoch [4/10], Training Loss: 1.249, Validation Accuracy: 53.42%\n",
            "Epoch [5/10], Training Loss: 1.242, Validation Accuracy: 53.32%\n",
            "Epoch [6/10], Training Loss: 1.233, Validation Accuracy: 53.99%\n",
            "Epoch [7/10], Training Loss: 1.220, Validation Accuracy: 54.00%\n",
            "Epoch [8/10], Training Loss: 1.217, Validation Accuracy: 54.65%\n",
            "Epoch [9/10], Training Loss: 1.204, Validation Accuracy: 54.54%\n",
            "Epoch [10/10], Training Loss: 1.204, Validation Accuracy: 54.46%\n",
            "Epoch [1/10], Training Loss: 1.285, Validation Accuracy: 54.80%\n",
            "Epoch [2/10], Training Loss: 1.263, Validation Accuracy: 54.87%\n",
            "Epoch [3/10], Training Loss: 1.249, Validation Accuracy: 55.11%\n",
            "Epoch [4/10], Training Loss: 1.238, Validation Accuracy: 53.72%\n",
            "Epoch [5/10], Training Loss: 1.226, Validation Accuracy: 54.54%\n",
            "Epoch [6/10], Training Loss: 1.212, Validation Accuracy: 54.78%\n",
            "Epoch [7/10], Training Loss: 1.208, Validation Accuracy: 54.60%\n",
            "Epoch [8/10], Training Loss: 1.200, Validation Accuracy: 55.04%\n",
            "Epoch [9/10], Training Loss: 1.193, Validation Accuracy: 54.42%\n",
            "Epoch [10/10], Training Loss: 1.180, Validation Accuracy: 55.25%\n",
            "Epoch [1/10], Training Loss: 1.206, Validation Accuracy: 54.79%\n",
            "Epoch [2/10], Training Loss: 1.197, Validation Accuracy: 54.66%\n",
            "Epoch [3/10], Training Loss: 1.182, Validation Accuracy: 56.04%\n",
            "Epoch [4/10], Training Loss: 1.179, Validation Accuracy: 55.26%\n",
            "Epoch [5/10], Training Loss: 1.158, Validation Accuracy: 55.04%\n",
            "Epoch [6/10], Training Loss: 1.152, Validation Accuracy: 54.83%\n",
            "Epoch [7/10], Training Loss: 1.147, Validation Accuracy: 55.84%\n",
            "Epoch [8/10], Training Loss: 1.130, Validation Accuracy: 56.02%\n",
            "Epoch [9/10], Training Loss: 1.129, Validation Accuracy: 54.99%\n",
            "Epoch [10/10], Training Loss: 1.119, Validation Accuracy: 54.54%\n",
            "Epoch [1/10], Training Loss: 1.221, Validation Accuracy: 56.27%\n",
            "Epoch [2/10], Training Loss: 1.183, Validation Accuracy: 56.79%\n",
            "Epoch [3/10], Training Loss: 1.170, Validation Accuracy: 56.30%\n",
            "Epoch [4/10], Training Loss: 1.164, Validation Accuracy: 56.72%\n",
            "Epoch [5/10], Training Loss: 1.139, Validation Accuracy: 56.57%\n",
            "Epoch [6/10], Training Loss: 1.128, Validation Accuracy: 56.85%\n",
            "Epoch [7/10], Training Loss: 1.124, Validation Accuracy: 56.43%\n",
            "Epoch [8/10], Training Loss: 1.113, Validation Accuracy: 56.60%\n",
            "Epoch [9/10], Training Loss: 1.107, Validation Accuracy: 56.85%\n",
            "Epoch [10/10], Training Loss: 1.095, Validation Accuracy: 56.41%\n",
            "Epoch [1/10], Training Loss: 1.183, Validation Accuracy: 57.13%\n",
            "Epoch [2/10], Training Loss: 1.164, Validation Accuracy: 57.68%\n",
            "Epoch [3/10], Training Loss: 1.136, Validation Accuracy: 57.69%\n",
            "Epoch [4/10], Training Loss: 1.140, Validation Accuracy: 56.66%\n",
            "Epoch [5/10], Training Loss: 1.122, Validation Accuracy: 57.50%\n",
            "Epoch [6/10], Training Loss: 1.102, Validation Accuracy: 57.88%\n",
            "Epoch [7/10], Training Loss: 1.096, Validation Accuracy: 57.26%\n",
            "Epoch [8/10], Training Loss: 1.088, Validation Accuracy: 56.80%\n",
            "Epoch [9/10], Training Loss: 1.076, Validation Accuracy: 57.34%\n",
            "Epoch [10/10], Training Loss: 1.074, Validation Accuracy: 57.82%\n",
            "Epoch [1/10], Training Loss: 1.171, Validation Accuracy: 57.33%\n",
            "Epoch [2/10], Training Loss: 1.144, Validation Accuracy: 57.97%\n",
            "Epoch [3/10], Training Loss: 1.129, Validation Accuracy: 57.50%\n",
            "Epoch [4/10], Training Loss: 1.124, Validation Accuracy: 56.97%\n",
            "Epoch [5/10], Training Loss: 1.111, Validation Accuracy: 57.95%\n",
            "Epoch [6/10], Training Loss: 1.091, Validation Accuracy: 57.50%\n",
            "Epoch [7/10], Training Loss: 1.093, Validation Accuracy: 56.99%\n",
            "Epoch [8/10], Training Loss: 1.081, Validation Accuracy: 58.27%\n",
            "Epoch [9/10], Training Loss: 1.064, Validation Accuracy: 57.13%\n",
            "Epoch [10/10], Training Loss: 1.057, Validation Accuracy: 57.97%\n",
            "Epoch [1/10], Training Loss: 1.174, Validation Accuracy: 57.42%\n",
            "Epoch [2/10], Training Loss: 1.152, Validation Accuracy: 58.57%\n",
            "Epoch [3/10], Training Loss: 1.125, Validation Accuracy: 57.42%\n",
            "Epoch [4/10], Training Loss: 1.112, Validation Accuracy: 58.55%\n",
            "Epoch [5/10], Training Loss: 1.103, Validation Accuracy: 58.33%\n",
            "Epoch [6/10], Training Loss: 1.088, Validation Accuracy: 58.86%\n",
            "Epoch [7/10], Training Loss: 1.076, Validation Accuracy: 58.68%\n",
            "Epoch [8/10], Training Loss: 1.062, Validation Accuracy: 58.41%\n",
            "Epoch [9/10], Training Loss: 1.056, Validation Accuracy: 58.52%\n",
            "Epoch [10/10], Training Loss: 1.043, Validation Accuracy: 59.45%\n",
            "Epoch [1/10], Training Loss: 1.122, Validation Accuracy: 58.18%\n",
            "Epoch [2/10], Training Loss: 1.097, Validation Accuracy: 58.43%\n",
            "Epoch [3/10], Training Loss: 1.075, Validation Accuracy: 58.26%\n",
            "Epoch [4/10], Training Loss: 1.063, Validation Accuracy: 59.20%\n",
            "Epoch [5/10], Training Loss: 1.052, Validation Accuracy: 58.86%\n",
            "Epoch [6/10], Training Loss: 1.035, Validation Accuracy: 58.84%\n",
            "Epoch [7/10], Training Loss: 1.026, Validation Accuracy: 59.63%\n",
            "Epoch [8/10], Training Loss: 1.020, Validation Accuracy: 58.26%\n",
            "Epoch [9/10], Training Loss: 1.009, Validation Accuracy: 58.83%\n",
            "Epoch [10/10], Training Loss: 1.006, Validation Accuracy: 58.83%\n",
            "Epoch [1/10], Training Loss: 1.105, Validation Accuracy: 59.03%\n",
            "Epoch [2/10], Training Loss: 1.073, Validation Accuracy: 57.94%\n",
            "Epoch [3/10], Training Loss: 1.059, Validation Accuracy: 58.89%\n",
            "Epoch [4/10], Training Loss: 1.048, Validation Accuracy: 59.23%\n",
            "Epoch [5/10], Training Loss: 1.031, Validation Accuracy: 58.06%\n",
            "Epoch [6/10], Training Loss: 1.026, Validation Accuracy: 58.66%\n",
            "Epoch [7/10], Training Loss: 1.004, Validation Accuracy: 59.18%\n",
            "Epoch [8/10], Training Loss: 0.992, Validation Accuracy: 59.24%\n",
            "Epoch [9/10], Training Loss: 0.986, Validation Accuracy: 59.09%\n",
            "Epoch [10/10], Training Loss: 0.983, Validation Accuracy: 58.31%\n",
            "Epoch [1/10], Training Loss: 1.091, Validation Accuracy: 60.06%\n",
            "Epoch [2/10], Training Loss: 1.048, Validation Accuracy: 59.48%\n",
            "Epoch [3/10], Training Loss: 1.032, Validation Accuracy: 59.86%\n",
            "Epoch [4/10], Training Loss: 1.016, Validation Accuracy: 60.53%\n",
            "Epoch [5/10], Training Loss: 1.004, Validation Accuracy: 60.27%\n",
            "Epoch [6/10], Training Loss: 0.992, Validation Accuracy: 60.08%\n",
            "Epoch [7/10], Training Loss: 0.974, Validation Accuracy: 60.59%\n",
            "Epoch [8/10], Training Loss: 0.961, Validation Accuracy: 59.89%\n",
            "Epoch [9/10], Training Loss: 0.954, Validation Accuracy: 60.18%\n",
            "Epoch [10/10], Training Loss: 0.945, Validation Accuracy: 59.85%\n",
            "Epoch [1/10], Training Loss: 1.083, Validation Accuracy: 59.32%\n",
            "Epoch [2/10], Training Loss: 1.051, Validation Accuracy: 59.48%\n",
            "Epoch [3/10], Training Loss: 1.035, Validation Accuracy: 59.85%\n",
            "Epoch [4/10], Training Loss: 1.019, Validation Accuracy: 60.61%\n",
            "Epoch [5/10], Training Loss: 0.995, Validation Accuracy: 59.69%\n",
            "Epoch [6/10], Training Loss: 0.987, Validation Accuracy: 60.34%\n",
            "Epoch [7/10], Training Loss: 0.989, Validation Accuracy: 59.18%\n",
            "Epoch [8/10], Training Loss: 0.971, Validation Accuracy: 60.00%\n",
            "Epoch [9/10], Training Loss: 0.954, Validation Accuracy: 58.22%\n",
            "Epoch [10/10], Training Loss: 0.954, Validation Accuracy: 60.08%\n",
            "Epoch [1/10], Training Loss: 1.088, Validation Accuracy: 60.35%\n",
            "Epoch [2/10], Training Loss: 1.057, Validation Accuracy: 60.71%\n",
            "Epoch [3/10], Training Loss: 1.023, Validation Accuracy: 60.87%\n",
            "Epoch [4/10], Training Loss: 1.021, Validation Accuracy: 59.30%\n",
            "Epoch [5/10], Training Loss: 1.004, Validation Accuracy: 59.70%\n",
            "Epoch [6/10], Training Loss: 0.991, Validation Accuracy: 60.34%\n",
            "Epoch [7/10], Training Loss: 0.968, Validation Accuracy: 60.01%\n",
            "Epoch [8/10], Training Loss: 0.954, Validation Accuracy: 60.89%\n",
            "Epoch [9/10], Training Loss: 0.951, Validation Accuracy: 60.71%\n",
            "Epoch [10/10], Training Loss: 0.939, Validation Accuracy: 60.53%\n",
            "Epoch [1/10], Training Loss: 1.048, Validation Accuracy: 59.00%\n",
            "Epoch [2/10], Training Loss: 1.012, Validation Accuracy: 60.71%\n",
            "Epoch [3/10], Training Loss: 0.987, Validation Accuracy: 59.98%\n",
            "Epoch [4/10], Training Loss: 0.977, Validation Accuracy: 60.90%\n",
            "Epoch [5/10], Training Loss: 0.961, Validation Accuracy: 60.81%\n",
            "Epoch [6/10], Training Loss: 0.962, Validation Accuracy: 60.39%\n",
            "Epoch [7/10], Training Loss: 0.940, Validation Accuracy: 59.86%\n",
            "Epoch [8/10], Training Loss: 0.926, Validation Accuracy: 60.96%\n",
            "Epoch [9/10], Training Loss: 0.915, Validation Accuracy: 61.01%\n",
            "Epoch [10/10], Training Loss: 0.899, Validation Accuracy: 60.11%\n",
            "Epoch [1/10], Training Loss: 1.036, Validation Accuracy: 60.66%\n",
            "Epoch [2/10], Training Loss: 0.998, Validation Accuracy: 60.92%\n",
            "Epoch [3/10], Training Loss: 0.979, Validation Accuracy: 60.74%\n",
            "Epoch [4/10], Training Loss: 0.951, Validation Accuracy: 60.13%\n",
            "Epoch [5/10], Training Loss: 0.945, Validation Accuracy: 60.44%\n",
            "Epoch [6/10], Training Loss: 0.927, Validation Accuracy: 60.68%\n",
            "Epoch [7/10], Training Loss: 0.915, Validation Accuracy: 60.38%\n",
            "Epoch [8/10], Training Loss: 0.897, Validation Accuracy: 60.38%\n",
            "Epoch [9/10], Training Loss: 0.894, Validation Accuracy: 60.82%\n",
            "Epoch [10/10], Training Loss: 0.877, Validation Accuracy: 60.29%\n",
            "Epoch [1/10], Training Loss: 1.019, Validation Accuracy: 60.36%\n",
            "Epoch [2/10], Training Loss: 0.974, Validation Accuracy: 61.15%\n",
            "Epoch [3/10], Training Loss: 0.949, Validation Accuracy: 60.32%\n",
            "Epoch [4/10], Training Loss: 0.922, Validation Accuracy: 61.56%\n",
            "Epoch [5/10], Training Loss: 0.912, Validation Accuracy: 61.80%\n",
            "Epoch [6/10], Training Loss: 0.894, Validation Accuracy: 60.79%\n",
            "Epoch [7/10], Training Loss: 0.888, Validation Accuracy: 60.98%\n",
            "Epoch [8/10], Training Loss: 0.872, Validation Accuracy: 60.56%\n",
            "Epoch [9/10], Training Loss: 0.862, Validation Accuracy: 61.41%\n",
            "Epoch [10/10], Training Loss: 0.843, Validation Accuracy: 61.82%\n",
            "Epoch [1/10], Training Loss: 1.015, Validation Accuracy: 61.72%\n",
            "Epoch [2/10], Training Loss: 0.973, Validation Accuracy: 61.45%\n",
            "Epoch [3/10], Training Loss: 0.950, Validation Accuracy: 60.68%\n",
            "Epoch [4/10], Training Loss: 0.930, Validation Accuracy: 61.36%\n",
            "Epoch [5/10], Training Loss: 0.909, Validation Accuracy: 61.95%\n",
            "Epoch [6/10], Training Loss: 0.898, Validation Accuracy: 61.21%\n",
            "Epoch [7/10], Training Loss: 0.890, Validation Accuracy: 60.85%\n",
            "Epoch [8/10], Training Loss: 0.880, Validation Accuracy: 61.41%\n",
            "Epoch [9/10], Training Loss: 0.867, Validation Accuracy: 61.03%\n",
            "Epoch [10/10], Training Loss: 0.856, Validation Accuracy: 60.73%\n",
            "Epoch [1/10], Training Loss: 1.028, Validation Accuracy: 61.22%\n",
            "Epoch [2/10], Training Loss: 0.978, Validation Accuracy: 61.57%\n",
            "Epoch [3/10], Training Loss: 0.955, Validation Accuracy: 61.54%\n",
            "Epoch [4/10], Training Loss: 0.927, Validation Accuracy: 60.47%\n",
            "Epoch [5/10], Training Loss: 0.922, Validation Accuracy: 61.19%\n",
            "Epoch [6/10], Training Loss: 0.888, Validation Accuracy: 61.59%\n",
            "Epoch [7/10], Training Loss: 0.878, Validation Accuracy: 61.77%\n",
            "Epoch [8/10], Training Loss: 0.869, Validation Accuracy: 61.23%\n",
            "Epoch [9/10], Training Loss: 0.849, Validation Accuracy: 60.88%\n",
            "Epoch [10/10], Training Loss: 0.845, Validation Accuracy: 61.05%\n",
            "Epoch [1/10], Training Loss: 0.985, Validation Accuracy: 60.93%\n",
            "Epoch [2/10], Training Loss: 0.944, Validation Accuracy: 60.93%\n",
            "Epoch [3/10], Training Loss: 0.913, Validation Accuracy: 61.50%\n",
            "Epoch [4/10], Training Loss: 0.885, Validation Accuracy: 61.41%\n",
            "Epoch [5/10], Training Loss: 0.876, Validation Accuracy: 61.84%\n",
            "Epoch [6/10], Training Loss: 0.855, Validation Accuracy: 61.86%\n",
            "Epoch [7/10], Training Loss: 0.844, Validation Accuracy: 61.83%\n",
            "Epoch [8/10], Training Loss: 0.828, Validation Accuracy: 61.02%\n",
            "Epoch [9/10], Training Loss: 0.817, Validation Accuracy: 61.72%\n",
            "Epoch [10/10], Training Loss: 0.806, Validation Accuracy: 61.58%\n",
            "Epoch [1/10], Training Loss: 0.974, Validation Accuracy: 61.58%\n",
            "Epoch [2/10], Training Loss: 0.928, Validation Accuracy: 61.93%\n",
            "Epoch [3/10], Training Loss: 0.902, Validation Accuracy: 61.41%\n",
            "Epoch [4/10], Training Loss: 0.883, Validation Accuracy: 60.69%\n",
            "Epoch [5/10], Training Loss: 0.864, Validation Accuracy: 61.09%\n",
            "Epoch [6/10], Training Loss: 0.839, Validation Accuracy: 61.97%\n",
            "Epoch [7/10], Training Loss: 0.826, Validation Accuracy: 61.96%\n",
            "Epoch [8/10], Training Loss: 0.816, Validation Accuracy: 61.21%\n",
            "Epoch [9/10], Training Loss: 0.807, Validation Accuracy: 61.77%\n",
            "Epoch [10/10], Training Loss: 0.783, Validation Accuracy: 61.41%\n",
            "Epoch [1/10], Training Loss: 0.950, Validation Accuracy: 62.09%\n",
            "Epoch [2/10], Training Loss: 0.902, Validation Accuracy: 62.58%\n",
            "Epoch [3/10], Training Loss: 0.860, Validation Accuracy: 61.97%\n",
            "Epoch [4/10], Training Loss: 0.845, Validation Accuracy: 62.37%\n",
            "Epoch [5/10], Training Loss: 0.819, Validation Accuracy: 61.64%\n",
            "Epoch [6/10], Training Loss: 0.814, Validation Accuracy: 62.27%\n",
            "Epoch [7/10], Training Loss: 0.792, Validation Accuracy: 62.36%\n",
            "Epoch [8/10], Training Loss: 0.768, Validation Accuracy: 62.02%\n",
            "Epoch [9/10], Training Loss: 0.763, Validation Accuracy: 62.07%\n",
            "Epoch [10/10], Training Loss: 0.755, Validation Accuracy: 61.77%\n",
            "Epoch [1/10], Training Loss: 0.962, Validation Accuracy: 62.14%\n",
            "Epoch [2/10], Training Loss: 0.905, Validation Accuracy: 61.98%\n",
            "Epoch [3/10], Training Loss: 0.874, Validation Accuracy: 62.13%\n",
            "Epoch [4/10], Training Loss: 0.855, Validation Accuracy: 61.53%\n",
            "Epoch [5/10], Training Loss: 0.834, Validation Accuracy: 61.92%\n",
            "Epoch [6/10], Training Loss: 0.811, Validation Accuracy: 62.29%\n",
            "Epoch [7/10], Training Loss: 0.795, Validation Accuracy: 62.16%\n",
            "Epoch [8/10], Training Loss: 0.789, Validation Accuracy: 61.53%\n",
            "Epoch [9/10], Training Loss: 0.770, Validation Accuracy: 61.77%\n",
            "Epoch [10/10], Training Loss: 0.758, Validation Accuracy: 61.24%\n",
            "Epoch [1/10], Training Loss: 0.976, Validation Accuracy: 62.02%\n",
            "Epoch [2/10], Training Loss: 0.911, Validation Accuracy: 61.97%\n",
            "Epoch [3/10], Training Loss: 0.884, Validation Accuracy: 61.75%\n",
            "Epoch [4/10], Training Loss: 0.850, Validation Accuracy: 62.02%\n",
            "Epoch [5/10], Training Loss: 0.843, Validation Accuracy: 61.45%\n",
            "Epoch [6/10], Training Loss: 0.815, Validation Accuracy: 61.68%\n",
            "Epoch [7/10], Training Loss: 0.795, Validation Accuracy: 61.16%\n",
            "Epoch [8/10], Training Loss: 0.784, Validation Accuracy: 61.92%\n",
            "Epoch [9/10], Training Loss: 0.775, Validation Accuracy: 61.94%\n",
            "Epoch [10/10], Training Loss: 0.750, Validation Accuracy: 61.43%\n",
            "Epoch [1/10], Training Loss: 0.937, Validation Accuracy: 61.35%\n",
            "Epoch [2/10], Training Loss: 0.876, Validation Accuracy: 61.08%\n",
            "Epoch [3/10], Training Loss: 0.842, Validation Accuracy: 61.74%\n",
            "Epoch [4/10], Training Loss: 0.817, Validation Accuracy: 62.12%\n",
            "Epoch [5/10], Training Loss: 0.801, Validation Accuracy: 61.84%\n",
            "Epoch [6/10], Training Loss: 0.788, Validation Accuracy: 60.92%\n",
            "Epoch [7/10], Training Loss: 0.762, Validation Accuracy: 61.76%\n",
            "Epoch [8/10], Training Loss: 0.738, Validation Accuracy: 62.39%\n",
            "Epoch [9/10], Training Loss: 0.730, Validation Accuracy: 62.28%\n",
            "Epoch [10/10], Training Loss: 0.723, Validation Accuracy: 62.17%\n",
            "Epoch [1/10], Training Loss: 0.921, Validation Accuracy: 62.56%\n",
            "Epoch [2/10], Training Loss: 0.856, Validation Accuracy: 62.39%\n",
            "Epoch [3/10], Training Loss: 0.831, Validation Accuracy: 62.25%\n",
            "Epoch [4/10], Training Loss: 0.803, Validation Accuracy: 62.10%\n",
            "Epoch [5/10], Training Loss: 0.783, Validation Accuracy: 61.78%\n",
            "Epoch [6/10], Training Loss: 0.769, Validation Accuracy: 62.25%\n",
            "Epoch [7/10], Training Loss: 0.747, Validation Accuracy: 62.45%\n",
            "Epoch [8/10], Training Loss: 0.732, Validation Accuracy: 62.52%\n",
            "Epoch [9/10], Training Loss: 0.713, Validation Accuracy: 61.96%\n",
            "Epoch [10/10], Training Loss: 0.700, Validation Accuracy: 61.62%\n",
            "Epoch [1/10], Training Loss: 0.892, Validation Accuracy: 62.92%\n",
            "Epoch [2/10], Training Loss: 0.826, Validation Accuracy: 63.07%\n",
            "Epoch [3/10], Training Loss: 0.793, Validation Accuracy: 62.84%\n",
            "Epoch [4/10], Training Loss: 0.767, Validation Accuracy: 62.84%\n",
            "Epoch [5/10], Training Loss: 0.742, Validation Accuracy: 62.83%\n",
            "Epoch [6/10], Training Loss: 0.721, Validation Accuracy: 63.33%\n",
            "Epoch [7/10], Training Loss: 0.701, Validation Accuracy: 62.74%\n",
            "Epoch [8/10], Training Loss: 0.683, Validation Accuracy: 63.00%\n",
            "Epoch [9/10], Training Loss: 0.678, Validation Accuracy: 62.42%\n",
            "Epoch [10/10], Training Loss: 0.656, Validation Accuracy: 62.67%\n",
            "Epoch [1/10], Training Loss: 0.916, Validation Accuracy: 62.46%\n",
            "Epoch [2/10], Training Loss: 0.842, Validation Accuracy: 62.67%\n",
            "Epoch [3/10], Training Loss: 0.807, Validation Accuracy: 62.15%\n",
            "Epoch [4/10], Training Loss: 0.778, Validation Accuracy: 62.46%\n",
            "Epoch [5/10], Training Loss: 0.748, Validation Accuracy: 62.60%\n",
            "Epoch [6/10], Training Loss: 0.733, Validation Accuracy: 62.35%\n",
            "Epoch [7/10], Training Loss: 0.711, Validation Accuracy: 62.63%\n",
            "Epoch [8/10], Training Loss: 0.700, Validation Accuracy: 62.53%\n",
            "Epoch [9/10], Training Loss: 0.687, Validation Accuracy: 62.84%\n",
            "Epoch [10/10], Training Loss: 0.666, Validation Accuracy: 61.82%\n",
            "Epoch [1/10], Training Loss: 0.927, Validation Accuracy: 61.82%\n",
            "Epoch [2/10], Training Loss: 0.850, Validation Accuracy: 62.45%\n",
            "Epoch [3/10], Training Loss: 0.802, Validation Accuracy: 62.67%\n",
            "Epoch [4/10], Training Loss: 0.769, Validation Accuracy: 62.21%\n",
            "Epoch [5/10], Training Loss: 0.748, Validation Accuracy: 62.53%\n",
            "Epoch [6/10], Training Loss: 0.729, Validation Accuracy: 62.22%\n",
            "Epoch [7/10], Training Loss: 0.717, Validation Accuracy: 62.47%\n",
            "Epoch [8/10], Training Loss: 0.687, Validation Accuracy: 62.28%\n",
            "Epoch [9/10], Training Loss: 0.672, Validation Accuracy: 62.39%\n",
            "Epoch [10/10], Training Loss: 0.650, Validation Accuracy: 62.35%\n",
            "Epoch [1/10], Training Loss: 0.889, Validation Accuracy: 61.77%\n",
            "Epoch [2/10], Training Loss: 0.815, Validation Accuracy: 62.43%\n",
            "Epoch [3/10], Training Loss: 0.774, Validation Accuracy: 62.60%\n",
            "Epoch [4/10], Training Loss: 0.750, Validation Accuracy: 62.29%\n",
            "Epoch [5/10], Training Loss: 0.721, Validation Accuracy: 62.45%\n",
            "Epoch [6/10], Training Loss: 0.695, Validation Accuracy: 61.83%\n",
            "Epoch [7/10], Training Loss: 0.674, Validation Accuracy: 62.58%\n",
            "Epoch [8/10], Training Loss: 0.657, Validation Accuracy: 62.39%\n",
            "Epoch [9/10], Training Loss: 0.639, Validation Accuracy: 62.30%\n",
            "Epoch [10/10], Training Loss: 0.630, Validation Accuracy: 61.99%\n",
            "Epoch [1/10], Training Loss: 0.873, Validation Accuracy: 62.89%\n",
            "Epoch [2/10], Training Loss: 0.794, Validation Accuracy: 62.58%\n",
            "Epoch [3/10], Training Loss: 0.760, Validation Accuracy: 62.90%\n",
            "Epoch [4/10], Training Loss: 0.734, Validation Accuracy: 62.15%\n",
            "Epoch [5/10], Training Loss: 0.702, Validation Accuracy: 62.98%\n",
            "Epoch [6/10], Training Loss: 0.677, Validation Accuracy: 61.96%\n",
            "Epoch [7/10], Training Loss: 0.659, Validation Accuracy: 62.41%\n",
            "Epoch [8/10], Training Loss: 0.646, Validation Accuracy: 61.88%\n",
            "Epoch [9/10], Training Loss: 0.634, Validation Accuracy: 62.26%\n",
            "Epoch [10/10], Training Loss: 0.609, Validation Accuracy: 62.52%\n",
            "Epoch [1/10], Training Loss: 0.838, Validation Accuracy: 63.22%\n",
            "Epoch [2/10], Training Loss: 0.755, Validation Accuracy: 63.34%\n",
            "Epoch [3/10], Training Loss: 0.711, Validation Accuracy: 63.15%\n",
            "Epoch [4/10], Training Loss: 0.688, Validation Accuracy: 62.99%\n",
            "Epoch [5/10], Training Loss: 0.665, Validation Accuracy: 62.85%\n",
            "Epoch [6/10], Training Loss: 0.648, Validation Accuracy: 62.90%\n",
            "Epoch [7/10], Training Loss: 0.623, Validation Accuracy: 63.53%\n",
            "Epoch [8/10], Training Loss: 0.595, Validation Accuracy: 62.56%\n",
            "Epoch [9/10], Training Loss: 0.586, Validation Accuracy: 62.68%\n",
            "Epoch [10/10], Training Loss: 0.566, Validation Accuracy: 63.39%\n",
            "Epoch [1/10], Training Loss: 0.865, Validation Accuracy: 62.42%\n",
            "Epoch [2/10], Training Loss: 0.788, Validation Accuracy: 62.68%\n",
            "Epoch [3/10], Training Loss: 0.739, Validation Accuracy: 63.38%\n",
            "Epoch [4/10], Training Loss: 0.700, Validation Accuracy: 63.10%\n",
            "Epoch [5/10], Training Loss: 0.683, Validation Accuracy: 62.65%\n",
            "Epoch [6/10], Training Loss: 0.647, Validation Accuracy: 62.42%\n",
            "Epoch [7/10], Training Loss: 0.634, Validation Accuracy: 62.34%\n",
            "Epoch [8/10], Training Loss: 0.606, Validation Accuracy: 62.79%\n",
            "Epoch [9/10], Training Loss: 0.615, Validation Accuracy: 62.37%\n",
            "Epoch [10/10], Training Loss: 0.582, Validation Accuracy: 62.04%\n",
            "Epoch [1/10], Training Loss: 0.896, Validation Accuracy: 61.92%\n",
            "Epoch [2/10], Training Loss: 0.796, Validation Accuracy: 62.83%\n",
            "Epoch [3/10], Training Loss: 0.743, Validation Accuracy: 62.89%\n",
            "Epoch [4/10], Training Loss: 0.706, Validation Accuracy: 62.76%\n",
            "Epoch [5/10], Training Loss: 0.672, Validation Accuracy: 63.08%\n",
            "Epoch [6/10], Training Loss: 0.645, Validation Accuracy: 62.88%\n",
            "Epoch [7/10], Training Loss: 0.623, Validation Accuracy: 62.16%\n",
            "Epoch [8/10], Training Loss: 0.599, Validation Accuracy: 62.03%\n",
            "Epoch [9/10], Training Loss: 0.584, Validation Accuracy: 62.07%\n",
            "Epoch [10/10], Training Loss: 0.565, Validation Accuracy: 62.40%\n",
            "Epoch [1/10], Training Loss: 0.851, Validation Accuracy: 62.52%\n",
            "Epoch [2/10], Training Loss: 0.755, Validation Accuracy: 61.61%\n",
            "Epoch [3/10], Training Loss: 0.705, Validation Accuracy: 62.43%\n",
            "Epoch [4/10], Training Loss: 0.675, Validation Accuracy: 62.82%\n",
            "Epoch [5/10], Training Loss: 0.643, Validation Accuracy: 62.21%\n",
            "Epoch [6/10], Training Loss: 0.620, Validation Accuracy: 62.04%\n",
            "Epoch [7/10], Training Loss: 0.606, Validation Accuracy: 62.52%\n",
            "Epoch [8/10], Training Loss: 0.575, Validation Accuracy: 61.90%\n",
            "Epoch [9/10], Training Loss: 0.563, Validation Accuracy: 61.91%\n",
            "Epoch [10/10], Training Loss: 0.536, Validation Accuracy: 62.32%\n",
            "Epoch [1/10], Training Loss: 0.836, Validation Accuracy: 62.62%\n",
            "Epoch [2/10], Training Loss: 0.747, Validation Accuracy: 63.35%\n",
            "Epoch [3/10], Training Loss: 0.692, Validation Accuracy: 62.60%\n",
            "Epoch [4/10], Training Loss: 0.658, Validation Accuracy: 62.95%\n",
            "Epoch [5/10], Training Loss: 0.634, Validation Accuracy: 62.71%\n",
            "Epoch [6/10], Training Loss: 0.609, Validation Accuracy: 62.83%\n",
            "Epoch [7/10], Training Loss: 0.582, Validation Accuracy: 63.02%\n",
            "Epoch [8/10], Training Loss: 0.561, Validation Accuracy: 62.37%\n",
            "Epoch [9/10], Training Loss: 0.549, Validation Accuracy: 62.50%\n",
            "Epoch [10/10], Training Loss: 0.543, Validation Accuracy: 62.40%\n",
            "Epoch [1/10], Training Loss: 0.811, Validation Accuracy: 62.34%\n",
            "Epoch [2/10], Training Loss: 0.726, Validation Accuracy: 63.01%\n",
            "Epoch [3/10], Training Loss: 0.653, Validation Accuracy: 63.08%\n",
            "Epoch [4/10], Training Loss: 0.614, Validation Accuracy: 63.23%\n",
            "Epoch [5/10], Training Loss: 0.595, Validation Accuracy: 63.06%\n",
            "Epoch [6/10], Training Loss: 0.562, Validation Accuracy: 63.15%\n",
            "Epoch [7/10], Training Loss: 0.539, Validation Accuracy: 62.78%\n",
            "Epoch [8/10], Training Loss: 0.517, Validation Accuracy: 63.15%\n",
            "Epoch [9/10], Training Loss: 0.497, Validation Accuracy: 63.29%\n",
            "Epoch [10/10], Training Loss: 0.491, Validation Accuracy: 63.07%\n",
            "Epoch [1/10], Training Loss: 0.829, Validation Accuracy: 62.22%\n",
            "Epoch [2/10], Training Loss: 0.721, Validation Accuracy: 62.13%\n",
            "Epoch [3/10], Training Loss: 0.677, Validation Accuracy: 63.70%\n",
            "Epoch [4/10], Training Loss: 0.624, Validation Accuracy: 62.36%\n",
            "Epoch [5/10], Training Loss: 0.600, Validation Accuracy: 62.87%\n",
            "Epoch [6/10], Training Loss: 0.570, Validation Accuracy: 63.07%\n",
            "Epoch [7/10], Training Loss: 0.546, Validation Accuracy: 63.13%\n",
            "Epoch [8/10], Training Loss: 0.524, Validation Accuracy: 62.50%\n",
            "Epoch [9/10], Training Loss: 0.504, Validation Accuracy: 62.76%\n",
            "Epoch [10/10], Training Loss: 0.486, Validation Accuracy: 61.85%\n",
            "Epoch [1/10], Training Loss: 0.845, Validation Accuracy: 61.67%\n",
            "Epoch [2/10], Training Loss: 0.726, Validation Accuracy: 63.03%\n",
            "Epoch [3/10], Training Loss: 0.668, Validation Accuracy: 62.75%\n",
            "Epoch [4/10], Training Loss: 0.634, Validation Accuracy: 62.83%\n",
            "Epoch [5/10], Training Loss: 0.590, Validation Accuracy: 61.76%\n",
            "Epoch [6/10], Training Loss: 0.566, Validation Accuracy: 63.23%\n",
            "Epoch [7/10], Training Loss: 0.534, Validation Accuracy: 62.73%\n",
            "Epoch [8/10], Training Loss: 0.514, Validation Accuracy: 62.86%\n",
            "Epoch [9/10], Training Loss: 0.490, Validation Accuracy: 61.28%\n",
            "Epoch [10/10], Training Loss: 0.473, Validation Accuracy: 62.14%\n",
            "Epoch [1/10], Training Loss: 0.803, Validation Accuracy: 61.81%\n",
            "Epoch [2/10], Training Loss: 0.710, Validation Accuracy: 62.30%\n",
            "Epoch [3/10], Training Loss: 0.645, Validation Accuracy: 62.89%\n",
            "Epoch [4/10], Training Loss: 0.603, Validation Accuracy: 62.38%\n",
            "Epoch [5/10], Training Loss: 0.574, Validation Accuracy: 61.98%\n",
            "Epoch [6/10], Training Loss: 0.545, Validation Accuracy: 62.27%\n",
            "Epoch [7/10], Training Loss: 0.514, Validation Accuracy: 62.58%\n",
            "Epoch [8/10], Training Loss: 0.497, Validation Accuracy: 62.60%\n",
            "Epoch [9/10], Training Loss: 0.473, Validation Accuracy: 62.27%\n",
            "Epoch [10/10], Training Loss: 0.455, Validation Accuracy: 62.09%\n",
            "Confusion Matrix:\n",
            "[[715  34  59  22  17   4  14  11  74  50]\n",
            " [ 52 683   9  12   3   9  13  10  49 160]\n",
            " [ 83   9 518  79  88  85  61  45  18  14]\n",
            " [ 31  19 100 406  52 207  87  59  20  19]\n",
            " [ 43   6 109  71 480  55 100 107  20   9]\n",
            " [ 15   2  83 178  50 524  45  86   8   9]\n",
            " [ 15   4  72  57  48  42 734  11  10   7]\n",
            " [ 21   5  55  58  43  95  11 681   7  24]\n",
            " [112  48  21  25   3   7   8   7 727  42]\n",
            " [ 58 103  12  25   5  14  22  27  41 693]]\n",
            "Test Accuracy: 61.61%\n",
            "True Positives (TP): [715 683 518 406 480 524 734 681 727 693]\n",
            "False Positives (FP): [430 230 520 527 309 518 361 363 247 334]\n",
            "True Negatives (TN): [8570 8770 8480 8473 8691 8482 8639 8637 8753 8666]\n",
            "False Negatives (FN): [285 317 482 594 520 476 266 319 273 307]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.62445415 0.74808324 0.49903661 0.43515541 0.60836502 0.50287908\n",
            " 0.67031963 0.65229885 0.74640657 0.67478092]\n",
            "Recall: [0.715 0.683 0.518 0.406 0.48  0.524 0.734 0.681 0.727 0.693]\n",
            "F1 Score: [0.66666667 0.71406168 0.50834151 0.42007243 0.53661263 0.51322233\n",
            " 0.70071599 0.66634051 0.73657548 0.68376912]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 0.1):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=0.1):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE,  distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average =  augmented_data_truncated\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=0.1)\n",
        "\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beta=0.5"
      ],
      "metadata": {
        "id": "NdhxMOUrEVmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n"
      ],
      "metadata": {
        "id": "4akQz3HzEU4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2953d631-e0ae-4ade-ef00-94570f453bcb",
        "id": "rnRpLL8NEZ6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Images per Class: [5896 6042 6055 6050 5884 6148 6029 6005 5997 5894]\n",
            "Epoch [1/10], Training Loss: 2.302, Validation Accuracy: 10.03%\n",
            "Epoch [2/10], Training Loss: 2.302, Validation Accuracy: 10.03%\n",
            "Epoch [3/10], Training Loss: 2.302, Validation Accuracy: 10.23%\n",
            "Epoch [4/10], Training Loss: 2.301, Validation Accuracy: 11.93%\n",
            "Epoch [5/10], Training Loss: 2.301, Validation Accuracy: 12.87%\n",
            "Epoch [6/10], Training Loss: 2.300, Validation Accuracy: 12.61%\n",
            "Epoch [7/10], Training Loss: 2.300, Validation Accuracy: 11.63%\n",
            "Epoch [8/10], Training Loss: 2.299, Validation Accuracy: 11.58%\n",
            "Epoch [9/10], Training Loss: 2.298, Validation Accuracy: 11.27%\n",
            "Epoch [10/10], Training Loss: 2.297, Validation Accuracy: 11.36%\n",
            "Epoch [1/10], Training Loss: 2.299, Validation Accuracy: 13.66%\n",
            "Epoch [2/10], Training Loss: 2.297, Validation Accuracy: 16.11%\n",
            "Epoch [3/10], Training Loss: 2.295, Validation Accuracy: 16.49%\n",
            "Epoch [4/10], Training Loss: 2.293, Validation Accuracy: 15.75%\n",
            "Epoch [5/10], Training Loss: 2.290, Validation Accuracy: 15.17%\n",
            "Epoch [6/10], Training Loss: 2.287, Validation Accuracy: 15.65%\n",
            "Epoch [7/10], Training Loss: 2.282, Validation Accuracy: 15.55%\n",
            "Epoch [8/10], Training Loss: 2.276, Validation Accuracy: 16.27%\n",
            "Epoch [9/10], Training Loss: 2.267, Validation Accuracy: 17.05%\n",
            "Epoch [10/10], Training Loss: 2.253, Validation Accuracy: 18.13%\n",
            "Epoch [1/10], Training Loss: 2.228, Validation Accuracy: 19.77%\n",
            "Epoch [2/10], Training Loss: 2.190, Validation Accuracy: 23.64%\n",
            "Epoch [3/10], Training Loss: 2.135, Validation Accuracy: 26.55%\n",
            "Epoch [4/10], Training Loss: 2.082, Validation Accuracy: 27.63%\n",
            "Epoch [5/10], Training Loss: 2.035, Validation Accuracy: 29.18%\n",
            "Epoch [6/10], Training Loss: 1.988, Validation Accuracy: 30.20%\n",
            "Epoch [7/10], Training Loss: 1.940, Validation Accuracy: 30.70%\n",
            "Epoch [8/10], Training Loss: 1.902, Validation Accuracy: 32.24%\n",
            "Epoch [9/10], Training Loss: 1.868, Validation Accuracy: 32.75%\n",
            "Epoch [10/10], Training Loss: 1.838, Validation Accuracy: 33.85%\n",
            "Epoch [1/10], Training Loss: 1.821, Validation Accuracy: 34.38%\n",
            "Epoch [2/10], Training Loss: 1.796, Validation Accuracy: 34.98%\n",
            "Epoch [3/10], Training Loss: 1.771, Validation Accuracy: 35.90%\n",
            "Epoch [4/10], Training Loss: 1.751, Validation Accuracy: 36.43%\n",
            "Epoch [5/10], Training Loss: 1.730, Validation Accuracy: 36.74%\n",
            "Epoch [6/10], Training Loss: 1.711, Validation Accuracy: 37.22%\n",
            "Epoch [7/10], Training Loss: 1.691, Validation Accuracy: 37.64%\n",
            "Epoch [8/10], Training Loss: 1.672, Validation Accuracy: 38.85%\n",
            "Epoch [9/10], Training Loss: 1.658, Validation Accuracy: 38.85%\n",
            "Epoch [10/10], Training Loss: 1.644, Validation Accuracy: 39.24%\n",
            "Epoch [1/10], Training Loss: 1.672, Validation Accuracy: 39.52%\n",
            "Epoch [2/10], Training Loss: 1.659, Validation Accuracy: 40.68%\n",
            "Epoch [3/10], Training Loss: 1.643, Validation Accuracy: 40.67%\n",
            "Epoch [4/10], Training Loss: 1.636, Validation Accuracy: 40.49%\n",
            "Epoch [5/10], Training Loss: 1.624, Validation Accuracy: 40.41%\n",
            "Epoch [6/10], Training Loss: 1.614, Validation Accuracy: 41.15%\n",
            "Epoch [7/10], Training Loss: 1.604, Validation Accuracy: 41.84%\n",
            "Epoch [8/10], Training Loss: 1.596, Validation Accuracy: 41.98%\n",
            "Epoch [9/10], Training Loss: 1.589, Validation Accuracy: 42.20%\n",
            "Epoch [10/10], Training Loss: 1.582, Validation Accuracy: 41.97%\n",
            "Epoch [1/10], Training Loss: 1.590, Validation Accuracy: 42.64%\n",
            "Epoch [2/10], Training Loss: 1.576, Validation Accuracy: 42.57%\n",
            "Epoch [3/10], Training Loss: 1.573, Validation Accuracy: 43.14%\n",
            "Epoch [4/10], Training Loss: 1.558, Validation Accuracy: 43.01%\n",
            "Epoch [5/10], Training Loss: 1.555, Validation Accuracy: 42.23%\n",
            "Epoch [6/10], Training Loss: 1.550, Validation Accuracy: 44.08%\n",
            "Epoch [7/10], Training Loss: 1.543, Validation Accuracy: 43.36%\n",
            "Epoch [8/10], Training Loss: 1.528, Validation Accuracy: 43.58%\n",
            "Epoch [9/10], Training Loss: 1.520, Validation Accuracy: 44.09%\n",
            "Epoch [10/10], Training Loss: 1.516, Validation Accuracy: 44.19%\n",
            "Epoch [1/10], Training Loss: 1.570, Validation Accuracy: 44.67%\n",
            "Epoch [2/10], Training Loss: 1.557, Validation Accuracy: 44.93%\n",
            "Epoch [3/10], Training Loss: 1.544, Validation Accuracy: 44.97%\n",
            "Epoch [4/10], Training Loss: 1.537, Validation Accuracy: 45.26%\n",
            "Epoch [5/10], Training Loss: 1.529, Validation Accuracy: 45.10%\n",
            "Epoch [6/10], Training Loss: 1.522, Validation Accuracy: 45.25%\n",
            "Epoch [7/10], Training Loss: 1.513, Validation Accuracy: 46.16%\n",
            "Epoch [8/10], Training Loss: 1.503, Validation Accuracy: 45.78%\n",
            "Epoch [9/10], Training Loss: 1.498, Validation Accuracy: 45.87%\n",
            "Epoch [10/10], Training Loss: 1.485, Validation Accuracy: 46.11%\n",
            "Epoch [1/10], Training Loss: 1.475, Validation Accuracy: 46.33%\n",
            "Epoch [2/10], Training Loss: 1.463, Validation Accuracy: 46.40%\n",
            "Epoch [3/10], Training Loss: 1.451, Validation Accuracy: 46.48%\n",
            "Epoch [4/10], Training Loss: 1.445, Validation Accuracy: 46.74%\n",
            "Epoch [5/10], Training Loss: 1.437, Validation Accuracy: 46.86%\n",
            "Epoch [6/10], Training Loss: 1.425, Validation Accuracy: 46.99%\n",
            "Epoch [7/10], Training Loss: 1.427, Validation Accuracy: 47.50%\n",
            "Epoch [8/10], Training Loss: 1.410, Validation Accuracy: 47.04%\n",
            "Epoch [9/10], Training Loss: 1.407, Validation Accuracy: 47.65%\n",
            "Epoch [10/10], Training Loss: 1.399, Validation Accuracy: 46.87%\n",
            "Epoch [1/10], Training Loss: 1.446, Validation Accuracy: 47.74%\n",
            "Epoch [2/10], Training Loss: 1.423, Validation Accuracy: 47.46%\n",
            "Epoch [3/10], Training Loss: 1.413, Validation Accuracy: 47.54%\n",
            "Epoch [4/10], Training Loss: 1.405, Validation Accuracy: 48.34%\n",
            "Epoch [5/10], Training Loss: 1.397, Validation Accuracy: 47.91%\n",
            "Epoch [6/10], Training Loss: 1.386, Validation Accuracy: 48.49%\n",
            "Epoch [7/10], Training Loss: 1.374, Validation Accuracy: 48.43%\n",
            "Epoch [8/10], Training Loss: 1.375, Validation Accuracy: 48.52%\n",
            "Epoch [9/10], Training Loss: 1.358, Validation Accuracy: 48.88%\n",
            "Epoch [10/10], Training Loss: 1.355, Validation Accuracy: 49.26%\n",
            "Epoch [1/10], Training Loss: 1.419, Validation Accuracy: 49.01%\n",
            "Epoch [2/10], Training Loss: 1.410, Validation Accuracy: 48.59%\n",
            "Epoch [3/10], Training Loss: 1.392, Validation Accuracy: 49.33%\n",
            "Epoch [4/10], Training Loss: 1.382, Validation Accuracy: 49.80%\n",
            "Epoch [5/10], Training Loss: 1.369, Validation Accuracy: 49.35%\n",
            "Epoch [6/10], Training Loss: 1.362, Validation Accuracy: 49.78%\n",
            "Epoch [7/10], Training Loss: 1.351, Validation Accuracy: 49.53%\n",
            "Epoch [8/10], Training Loss: 1.350, Validation Accuracy: 49.28%\n",
            "Epoch [9/10], Training Loss: 1.340, Validation Accuracy: 49.84%\n",
            "Epoch [10/10], Training Loss: 1.335, Validation Accuracy: 49.01%\n",
            "Epoch [1/10], Training Loss: 1.395, Validation Accuracy: 50.70%\n",
            "Epoch [2/10], Training Loss: 1.371, Validation Accuracy: 51.18%\n",
            "Epoch [3/10], Training Loss: 1.356, Validation Accuracy: 50.90%\n",
            "Epoch [4/10], Training Loss: 1.341, Validation Accuracy: 51.06%\n",
            "Epoch [5/10], Training Loss: 1.330, Validation Accuracy: 51.14%\n",
            "Epoch [6/10], Training Loss: 1.324, Validation Accuracy: 50.86%\n",
            "Epoch [7/10], Training Loss: 1.315, Validation Accuracy: 51.69%\n",
            "Epoch [8/10], Training Loss: 1.306, Validation Accuracy: 51.64%\n",
            "Epoch [9/10], Training Loss: 1.300, Validation Accuracy: 51.05%\n",
            "Epoch [10/10], Training Loss: 1.290, Validation Accuracy: 51.62%\n",
            "Epoch [1/10], Training Loss: 1.366, Validation Accuracy: 51.96%\n",
            "Epoch [2/10], Training Loss: 1.346, Validation Accuracy: 51.47%\n",
            "Epoch [3/10], Training Loss: 1.336, Validation Accuracy: 51.65%\n",
            "Epoch [4/10], Training Loss: 1.330, Validation Accuracy: 52.40%\n",
            "Epoch [5/10], Training Loss: 1.318, Validation Accuracy: 51.37%\n",
            "Epoch [6/10], Training Loss: 1.306, Validation Accuracy: 52.16%\n",
            "Epoch [7/10], Training Loss: 1.299, Validation Accuracy: 51.52%\n",
            "Epoch [8/10], Training Loss: 1.299, Validation Accuracy: 51.82%\n",
            "Epoch [9/10], Training Loss: 1.286, Validation Accuracy: 52.80%\n",
            "Epoch [10/10], Training Loss: 1.270, Validation Accuracy: 52.72%\n",
            "Epoch [1/10], Training Loss: 1.301, Validation Accuracy: 52.87%\n",
            "Epoch [2/10], Training Loss: 1.281, Validation Accuracy: 52.21%\n",
            "Epoch [3/10], Training Loss: 1.275, Validation Accuracy: 51.98%\n",
            "Epoch [4/10], Training Loss: 1.255, Validation Accuracy: 52.44%\n",
            "Epoch [5/10], Training Loss: 1.257, Validation Accuracy: 52.52%\n",
            "Epoch [6/10], Training Loss: 1.236, Validation Accuracy: 52.58%\n",
            "Epoch [7/10], Training Loss: 1.228, Validation Accuracy: 52.58%\n",
            "Epoch [8/10], Training Loss: 1.220, Validation Accuracy: 52.84%\n",
            "Epoch [9/10], Training Loss: 1.210, Validation Accuracy: 53.11%\n",
            "Epoch [10/10], Training Loss: 1.200, Validation Accuracy: 52.74%\n",
            "Epoch [1/10], Training Loss: 1.276, Validation Accuracy: 53.12%\n",
            "Epoch [2/10], Training Loss: 1.260, Validation Accuracy: 53.54%\n",
            "Epoch [3/10], Training Loss: 1.242, Validation Accuracy: 54.09%\n",
            "Epoch [4/10], Training Loss: 1.224, Validation Accuracy: 53.36%\n",
            "Epoch [5/10], Training Loss: 1.211, Validation Accuracy: 53.69%\n",
            "Epoch [6/10], Training Loss: 1.215, Validation Accuracy: 53.88%\n",
            "Epoch [7/10], Training Loss: 1.198, Validation Accuracy: 53.15%\n",
            "Epoch [8/10], Training Loss: 1.195, Validation Accuracy: 53.77%\n",
            "Epoch [9/10], Training Loss: 1.182, Validation Accuracy: 53.79%\n",
            "Epoch [10/10], Training Loss: 1.180, Validation Accuracy: 53.92%\n",
            "Epoch [1/10], Training Loss: 1.267, Validation Accuracy: 54.15%\n",
            "Epoch [2/10], Training Loss: 1.247, Validation Accuracy: 54.56%\n",
            "Epoch [3/10], Training Loss: 1.228, Validation Accuracy: 54.94%\n",
            "Epoch [4/10], Training Loss: 1.216, Validation Accuracy: 55.04%\n",
            "Epoch [5/10], Training Loss: 1.209, Validation Accuracy: 53.84%\n",
            "Epoch [6/10], Training Loss: 1.202, Validation Accuracy: 55.35%\n",
            "Epoch [7/10], Training Loss: 1.182, Validation Accuracy: 55.64%\n",
            "Epoch [8/10], Training Loss: 1.175, Validation Accuracy: 55.26%\n",
            "Epoch [9/10], Training Loss: 1.166, Validation Accuracy: 54.51%\n",
            "Epoch [10/10], Training Loss: 1.159, Validation Accuracy: 55.30%\n",
            "Epoch [1/10], Training Loss: 1.243, Validation Accuracy: 55.23%\n",
            "Epoch [2/10], Training Loss: 1.220, Validation Accuracy: 56.35%\n",
            "Epoch [3/10], Training Loss: 1.204, Validation Accuracy: 54.19%\n",
            "Epoch [4/10], Training Loss: 1.191, Validation Accuracy: 55.42%\n",
            "Epoch [5/10], Training Loss: 1.172, Validation Accuracy: 56.06%\n",
            "Epoch [6/10], Training Loss: 1.161, Validation Accuracy: 55.76%\n",
            "Epoch [7/10], Training Loss: 1.157, Validation Accuracy: 55.66%\n",
            "Epoch [8/10], Training Loss: 1.148, Validation Accuracy: 54.46%\n",
            "Epoch [9/10], Training Loss: 1.139, Validation Accuracy: 55.93%\n",
            "Epoch [10/10], Training Loss: 1.119, Validation Accuracy: 56.26%\n",
            "Epoch [1/10], Training Loss: 1.248, Validation Accuracy: 55.68%\n",
            "Epoch [2/10], Training Loss: 1.228, Validation Accuracy: 56.72%\n",
            "Epoch [3/10], Training Loss: 1.201, Validation Accuracy: 56.85%\n",
            "Epoch [4/10], Training Loss: 1.196, Validation Accuracy: 56.46%\n",
            "Epoch [5/10], Training Loss: 1.172, Validation Accuracy: 56.62%\n",
            "Epoch [6/10], Training Loss: 1.165, Validation Accuracy: 56.32%\n",
            "Epoch [7/10], Training Loss: 1.152, Validation Accuracy: 55.61%\n",
            "Epoch [8/10], Training Loss: 1.141, Validation Accuracy: 56.75%\n",
            "Epoch [9/10], Training Loss: 1.130, Validation Accuracy: 56.35%\n",
            "Epoch [10/10], Training Loss: 1.140, Validation Accuracy: 54.45%\n",
            "Epoch [1/10], Training Loss: 1.196, Validation Accuracy: 55.65%\n",
            "Epoch [2/10], Training Loss: 1.160, Validation Accuracy: 55.90%\n",
            "Epoch [3/10], Training Loss: 1.133, Validation Accuracy: 56.78%\n",
            "Epoch [4/10], Training Loss: 1.114, Validation Accuracy: 57.06%\n",
            "Epoch [5/10], Training Loss: 1.100, Validation Accuracy: 56.96%\n",
            "Epoch [6/10], Training Loss: 1.088, Validation Accuracy: 56.93%\n",
            "Epoch [7/10], Training Loss: 1.079, Validation Accuracy: 56.38%\n",
            "Epoch [8/10], Training Loss: 1.064, Validation Accuracy: 56.37%\n",
            "Epoch [9/10], Training Loss: 1.056, Validation Accuracy: 56.94%\n",
            "Epoch [10/10], Training Loss: 1.048, Validation Accuracy: 57.79%\n",
            "Epoch [1/10], Training Loss: 1.171, Validation Accuracy: 56.38%\n",
            "Epoch [2/10], Training Loss: 1.151, Validation Accuracy: 56.97%\n",
            "Epoch [3/10], Training Loss: 1.119, Validation Accuracy: 57.47%\n",
            "Epoch [4/10], Training Loss: 1.101, Validation Accuracy: 57.49%\n",
            "Epoch [5/10], Training Loss: 1.095, Validation Accuracy: 57.10%\n",
            "Epoch [6/10], Training Loss: 1.077, Validation Accuracy: 57.88%\n",
            "Epoch [7/10], Training Loss: 1.059, Validation Accuracy: 57.20%\n",
            "Epoch [8/10], Training Loss: 1.054, Validation Accuracy: 57.01%\n",
            "Epoch [9/10], Training Loss: 1.041, Validation Accuracy: 57.01%\n",
            "Epoch [10/10], Training Loss: 1.034, Validation Accuracy: 57.30%\n",
            "Epoch [1/10], Training Loss: 1.160, Validation Accuracy: 58.14%\n",
            "Epoch [2/10], Training Loss: 1.129, Validation Accuracy: 57.90%\n",
            "Epoch [3/10], Training Loss: 1.102, Validation Accuracy: 57.64%\n",
            "Epoch [4/10], Training Loss: 1.092, Validation Accuracy: 58.35%\n",
            "Epoch [5/10], Training Loss: 1.080, Validation Accuracy: 58.16%\n",
            "Epoch [6/10], Training Loss: 1.071, Validation Accuracy: 58.18%\n",
            "Epoch [7/10], Training Loss: 1.055, Validation Accuracy: 58.66%\n",
            "Epoch [8/10], Training Loss: 1.036, Validation Accuracy: 58.40%\n",
            "Epoch [9/10], Training Loss: 1.035, Validation Accuracy: 58.06%\n",
            "Epoch [10/10], Training Loss: 1.022, Validation Accuracy: 58.22%\n",
            "Epoch [1/10], Training Loss: 1.149, Validation Accuracy: 59.00%\n",
            "Epoch [2/10], Training Loss: 1.102, Validation Accuracy: 58.48%\n",
            "Epoch [3/10], Training Loss: 1.082, Validation Accuracy: 58.65%\n",
            "Epoch [4/10], Training Loss: 1.070, Validation Accuracy: 58.93%\n",
            "Epoch [5/10], Training Loss: 1.052, Validation Accuracy: 57.92%\n",
            "Epoch [6/10], Training Loss: 1.041, Validation Accuracy: 58.44%\n",
            "Epoch [7/10], Training Loss: 1.021, Validation Accuracy: 58.66%\n",
            "Epoch [8/10], Training Loss: 1.013, Validation Accuracy: 59.02%\n",
            "Epoch [9/10], Training Loss: 1.002, Validation Accuracy: 58.40%\n",
            "Epoch [10/10], Training Loss: 0.982, Validation Accuracy: 58.19%\n",
            "Epoch [1/10], Training Loss: 1.147, Validation Accuracy: 58.39%\n",
            "Epoch [2/10], Training Loss: 1.116, Validation Accuracy: 58.81%\n",
            "Epoch [3/10], Training Loss: 1.093, Validation Accuracy: 58.84%\n",
            "Epoch [4/10], Training Loss: 1.067, Validation Accuracy: 58.53%\n",
            "Epoch [5/10], Training Loss: 1.056, Validation Accuracy: 58.88%\n",
            "Epoch [6/10], Training Loss: 1.044, Validation Accuracy: 59.14%\n",
            "Epoch [7/10], Training Loss: 1.029, Validation Accuracy: 57.28%\n",
            "Epoch [8/10], Training Loss: 1.020, Validation Accuracy: 58.91%\n",
            "Epoch [9/10], Training Loss: 1.004, Validation Accuracy: 58.70%\n",
            "Epoch [10/10], Training Loss: 0.986, Validation Accuracy: 58.98%\n",
            "Epoch [1/10], Training Loss: 1.091, Validation Accuracy: 58.77%\n",
            "Epoch [2/10], Training Loss: 1.053, Validation Accuracy: 57.99%\n",
            "Epoch [3/10], Training Loss: 1.033, Validation Accuracy: 58.86%\n",
            "Epoch [4/10], Training Loss: 1.000, Validation Accuracy: 59.08%\n",
            "Epoch [5/10], Training Loss: 0.992, Validation Accuracy: 57.81%\n",
            "Epoch [6/10], Training Loss: 0.979, Validation Accuracy: 59.27%\n",
            "Epoch [7/10], Training Loss: 0.958, Validation Accuracy: 59.12%\n",
            "Epoch [8/10], Training Loss: 0.945, Validation Accuracy: 58.89%\n",
            "Epoch [9/10], Training Loss: 0.928, Validation Accuracy: 58.55%\n",
            "Epoch [10/10], Training Loss: 0.915, Validation Accuracy: 59.21%\n",
            "Epoch [1/10], Training Loss: 1.092, Validation Accuracy: 59.18%\n",
            "Epoch [2/10], Training Loss: 1.049, Validation Accuracy: 59.10%\n",
            "Epoch [3/10], Training Loss: 1.021, Validation Accuracy: 59.58%\n",
            "Epoch [4/10], Training Loss: 0.997, Validation Accuracy: 58.96%\n",
            "Epoch [5/10], Training Loss: 0.982, Validation Accuracy: 58.67%\n",
            "Epoch [6/10], Training Loss: 0.963, Validation Accuracy: 58.61%\n",
            "Epoch [7/10], Training Loss: 0.958, Validation Accuracy: 59.21%\n",
            "Epoch [8/10], Training Loss: 0.948, Validation Accuracy: 59.55%\n",
            "Epoch [9/10], Training Loss: 0.919, Validation Accuracy: 58.86%\n",
            "Epoch [10/10], Training Loss: 0.910, Validation Accuracy: 59.46%\n",
            "Epoch [1/10], Training Loss: 1.087, Validation Accuracy: 59.63%\n",
            "Epoch [2/10], Training Loss: 1.028, Validation Accuracy: 59.13%\n",
            "Epoch [3/10], Training Loss: 1.001, Validation Accuracy: 59.21%\n",
            "Epoch [4/10], Training Loss: 0.989, Validation Accuracy: 59.49%\n",
            "Epoch [5/10], Training Loss: 0.968, Validation Accuracy: 59.29%\n",
            "Epoch [6/10], Training Loss: 0.954, Validation Accuracy: 59.49%\n",
            "Epoch [7/10], Training Loss: 0.935, Validation Accuracy: 60.30%\n",
            "Epoch [8/10], Training Loss: 0.920, Validation Accuracy: 59.84%\n",
            "Epoch [9/10], Training Loss: 0.913, Validation Accuracy: 60.21%\n",
            "Epoch [10/10], Training Loss: 0.892, Validation Accuracy: 59.51%\n",
            "Epoch [1/10], Training Loss: 1.060, Validation Accuracy: 59.81%\n",
            "Epoch [2/10], Training Loss: 1.015, Validation Accuracy: 60.23%\n",
            "Epoch [3/10], Training Loss: 0.990, Validation Accuracy: 60.34%\n",
            "Epoch [4/10], Training Loss: 0.963, Validation Accuracy: 60.11%\n",
            "Epoch [5/10], Training Loss: 0.947, Validation Accuracy: 60.32%\n",
            "Epoch [6/10], Training Loss: 0.921, Validation Accuracy: 59.26%\n",
            "Epoch [7/10], Training Loss: 0.908, Validation Accuracy: 59.31%\n",
            "Epoch [8/10], Training Loss: 0.899, Validation Accuracy: 59.78%\n",
            "Epoch [9/10], Training Loss: 0.881, Validation Accuracy: 59.88%\n",
            "Epoch [10/10], Training Loss: 0.860, Validation Accuracy: 59.34%\n",
            "Epoch [1/10], Training Loss: 1.081, Validation Accuracy: 59.25%\n",
            "Epoch [2/10], Training Loss: 1.024, Validation Accuracy: 59.83%\n",
            "Epoch [3/10], Training Loss: 0.995, Validation Accuracy: 59.66%\n",
            "Epoch [4/10], Training Loss: 0.978, Validation Accuracy: 60.07%\n",
            "Epoch [5/10], Training Loss: 0.951, Validation Accuracy: 59.40%\n",
            "Epoch [6/10], Training Loss: 0.937, Validation Accuracy: 60.05%\n",
            "Epoch [7/10], Training Loss: 0.922, Validation Accuracy: 59.81%\n",
            "Epoch [8/10], Training Loss: 0.908, Validation Accuracy: 60.07%\n",
            "Epoch [9/10], Training Loss: 0.889, Validation Accuracy: 59.04%\n",
            "Epoch [10/10], Training Loss: 0.883, Validation Accuracy: 60.10%\n",
            "Epoch [1/10], Training Loss: 1.028, Validation Accuracy: 60.22%\n",
            "Epoch [2/10], Training Loss: 0.974, Validation Accuracy: 60.84%\n",
            "Epoch [3/10], Training Loss: 0.935, Validation Accuracy: 60.26%\n",
            "Epoch [4/10], Training Loss: 0.918, Validation Accuracy: 60.54%\n",
            "Epoch [5/10], Training Loss: 0.892, Validation Accuracy: 60.08%\n",
            "Epoch [6/10], Training Loss: 0.870, Validation Accuracy: 60.58%\n",
            "Epoch [7/10], Training Loss: 0.854, Validation Accuracy: 60.70%\n",
            "Epoch [8/10], Training Loss: 0.838, Validation Accuracy: 60.05%\n",
            "Epoch [9/10], Training Loss: 0.829, Validation Accuracy: 60.04%\n",
            "Epoch [10/10], Training Loss: 0.810, Validation Accuracy: 60.53%\n",
            "Epoch [1/10], Training Loss: 1.038, Validation Accuracy: 60.49%\n",
            "Epoch [2/10], Training Loss: 0.969, Validation Accuracy: 60.90%\n",
            "Epoch [3/10], Training Loss: 0.931, Validation Accuracy: 59.09%\n",
            "Epoch [4/10], Training Loss: 0.908, Validation Accuracy: 60.99%\n",
            "Epoch [5/10], Training Loss: 0.886, Validation Accuracy: 60.37%\n",
            "Epoch [6/10], Training Loss: 0.880, Validation Accuracy: 60.16%\n",
            "Epoch [7/10], Training Loss: 0.850, Validation Accuracy: 60.84%\n",
            "Epoch [8/10], Training Loss: 0.833, Validation Accuracy: 60.02%\n",
            "Epoch [9/10], Training Loss: 0.824, Validation Accuracy: 59.59%\n",
            "Epoch [10/10], Training Loss: 0.810, Validation Accuracy: 59.81%\n",
            "Epoch [1/10], Training Loss: 1.003, Validation Accuracy: 59.91%\n",
            "Epoch [2/10], Training Loss: 0.947, Validation Accuracy: 60.12%\n",
            "Epoch [3/10], Training Loss: 0.912, Validation Accuracy: 61.02%\n",
            "Epoch [4/10], Training Loss: 0.892, Validation Accuracy: 61.26%\n",
            "Epoch [5/10], Training Loss: 0.866, Validation Accuracy: 61.22%\n",
            "Epoch [6/10], Training Loss: 0.848, Validation Accuracy: 61.05%\n",
            "Epoch [7/10], Training Loss: 0.835, Validation Accuracy: 60.69%\n",
            "Epoch [8/10], Training Loss: 0.815, Validation Accuracy: 60.10%\n",
            "Epoch [9/10], Training Loss: 0.798, Validation Accuracy: 59.93%\n",
            "Epoch [10/10], Training Loss: 0.784, Validation Accuracy: 60.78%\n",
            "Epoch [1/10], Training Loss: 1.008, Validation Accuracy: 60.49%\n",
            "Epoch [2/10], Training Loss: 0.931, Validation Accuracy: 60.67%\n",
            "Epoch [3/10], Training Loss: 0.890, Validation Accuracy: 61.09%\n",
            "Epoch [4/10], Training Loss: 0.870, Validation Accuracy: 60.70%\n",
            "Epoch [5/10], Training Loss: 0.843, Validation Accuracy: 60.76%\n",
            "Epoch [6/10], Training Loss: 0.818, Validation Accuracy: 60.72%\n",
            "Epoch [7/10], Training Loss: 0.802, Validation Accuracy: 60.52%\n",
            "Epoch [8/10], Training Loss: 0.784, Validation Accuracy: 60.73%\n",
            "Epoch [9/10], Training Loss: 0.769, Validation Accuracy: 60.18%\n",
            "Epoch [10/10], Training Loss: 0.750, Validation Accuracy: 60.94%\n",
            "Epoch [1/10], Training Loss: 1.026, Validation Accuracy: 61.07%\n",
            "Epoch [2/10], Training Loss: 0.957, Validation Accuracy: 60.38%\n",
            "Epoch [3/10], Training Loss: 0.915, Validation Accuracy: 60.67%\n",
            "Epoch [4/10], Training Loss: 0.888, Validation Accuracy: 60.85%\n",
            "Epoch [5/10], Training Loss: 0.864, Validation Accuracy: 60.53%\n",
            "Epoch [6/10], Training Loss: 0.849, Validation Accuracy: 60.80%\n",
            "Epoch [7/10], Training Loss: 0.827, Validation Accuracy: 59.74%\n",
            "Epoch [8/10], Training Loss: 0.810, Validation Accuracy: 60.30%\n",
            "Epoch [9/10], Training Loss: 0.793, Validation Accuracy: 60.65%\n",
            "Epoch [10/10], Training Loss: 0.775, Validation Accuracy: 60.18%\n",
            "Epoch [1/10], Training Loss: 0.959, Validation Accuracy: 60.96%\n",
            "Epoch [2/10], Training Loss: 0.885, Validation Accuracy: 60.96%\n",
            "Epoch [3/10], Training Loss: 0.857, Validation Accuracy: 60.63%\n",
            "Epoch [4/10], Training Loss: 0.830, Validation Accuracy: 61.30%\n",
            "Epoch [5/10], Training Loss: 0.814, Validation Accuracy: 61.59%\n",
            "Epoch [6/10], Training Loss: 0.774, Validation Accuracy: 60.83%\n",
            "Epoch [7/10], Training Loss: 0.758, Validation Accuracy: 60.39%\n",
            "Epoch [8/10], Training Loss: 0.749, Validation Accuracy: 60.66%\n",
            "Epoch [9/10], Training Loss: 0.729, Validation Accuracy: 60.62%\n",
            "Epoch [10/10], Training Loss: 0.707, Validation Accuracy: 60.72%\n",
            "Epoch [1/10], Training Loss: 0.982, Validation Accuracy: 59.97%\n",
            "Epoch [2/10], Training Loss: 0.909, Validation Accuracy: 60.50%\n",
            "Epoch [3/10], Training Loss: 0.860, Validation Accuracy: 61.20%\n",
            "Epoch [4/10], Training Loss: 0.825, Validation Accuracy: 61.51%\n",
            "Epoch [5/10], Training Loss: 0.802, Validation Accuracy: 60.98%\n",
            "Epoch [6/10], Training Loss: 0.789, Validation Accuracy: 60.59%\n",
            "Epoch [7/10], Training Loss: 0.767, Validation Accuracy: 60.72%\n",
            "Epoch [8/10], Training Loss: 0.737, Validation Accuracy: 60.35%\n",
            "Epoch [9/10], Training Loss: 0.723, Validation Accuracy: 60.52%\n",
            "Epoch [10/10], Training Loss: 0.716, Validation Accuracy: 60.27%\n",
            "Epoch [1/10], Training Loss: 0.954, Validation Accuracy: 59.99%\n",
            "Epoch [2/10], Training Loss: 0.878, Validation Accuracy: 61.44%\n",
            "Epoch [3/10], Training Loss: 0.828, Validation Accuracy: 61.04%\n",
            "Epoch [4/10], Training Loss: 0.797, Validation Accuracy: 60.98%\n",
            "Epoch [5/10], Training Loss: 0.778, Validation Accuracy: 61.22%\n",
            "Epoch [6/10], Training Loss: 0.755, Validation Accuracy: 60.81%\n",
            "Epoch [7/10], Training Loss: 0.731, Validation Accuracy: 60.81%\n",
            "Epoch [8/10], Training Loss: 0.707, Validation Accuracy: 60.01%\n",
            "Epoch [9/10], Training Loss: 0.698, Validation Accuracy: 60.70%\n",
            "Epoch [10/10], Training Loss: 0.691, Validation Accuracy: 59.56%\n",
            "Epoch [1/10], Training Loss: 0.947, Validation Accuracy: 61.26%\n",
            "Epoch [2/10], Training Loss: 0.857, Validation Accuracy: 61.16%\n",
            "Epoch [3/10], Training Loss: 0.810, Validation Accuracy: 60.51%\n",
            "Epoch [4/10], Training Loss: 0.778, Validation Accuracy: 61.39%\n",
            "Epoch [5/10], Training Loss: 0.751, Validation Accuracy: 60.72%\n",
            "Epoch [6/10], Training Loss: 0.721, Validation Accuracy: 61.17%\n",
            "Epoch [7/10], Training Loss: 0.714, Validation Accuracy: 60.67%\n",
            "Epoch [8/10], Training Loss: 0.687, Validation Accuracy: 61.00%\n",
            "Epoch [9/10], Training Loss: 0.672, Validation Accuracy: 61.01%\n",
            "Epoch [10/10], Training Loss: 0.653, Validation Accuracy: 61.08%\n",
            "Epoch [1/10], Training Loss: 0.982, Validation Accuracy: 60.39%\n",
            "Epoch [2/10], Training Loss: 0.883, Validation Accuracy: 61.33%\n",
            "Epoch [3/10], Training Loss: 0.849, Validation Accuracy: 60.48%\n",
            "Epoch [4/10], Training Loss: 0.811, Validation Accuracy: 61.34%\n",
            "Epoch [5/10], Training Loss: 0.778, Validation Accuracy: 60.38%\n",
            "Epoch [6/10], Training Loss: 0.756, Validation Accuracy: 60.94%\n",
            "Epoch [7/10], Training Loss: 0.735, Validation Accuracy: 60.95%\n",
            "Epoch [8/10], Training Loss: 0.722, Validation Accuracy: 60.63%\n",
            "Epoch [9/10], Training Loss: 0.692, Validation Accuracy: 60.96%\n",
            "Epoch [10/10], Training Loss: 0.672, Validation Accuracy: 60.62%\n",
            "Epoch [1/10], Training Loss: 0.907, Validation Accuracy: 61.18%\n",
            "Epoch [2/10], Training Loss: 0.813, Validation Accuracy: 61.43%\n",
            "Epoch [3/10], Training Loss: 0.777, Validation Accuracy: 60.77%\n",
            "Epoch [4/10], Training Loss: 0.743, Validation Accuracy: 61.11%\n",
            "Epoch [5/10], Training Loss: 0.722, Validation Accuracy: 60.59%\n",
            "Epoch [6/10], Training Loss: 0.688, Validation Accuracy: 60.60%\n",
            "Epoch [7/10], Training Loss: 0.673, Validation Accuracy: 59.92%\n",
            "Epoch [8/10], Training Loss: 0.654, Validation Accuracy: 60.36%\n",
            "Epoch [9/10], Training Loss: 0.627, Validation Accuracy: 60.83%\n",
            "Epoch [10/10], Training Loss: 0.606, Validation Accuracy: 60.83%\n",
            "Epoch [1/10], Training Loss: 0.930, Validation Accuracy: 59.42%\n",
            "Epoch [2/10], Training Loss: 0.841, Validation Accuracy: 61.23%\n",
            "Epoch [3/10], Training Loss: 0.792, Validation Accuracy: 61.31%\n",
            "Epoch [4/10], Training Loss: 0.751, Validation Accuracy: 60.73%\n",
            "Epoch [5/10], Training Loss: 0.735, Validation Accuracy: 61.26%\n",
            "Epoch [6/10], Training Loss: 0.698, Validation Accuracy: 61.00%\n",
            "Epoch [7/10], Training Loss: 0.673, Validation Accuracy: 60.76%\n",
            "Epoch [8/10], Training Loss: 0.661, Validation Accuracy: 60.69%\n",
            "Epoch [9/10], Training Loss: 0.635, Validation Accuracy: 60.15%\n",
            "Epoch [10/10], Training Loss: 0.625, Validation Accuracy: 59.86%\n",
            "Epoch [1/10], Training Loss: 0.885, Validation Accuracy: 60.18%\n",
            "Epoch [2/10], Training Loss: 0.798, Validation Accuracy: 61.49%\n",
            "Epoch [3/10], Training Loss: 0.749, Validation Accuracy: 61.22%\n",
            "Epoch [4/10], Training Loss: 0.716, Validation Accuracy: 61.27%\n",
            "Epoch [5/10], Training Loss: 0.695, Validation Accuracy: 60.80%\n",
            "Epoch [6/10], Training Loss: 0.663, Validation Accuracy: 61.31%\n",
            "Epoch [7/10], Training Loss: 0.638, Validation Accuracy: 61.00%\n",
            "Epoch [8/10], Training Loss: 0.626, Validation Accuracy: 60.61%\n",
            "Epoch [9/10], Training Loss: 0.601, Validation Accuracy: 60.94%\n",
            "Epoch [10/10], Training Loss: 0.583, Validation Accuracy: 60.80%\n",
            "Epoch [1/10], Training Loss: 0.904, Validation Accuracy: 60.48%\n",
            "Epoch [2/10], Training Loss: 0.794, Validation Accuracy: 60.83%\n",
            "Epoch [3/10], Training Loss: 0.733, Validation Accuracy: 61.73%\n",
            "Epoch [4/10], Training Loss: 0.698, Validation Accuracy: 61.23%\n",
            "Epoch [5/10], Training Loss: 0.664, Validation Accuracy: 61.37%\n",
            "Epoch [6/10], Training Loss: 0.647, Validation Accuracy: 60.44%\n",
            "Epoch [7/10], Training Loss: 0.631, Validation Accuracy: 60.45%\n",
            "Epoch [8/10], Training Loss: 0.587, Validation Accuracy: 60.81%\n",
            "Epoch [9/10], Training Loss: 0.577, Validation Accuracy: 60.78%\n",
            "Epoch [10/10], Training Loss: 0.564, Validation Accuracy: 60.28%\n",
            "Epoch [1/10], Training Loss: 0.934, Validation Accuracy: 60.62%\n",
            "Epoch [2/10], Training Loss: 0.830, Validation Accuracy: 61.03%\n",
            "Epoch [3/10], Training Loss: 0.777, Validation Accuracy: 61.35%\n",
            "Epoch [4/10], Training Loss: 0.738, Validation Accuracy: 61.30%\n",
            "Epoch [5/10], Training Loss: 0.702, Validation Accuracy: 61.08%\n",
            "Epoch [6/10], Training Loss: 0.674, Validation Accuracy: 61.08%\n",
            "Epoch [7/10], Training Loss: 0.640, Validation Accuracy: 61.20%\n",
            "Epoch [8/10], Training Loss: 0.622, Validation Accuracy: 60.12%\n",
            "Epoch [9/10], Training Loss: 0.597, Validation Accuracy: 60.92%\n",
            "Epoch [10/10], Training Loss: 0.589, Validation Accuracy: 60.65%\n",
            "Epoch [1/10], Training Loss: 0.859, Validation Accuracy: 61.15%\n",
            "Epoch [2/10], Training Loss: 0.760, Validation Accuracy: 60.45%\n",
            "Epoch [3/10], Training Loss: 0.709, Validation Accuracy: 61.32%\n",
            "Epoch [4/10], Training Loss: 0.668, Validation Accuracy: 60.76%\n",
            "Epoch [5/10], Training Loss: 0.633, Validation Accuracy: 60.84%\n",
            "Epoch [6/10], Training Loss: 0.603, Validation Accuracy: 61.17%\n",
            "Epoch [7/10], Training Loss: 0.582, Validation Accuracy: 61.05%\n",
            "Epoch [8/10], Training Loss: 0.554, Validation Accuracy: 61.11%\n",
            "Epoch [9/10], Training Loss: 0.534, Validation Accuracy: 60.69%\n",
            "Epoch [10/10], Training Loss: 0.521, Validation Accuracy: 59.97%\n",
            "Epoch [1/10], Training Loss: 0.887, Validation Accuracy: 59.74%\n",
            "Epoch [2/10], Training Loss: 0.776, Validation Accuracy: 60.81%\n",
            "Epoch [3/10], Training Loss: 0.724, Validation Accuracy: 60.36%\n",
            "Epoch [4/10], Training Loss: 0.685, Validation Accuracy: 61.58%\n",
            "Epoch [5/10], Training Loss: 0.641, Validation Accuracy: 61.10%\n",
            "Epoch [6/10], Training Loss: 0.609, Validation Accuracy: 60.72%\n",
            "Epoch [7/10], Training Loss: 0.598, Validation Accuracy: 60.55%\n",
            "Epoch [8/10], Training Loss: 0.564, Validation Accuracy: 60.50%\n",
            "Epoch [9/10], Training Loss: 0.538, Validation Accuracy: 60.02%\n",
            "Epoch [10/10], Training Loss: 0.525, Validation Accuracy: 60.90%\n",
            "Epoch [1/10], Training Loss: 0.856, Validation Accuracy: 60.65%\n",
            "Epoch [2/10], Training Loss: 0.735, Validation Accuracy: 60.99%\n",
            "Epoch [3/10], Training Loss: 0.685, Validation Accuracy: 60.81%\n",
            "Epoch [4/10], Training Loss: 0.640, Validation Accuracy: 61.52%\n",
            "Epoch [5/10], Training Loss: 0.603, Validation Accuracy: 60.98%\n",
            "Epoch [6/10], Training Loss: 0.579, Validation Accuracy: 60.71%\n",
            "Epoch [7/10], Training Loss: 0.555, Validation Accuracy: 60.57%\n",
            "Epoch [8/10], Training Loss: 0.522, Validation Accuracy: 60.36%\n",
            "Epoch [9/10], Training Loss: 0.516, Validation Accuracy: 60.44%\n",
            "Epoch [10/10], Training Loss: 0.482, Validation Accuracy: 60.43%\n",
            "Epoch [1/10], Training Loss: 0.855, Validation Accuracy: 61.12%\n",
            "Epoch [2/10], Training Loss: 0.725, Validation Accuracy: 60.61%\n",
            "Epoch [3/10], Training Loss: 0.668, Validation Accuracy: 60.61%\n",
            "Epoch [4/10], Training Loss: 0.617, Validation Accuracy: 60.75%\n",
            "Epoch [5/10], Training Loss: 0.591, Validation Accuracy: 60.48%\n",
            "Epoch [6/10], Training Loss: 0.568, Validation Accuracy: 60.58%\n",
            "Epoch [7/10], Training Loss: 0.535, Validation Accuracy: 61.10%\n",
            "Epoch [8/10], Training Loss: 0.512, Validation Accuracy: 60.45%\n",
            "Epoch [9/10], Training Loss: 0.492, Validation Accuracy: 60.11%\n",
            "Epoch [10/10], Training Loss: 0.470, Validation Accuracy: 60.38%\n",
            "Epoch [1/10], Training Loss: 0.903, Validation Accuracy: 60.48%\n",
            "Epoch [2/10], Training Loss: 0.765, Validation Accuracy: 60.63%\n",
            "Epoch [3/10], Training Loss: 0.716, Validation Accuracy: 60.36%\n",
            "Epoch [4/10], Training Loss: 0.649, Validation Accuracy: 60.95%\n",
            "Epoch [5/10], Training Loss: 0.610, Validation Accuracy: 60.38%\n",
            "Epoch [6/10], Training Loss: 0.580, Validation Accuracy: 60.75%\n",
            "Epoch [7/10], Training Loss: 0.549, Validation Accuracy: 60.30%\n",
            "Epoch [8/10], Training Loss: 0.529, Validation Accuracy: 60.37%\n",
            "Epoch [9/10], Training Loss: 0.499, Validation Accuracy: 61.07%\n",
            "Epoch [10/10], Training Loss: 0.489, Validation Accuracy: 60.89%\n",
            "Epoch [1/10], Training Loss: 0.830, Validation Accuracy: 60.05%\n",
            "Epoch [2/10], Training Loss: 0.703, Validation Accuracy: 60.35%\n",
            "Epoch [3/10], Training Loss: 0.635, Validation Accuracy: 60.55%\n",
            "Epoch [4/10], Training Loss: 0.584, Validation Accuracy: 60.61%\n",
            "Epoch [5/10], Training Loss: 0.553, Validation Accuracy: 60.71%\n",
            "Epoch [6/10], Training Loss: 0.515, Validation Accuracy: 60.63%\n",
            "Epoch [7/10], Training Loss: 0.489, Validation Accuracy: 60.32%\n",
            "Epoch [8/10], Training Loss: 0.472, Validation Accuracy: 60.62%\n",
            "Epoch [9/10], Training Loss: 0.447, Validation Accuracy: 59.92%\n",
            "Epoch [10/10], Training Loss: 0.442, Validation Accuracy: 60.03%\n",
            "Epoch [1/10], Training Loss: 0.860, Validation Accuracy: 60.82%\n",
            "Epoch [2/10], Training Loss: 0.713, Validation Accuracy: 60.89%\n",
            "Epoch [3/10], Training Loss: 0.643, Validation Accuracy: 60.81%\n",
            "Epoch [4/10], Training Loss: 0.606, Validation Accuracy: 60.81%\n",
            "Epoch [5/10], Training Loss: 0.560, Validation Accuracy: 60.30%\n",
            "Epoch [6/10], Training Loss: 0.544, Validation Accuracy: 60.72%\n",
            "Epoch [7/10], Training Loss: 0.508, Validation Accuracy: 60.63%\n",
            "Epoch [8/10], Training Loss: 0.484, Validation Accuracy: 59.61%\n",
            "Epoch [9/10], Training Loss: 0.460, Validation Accuracy: 59.48%\n",
            "Epoch [10/10], Training Loss: 0.436, Validation Accuracy: 60.09%\n",
            "Epoch [1/10], Training Loss: 0.816, Validation Accuracy: 59.35%\n",
            "Epoch [2/10], Training Loss: 0.675, Validation Accuracy: 61.20%\n",
            "Epoch [3/10], Training Loss: 0.608, Validation Accuracy: 60.33%\n",
            "Epoch [4/10], Training Loss: 0.553, Validation Accuracy: 60.63%\n",
            "Epoch [5/10], Training Loss: 0.519, Validation Accuracy: 60.20%\n",
            "Epoch [6/10], Training Loss: 0.494, Validation Accuracy: 59.06%\n",
            "Epoch [7/10], Training Loss: 0.463, Validation Accuracy: 60.28%\n",
            "Epoch [8/10], Training Loss: 0.431, Validation Accuracy: 60.33%\n",
            "Epoch [9/10], Training Loss: 0.412, Validation Accuracy: 59.64%\n",
            "Epoch [10/10], Training Loss: 0.393, Validation Accuracy: 59.75%\n",
            "Confusion Matrix:\n",
            "[[640  76  27  24  28  15  20  12 117  41]\n",
            " [ 21 790   5  11   7   9  19   6  42  90]\n",
            " [ 89  23 326 104 165 113  93  47  24  16]\n",
            " [ 21  27  29 425  80 206  94  54  39  25]\n",
            " [ 25  15  49  86 581  62  93  61  18  10]\n",
            " [ 23   8  30 196  76 528  46  66  12  15]\n",
            " [ 11  22  27  79  64  46 718  14   7  12]\n",
            " [ 22  10  20  59 100  82  21 641  14  31]\n",
            " [ 64  79  11  32  18  10  10   7 732  37]\n",
            " [ 41 197   8  36  12  14  22  25  62 583]]\n",
            "Test Accuracy: 59.64%\n",
            "True Positives (TP): [640 790 326 425 581 528 718 641 732 583]\n",
            "False Positives (FP): [317 457 206 627 550 557 418 292 335 277]\n",
            "True Negatives (TN): [8683 8543 8794 8373 8450 8443 8582 8708 8665 8723]\n",
            "False Negatives (FN): [360 210 674 575 419 472 282 359 268 417]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.66875653 0.63352045 0.61278195 0.4039924  0.51370469 0.48663594\n",
            " 0.63204225 0.68703108 0.68603561 0.67790698]\n",
            "Recall: [0.64  0.79  0.326 0.425 0.581 0.528 0.718 0.641 0.732 0.583]\n",
            "F1 Score: [0.65406234 0.70315977 0.42558747 0.41423002 0.5452839  0.50647482\n",
            " 0.67228464 0.6632178  0.70827286 0.62688172]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 0.5):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=0.5):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE,  distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average =  augmented_data_truncated\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=0.5)\n",
        "\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "beta=1"
      ],
      "metadata": {
        "id": "-ql0YSmiSHMi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kMMJA1D8SI_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7138e851-8078-48ce-d863-7e77f0816a1b",
        "id": "o41xZOO4SJZ5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:11<00:00, 15.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Images per Class: [6102 6112 6076 5866 5966 5879 5922 6022 6175 5880]\n",
            "Epoch [1/10], Training Loss: 2.304, Validation Accuracy: 9.39%\n",
            "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 9.39%\n",
            "Epoch [3/10], Training Loss: 2.303, Validation Accuracy: 9.39%\n",
            "Epoch [4/10], Training Loss: 2.302, Validation Accuracy: 9.39%\n",
            "Epoch [5/10], Training Loss: 2.302, Validation Accuracy: 9.37%\n",
            "Epoch [6/10], Training Loss: 2.301, Validation Accuracy: 9.40%\n",
            "Epoch [7/10], Training Loss: 2.301, Validation Accuracy: 9.44%\n",
            "Epoch [8/10], Training Loss: 2.301, Validation Accuracy: 9.41%\n",
            "Epoch [9/10], Training Loss: 2.300, Validation Accuracy: 9.40%\n",
            "Epoch [10/10], Training Loss: 2.299, Validation Accuracy: 9.40%\n",
            "Epoch [1/10], Training Loss: 2.300, Validation Accuracy: 9.39%\n",
            "Epoch [2/10], Training Loss: 2.299, Validation Accuracy: 9.39%\n",
            "Epoch [3/10], Training Loss: 2.297, Validation Accuracy: 9.41%\n",
            "Epoch [4/10], Training Loss: 2.295, Validation Accuracy: 9.43%\n",
            "Epoch [5/10], Training Loss: 2.293, Validation Accuracy: 10.43%\n",
            "Epoch [6/10], Training Loss: 2.289, Validation Accuracy: 12.70%\n",
            "Epoch [7/10], Training Loss: 2.284, Validation Accuracy: 15.17%\n",
            "Epoch [8/10], Training Loss: 2.277, Validation Accuracy: 17.09%\n",
            "Epoch [9/10], Training Loss: 2.267, Validation Accuracy: 19.17%\n",
            "Epoch [10/10], Training Loss: 2.253, Validation Accuracy: 19.31%\n",
            "Epoch [1/10], Training Loss: 2.237, Validation Accuracy: 19.93%\n",
            "Epoch [2/10], Training Loss: 2.211, Validation Accuracy: 20.41%\n",
            "Epoch [3/10], Training Loss: 2.177, Validation Accuracy: 20.83%\n",
            "Epoch [4/10], Training Loss: 2.141, Validation Accuracy: 21.83%\n",
            "Epoch [5/10], Training Loss: 2.110, Validation Accuracy: 23.63%\n",
            "Epoch [6/10], Training Loss: 2.083, Validation Accuracy: 24.77%\n",
            "Epoch [7/10], Training Loss: 2.058, Validation Accuracy: 25.59%\n",
            "Epoch [8/10], Training Loss: 2.040, Validation Accuracy: 26.38%\n",
            "Epoch [9/10], Training Loss: 2.022, Validation Accuracy: 26.48%\n",
            "Epoch [10/10], Training Loss: 2.001, Validation Accuracy: 27.14%\n",
            "Epoch [1/10], Training Loss: 1.975, Validation Accuracy: 27.73%\n",
            "Epoch [2/10], Training Loss: 1.956, Validation Accuracy: 28.79%\n",
            "Epoch [3/10], Training Loss: 1.939, Validation Accuracy: 29.03%\n",
            "Epoch [4/10], Training Loss: 1.925, Validation Accuracy: 30.22%\n",
            "Epoch [5/10], Training Loss: 1.912, Validation Accuracy: 30.34%\n",
            "Epoch [6/10], Training Loss: 1.897, Validation Accuracy: 31.40%\n",
            "Epoch [7/10], Training Loss: 1.884, Validation Accuracy: 31.91%\n",
            "Epoch [8/10], Training Loss: 1.876, Validation Accuracy: 31.41%\n",
            "Epoch [9/10], Training Loss: 1.862, Validation Accuracy: 32.29%\n",
            "Epoch [10/10], Training Loss: 1.852, Validation Accuracy: 33.15%\n",
            "Epoch [1/10], Training Loss: 1.865, Validation Accuracy: 33.40%\n",
            "Epoch [2/10], Training Loss: 1.852, Validation Accuracy: 33.67%\n",
            "Epoch [3/10], Training Loss: 1.839, Validation Accuracy: 34.31%\n",
            "Epoch [4/10], Training Loss: 1.828, Validation Accuracy: 34.97%\n",
            "Epoch [5/10], Training Loss: 1.815, Validation Accuracy: 35.14%\n",
            "Epoch [6/10], Training Loss: 1.803, Validation Accuracy: 35.29%\n",
            "Epoch [7/10], Training Loss: 1.788, Validation Accuracy: 35.22%\n",
            "Epoch [8/10], Training Loss: 1.779, Validation Accuracy: 36.57%\n",
            "Epoch [9/10], Training Loss: 1.767, Validation Accuracy: 36.87%\n",
            "Epoch [10/10], Training Loss: 1.751, Validation Accuracy: 37.29%\n",
            "Epoch [1/10], Training Loss: 1.757, Validation Accuracy: 36.94%\n",
            "Epoch [2/10], Training Loss: 1.748, Validation Accuracy: 37.80%\n",
            "Epoch [3/10], Training Loss: 1.732, Validation Accuracy: 37.70%\n",
            "Epoch [4/10], Training Loss: 1.717, Validation Accuracy: 38.70%\n",
            "Epoch [5/10], Training Loss: 1.705, Validation Accuracy: 39.03%\n",
            "Epoch [6/10], Training Loss: 1.692, Validation Accuracy: 39.46%\n",
            "Epoch [7/10], Training Loss: 1.680, Validation Accuracy: 38.59%\n",
            "Epoch [8/10], Training Loss: 1.669, Validation Accuracy: 40.24%\n",
            "Epoch [9/10], Training Loss: 1.654, Validation Accuracy: 40.60%\n",
            "Epoch [10/10], Training Loss: 1.639, Validation Accuracy: 40.68%\n",
            "Epoch [1/10], Training Loss: 1.652, Validation Accuracy: 40.98%\n",
            "Epoch [2/10], Training Loss: 1.639, Validation Accuracy: 41.74%\n",
            "Epoch [3/10], Training Loss: 1.623, Validation Accuracy: 42.22%\n",
            "Epoch [4/10], Training Loss: 1.610, Validation Accuracy: 42.66%\n",
            "Epoch [5/10], Training Loss: 1.597, Validation Accuracy: 42.61%\n",
            "Epoch [6/10], Training Loss: 1.586, Validation Accuracy: 42.52%\n",
            "Epoch [7/10], Training Loss: 1.571, Validation Accuracy: 42.43%\n",
            "Epoch [8/10], Training Loss: 1.560, Validation Accuracy: 43.01%\n",
            "Epoch [9/10], Training Loss: 1.551, Validation Accuracy: 43.38%\n",
            "Epoch [10/10], Training Loss: 1.540, Validation Accuracy: 44.34%\n",
            "Epoch [1/10], Training Loss: 1.549, Validation Accuracy: 44.51%\n",
            "Epoch [2/10], Training Loss: 1.539, Validation Accuracy: 45.15%\n",
            "Epoch [3/10], Training Loss: 1.516, Validation Accuracy: 45.35%\n",
            "Epoch [4/10], Training Loss: 1.501, Validation Accuracy: 45.00%\n",
            "Epoch [5/10], Training Loss: 1.493, Validation Accuracy: 45.25%\n",
            "Epoch [6/10], Training Loss: 1.484, Validation Accuracy: 45.84%\n",
            "Epoch [7/10], Training Loss: 1.470, Validation Accuracy: 46.15%\n",
            "Epoch [8/10], Training Loss: 1.465, Validation Accuracy: 45.90%\n",
            "Epoch [9/10], Training Loss: 1.455, Validation Accuracy: 46.51%\n",
            "Epoch [10/10], Training Loss: 1.443, Validation Accuracy: 46.05%\n",
            "Epoch [1/10], Training Loss: 1.470, Validation Accuracy: 47.12%\n",
            "Epoch [2/10], Training Loss: 1.454, Validation Accuracy: 46.87%\n",
            "Epoch [3/10], Training Loss: 1.444, Validation Accuracy: 47.91%\n",
            "Epoch [4/10], Training Loss: 1.434, Validation Accuracy: 47.90%\n",
            "Epoch [5/10], Training Loss: 1.418, Validation Accuracy: 48.02%\n",
            "Epoch [6/10], Training Loss: 1.420, Validation Accuracy: 48.35%\n",
            "Epoch [7/10], Training Loss: 1.405, Validation Accuracy: 48.22%\n",
            "Epoch [8/10], Training Loss: 1.394, Validation Accuracy: 48.75%\n",
            "Epoch [9/10], Training Loss: 1.383, Validation Accuracy: 48.71%\n",
            "Epoch [10/10], Training Loss: 1.381, Validation Accuracy: 48.30%\n",
            "Epoch [1/10], Training Loss: 1.430, Validation Accuracy: 48.72%\n",
            "Epoch [2/10], Training Loss: 1.413, Validation Accuracy: 48.93%\n",
            "Epoch [3/10], Training Loss: 1.404, Validation Accuracy: 47.68%\n",
            "Epoch [4/10], Training Loss: 1.396, Validation Accuracy: 49.16%\n",
            "Epoch [5/10], Training Loss: 1.393, Validation Accuracy: 48.64%\n",
            "Epoch [6/10], Training Loss: 1.376, Validation Accuracy: 48.77%\n",
            "Epoch [7/10], Training Loss: 1.367, Validation Accuracy: 49.77%\n",
            "Epoch [8/10], Training Loss: 1.355, Validation Accuracy: 49.62%\n",
            "Epoch [9/10], Training Loss: 1.352, Validation Accuracy: 50.02%\n",
            "Epoch [10/10], Training Loss: 1.342, Validation Accuracy: 50.12%\n",
            "Epoch [1/10], Training Loss: 1.389, Validation Accuracy: 50.48%\n",
            "Epoch [2/10], Training Loss: 1.373, Validation Accuracy: 50.16%\n",
            "Epoch [3/10], Training Loss: 1.356, Validation Accuracy: 50.91%\n",
            "Epoch [4/10], Training Loss: 1.352, Validation Accuracy: 50.14%\n",
            "Epoch [5/10], Training Loss: 1.343, Validation Accuracy: 50.44%\n",
            "Epoch [6/10], Training Loss: 1.332, Validation Accuracy: 50.37%\n",
            "Epoch [7/10], Training Loss: 1.323, Validation Accuracy: 51.04%\n",
            "Epoch [8/10], Training Loss: 1.316, Validation Accuracy: 51.41%\n",
            "Epoch [9/10], Training Loss: 1.311, Validation Accuracy: 48.51%\n",
            "Epoch [10/10], Training Loss: 1.307, Validation Accuracy: 51.60%\n",
            "Epoch [1/10], Training Loss: 1.358, Validation Accuracy: 52.24%\n",
            "Epoch [2/10], Training Loss: 1.343, Validation Accuracy: 51.75%\n",
            "Epoch [3/10], Training Loss: 1.334, Validation Accuracy: 51.51%\n",
            "Epoch [4/10], Training Loss: 1.318, Validation Accuracy: 52.37%\n",
            "Epoch [5/10], Training Loss: 1.311, Validation Accuracy: 51.94%\n",
            "Epoch [6/10], Training Loss: 1.310, Validation Accuracy: 51.01%\n",
            "Epoch [7/10], Training Loss: 1.301, Validation Accuracy: 51.70%\n",
            "Epoch [8/10], Training Loss: 1.286, Validation Accuracy: 52.53%\n",
            "Epoch [9/10], Training Loss: 1.280, Validation Accuracy: 51.80%\n",
            "Epoch [10/10], Training Loss: 1.270, Validation Accuracy: 52.90%\n",
            "Epoch [1/10], Training Loss: 1.314, Validation Accuracy: 52.76%\n",
            "Epoch [2/10], Training Loss: 1.301, Validation Accuracy: 52.07%\n",
            "Epoch [3/10], Training Loss: 1.295, Validation Accuracy: 52.59%\n",
            "Epoch [4/10], Training Loss: 1.278, Validation Accuracy: 52.29%\n",
            "Epoch [5/10], Training Loss: 1.273, Validation Accuracy: 52.45%\n",
            "Epoch [6/10], Training Loss: 1.264, Validation Accuracy: 52.53%\n",
            "Epoch [7/10], Training Loss: 1.248, Validation Accuracy: 53.16%\n",
            "Epoch [8/10], Training Loss: 1.246, Validation Accuracy: 52.80%\n",
            "Epoch [9/10], Training Loss: 1.237, Validation Accuracy: 53.35%\n",
            "Epoch [10/10], Training Loss: 1.226, Validation Accuracy: 52.26%\n",
            "Epoch [1/10], Training Loss: 1.287, Validation Accuracy: 53.49%\n",
            "Epoch [2/10], Training Loss: 1.272, Validation Accuracy: 53.27%\n",
            "Epoch [3/10], Training Loss: 1.250, Validation Accuracy: 53.52%\n",
            "Epoch [4/10], Training Loss: 1.238, Validation Accuracy: 53.54%\n",
            "Epoch [5/10], Training Loss: 1.223, Validation Accuracy: 53.74%\n",
            "Epoch [6/10], Training Loss: 1.213, Validation Accuracy: 53.41%\n",
            "Epoch [7/10], Training Loss: 1.211, Validation Accuracy: 54.02%\n",
            "Epoch [8/10], Training Loss: 1.197, Validation Accuracy: 54.56%\n",
            "Epoch [9/10], Training Loss: 1.185, Validation Accuracy: 54.39%\n",
            "Epoch [10/10], Training Loss: 1.179, Validation Accuracy: 54.09%\n",
            "Epoch [1/10], Training Loss: 1.250, Validation Accuracy: 54.82%\n",
            "Epoch [2/10], Training Loss: 1.233, Validation Accuracy: 54.29%\n",
            "Epoch [3/10], Training Loss: 1.220, Validation Accuracy: 54.97%\n",
            "Epoch [4/10], Training Loss: 1.199, Validation Accuracy: 54.73%\n",
            "Epoch [5/10], Training Loss: 1.205, Validation Accuracy: 55.22%\n",
            "Epoch [6/10], Training Loss: 1.186, Validation Accuracy: 54.68%\n",
            "Epoch [7/10], Training Loss: 1.171, Validation Accuracy: 54.63%\n",
            "Epoch [8/10], Training Loss: 1.167, Validation Accuracy: 54.90%\n",
            "Epoch [9/10], Training Loss: 1.155, Validation Accuracy: 54.84%\n",
            "Epoch [10/10], Training Loss: 1.150, Validation Accuracy: 55.22%\n",
            "Epoch [1/10], Training Loss: 1.222, Validation Accuracy: 53.97%\n",
            "Epoch [2/10], Training Loss: 1.207, Validation Accuracy: 55.73%\n",
            "Epoch [3/10], Training Loss: 1.186, Validation Accuracy: 55.77%\n",
            "Epoch [4/10], Training Loss: 1.179, Validation Accuracy: 55.67%\n",
            "Epoch [5/10], Training Loss: 1.173, Validation Accuracy: 55.41%\n",
            "Epoch [6/10], Training Loss: 1.153, Validation Accuracy: 55.83%\n",
            "Epoch [7/10], Training Loss: 1.142, Validation Accuracy: 55.46%\n",
            "Epoch [8/10], Training Loss: 1.133, Validation Accuracy: 55.23%\n",
            "Epoch [9/10], Training Loss: 1.128, Validation Accuracy: 56.26%\n",
            "Epoch [10/10], Training Loss: 1.118, Validation Accuracy: 56.01%\n",
            "Epoch [1/10], Training Loss: 1.216, Validation Accuracy: 56.09%\n",
            "Epoch [2/10], Training Loss: 1.187, Validation Accuracy: 55.97%\n",
            "Epoch [3/10], Training Loss: 1.174, Validation Accuracy: 56.02%\n",
            "Epoch [4/10], Training Loss: 1.164, Validation Accuracy: 56.40%\n",
            "Epoch [5/10], Training Loss: 1.151, Validation Accuracy: 55.89%\n",
            "Epoch [6/10], Training Loss: 1.141, Validation Accuracy: 56.77%\n",
            "Epoch [7/10], Training Loss: 1.127, Validation Accuracy: 56.87%\n",
            "Epoch [8/10], Training Loss: 1.127, Validation Accuracy: 55.21%\n",
            "Epoch [9/10], Training Loss: 1.118, Validation Accuracy: 56.21%\n",
            "Epoch [10/10], Training Loss: 1.109, Validation Accuracy: 56.46%\n",
            "Epoch [1/10], Training Loss: 1.196, Validation Accuracy: 56.67%\n",
            "Epoch [2/10], Training Loss: 1.176, Validation Accuracy: 56.44%\n",
            "Epoch [3/10], Training Loss: 1.157, Validation Accuracy: 56.65%\n",
            "Epoch [4/10], Training Loss: 1.140, Validation Accuracy: 56.77%\n",
            "Epoch [5/10], Training Loss: 1.123, Validation Accuracy: 56.75%\n",
            "Epoch [6/10], Training Loss: 1.123, Validation Accuracy: 56.54%\n",
            "Epoch [7/10], Training Loss: 1.108, Validation Accuracy: 56.18%\n",
            "Epoch [8/10], Training Loss: 1.105, Validation Accuracy: 57.06%\n",
            "Epoch [9/10], Training Loss: 1.086, Validation Accuracy: 56.98%\n",
            "Epoch [10/10], Training Loss: 1.076, Validation Accuracy: 56.04%\n",
            "Epoch [1/10], Training Loss: 1.157, Validation Accuracy: 57.14%\n",
            "Epoch [2/10], Training Loss: 1.129, Validation Accuracy: 57.18%\n",
            "Epoch [3/10], Training Loss: 1.109, Validation Accuracy: 57.52%\n",
            "Epoch [4/10], Training Loss: 1.091, Validation Accuracy: 57.19%\n",
            "Epoch [5/10], Training Loss: 1.079, Validation Accuracy: 57.23%\n",
            "Epoch [6/10], Training Loss: 1.070, Validation Accuracy: 56.98%\n",
            "Epoch [7/10], Training Loss: 1.063, Validation Accuracy: 57.33%\n",
            "Epoch [8/10], Training Loss: 1.049, Validation Accuracy: 57.81%\n",
            "Epoch [9/10], Training Loss: 1.034, Validation Accuracy: 57.70%\n",
            "Epoch [10/10], Training Loss: 1.023, Validation Accuracy: 57.22%\n",
            "Epoch [1/10], Training Loss: 1.152, Validation Accuracy: 56.89%\n",
            "Epoch [2/10], Training Loss: 1.111, Validation Accuracy: 57.90%\n",
            "Epoch [3/10], Training Loss: 1.098, Validation Accuracy: 57.48%\n",
            "Epoch [4/10], Training Loss: 1.085, Validation Accuracy: 57.87%\n",
            "Epoch [5/10], Training Loss: 1.075, Validation Accuracy: 57.78%\n",
            "Epoch [6/10], Training Loss: 1.056, Validation Accuracy: 57.75%\n",
            "Epoch [7/10], Training Loss: 1.043, Validation Accuracy: 57.79%\n",
            "Epoch [8/10], Training Loss: 1.031, Validation Accuracy: 58.64%\n",
            "Epoch [9/10], Training Loss: 1.021, Validation Accuracy: 58.57%\n",
            "Epoch [10/10], Training Loss: 1.011, Validation Accuracy: 58.33%\n",
            "Epoch [1/10], Training Loss: 1.116, Validation Accuracy: 58.72%\n",
            "Epoch [2/10], Training Loss: 1.079, Validation Accuracy: 58.22%\n",
            "Epoch [3/10], Training Loss: 1.067, Validation Accuracy: 58.39%\n",
            "Epoch [4/10], Training Loss: 1.056, Validation Accuracy: 57.76%\n",
            "Epoch [5/10], Training Loss: 1.042, Validation Accuracy: 58.75%\n",
            "Epoch [6/10], Training Loss: 1.022, Validation Accuracy: 58.59%\n",
            "Epoch [7/10], Training Loss: 1.009, Validation Accuracy: 58.82%\n",
            "Epoch [8/10], Training Loss: 1.003, Validation Accuracy: 58.78%\n",
            "Epoch [9/10], Training Loss: 0.999, Validation Accuracy: 58.26%\n",
            "Epoch [10/10], Training Loss: 0.990, Validation Accuracy: 57.84%\n",
            "Epoch [1/10], Training Loss: 1.114, Validation Accuracy: 58.72%\n",
            "Epoch [2/10], Training Loss: 1.082, Validation Accuracy: 58.77%\n",
            "Epoch [3/10], Training Loss: 1.062, Validation Accuracy: 58.96%\n",
            "Epoch [4/10], Training Loss: 1.043, Validation Accuracy: 59.45%\n",
            "Epoch [5/10], Training Loss: 1.039, Validation Accuracy: 58.79%\n",
            "Epoch [6/10], Training Loss: 1.027, Validation Accuracy: 58.71%\n",
            "Epoch [7/10], Training Loss: 1.016, Validation Accuracy: 58.86%\n",
            "Epoch [8/10], Training Loss: 1.010, Validation Accuracy: 59.18%\n",
            "Epoch [9/10], Training Loss: 0.989, Validation Accuracy: 58.49%\n",
            "Epoch [10/10], Training Loss: 0.985, Validation Accuracy: 58.87%\n",
            "Epoch [1/10], Training Loss: 1.105, Validation Accuracy: 58.86%\n",
            "Epoch [2/10], Training Loss: 1.068, Validation Accuracy: 58.47%\n",
            "Epoch [3/10], Training Loss: 1.052, Validation Accuracy: 58.99%\n",
            "Epoch [4/10], Training Loss: 1.030, Validation Accuracy: 59.21%\n",
            "Epoch [5/10], Training Loss: 1.015, Validation Accuracy: 58.77%\n",
            "Epoch [6/10], Training Loss: 1.007, Validation Accuracy: 59.22%\n",
            "Epoch [7/10], Training Loss: 1.000, Validation Accuracy: 58.60%\n",
            "Epoch [8/10], Training Loss: 0.983, Validation Accuracy: 58.77%\n",
            "Epoch [9/10], Training Loss: 0.973, Validation Accuracy: 59.08%\n",
            "Epoch [10/10], Training Loss: 0.958, Validation Accuracy: 59.33%\n",
            "Epoch [1/10], Training Loss: 1.062, Validation Accuracy: 59.57%\n",
            "Epoch [2/10], Training Loss: 1.015, Validation Accuracy: 59.91%\n",
            "Epoch [3/10], Training Loss: 0.995, Validation Accuracy: 59.20%\n",
            "Epoch [4/10], Training Loss: 0.978, Validation Accuracy: 59.85%\n",
            "Epoch [5/10], Training Loss: 0.965, Validation Accuracy: 59.73%\n",
            "Epoch [6/10], Training Loss: 0.957, Validation Accuracy: 59.94%\n",
            "Epoch [7/10], Training Loss: 0.945, Validation Accuracy: 59.86%\n",
            "Epoch [8/10], Training Loss: 0.933, Validation Accuracy: 59.92%\n",
            "Epoch [9/10], Training Loss: 0.924, Validation Accuracy: 59.51%\n",
            "Epoch [10/10], Training Loss: 0.906, Validation Accuracy: 59.31%\n",
            "Epoch [1/10], Training Loss: 1.067, Validation Accuracy: 59.47%\n",
            "Epoch [2/10], Training Loss: 1.021, Validation Accuracy: 59.96%\n",
            "Epoch [3/10], Training Loss: 0.995, Validation Accuracy: 60.20%\n",
            "Epoch [4/10], Training Loss: 0.976, Validation Accuracy: 60.01%\n",
            "Epoch [5/10], Training Loss: 0.962, Validation Accuracy: 60.04%\n",
            "Epoch [6/10], Training Loss: 0.951, Validation Accuracy: 58.86%\n",
            "Epoch [7/10], Training Loss: 0.935, Validation Accuracy: 59.87%\n",
            "Epoch [8/10], Training Loss: 0.922, Validation Accuracy: 59.63%\n",
            "Epoch [9/10], Training Loss: 0.910, Validation Accuracy: 60.08%\n",
            "Epoch [10/10], Training Loss: 0.911, Validation Accuracy: 59.99%\n",
            "Epoch [1/10], Training Loss: 1.026, Validation Accuracy: 59.79%\n",
            "Epoch [2/10], Training Loss: 0.997, Validation Accuracy: 60.26%\n",
            "Epoch [3/10], Training Loss: 0.976, Validation Accuracy: 60.04%\n",
            "Epoch [4/10], Training Loss: 0.948, Validation Accuracy: 59.14%\n",
            "Epoch [5/10], Training Loss: 0.936, Validation Accuracy: 59.93%\n",
            "Epoch [6/10], Training Loss: 0.920, Validation Accuracy: 59.93%\n",
            "Epoch [7/10], Training Loss: 0.911, Validation Accuracy: 59.90%\n",
            "Epoch [8/10], Training Loss: 0.899, Validation Accuracy: 59.92%\n",
            "Epoch [9/10], Training Loss: 0.892, Validation Accuracy: 59.45%\n",
            "Epoch [10/10], Training Loss: 0.880, Validation Accuracy: 59.33%\n",
            "Epoch [1/10], Training Loss: 1.050, Validation Accuracy: 60.85%\n",
            "Epoch [2/10], Training Loss: 1.006, Validation Accuracy: 60.31%\n",
            "Epoch [3/10], Training Loss: 0.974, Validation Accuracy: 60.33%\n",
            "Epoch [4/10], Training Loss: 0.963, Validation Accuracy: 60.61%\n",
            "Epoch [5/10], Training Loss: 0.944, Validation Accuracy: 60.86%\n",
            "Epoch [6/10], Training Loss: 0.930, Validation Accuracy: 60.42%\n",
            "Epoch [7/10], Training Loss: 0.914, Validation Accuracy: 60.72%\n",
            "Epoch [8/10], Training Loss: 0.899, Validation Accuracy: 61.02%\n",
            "Epoch [9/10], Training Loss: 0.890, Validation Accuracy: 60.81%\n",
            "Epoch [10/10], Training Loss: 0.876, Validation Accuracy: 59.99%\n",
            "Epoch [1/10], Training Loss: 1.031, Validation Accuracy: 60.73%\n",
            "Epoch [2/10], Training Loss: 0.988, Validation Accuracy: 60.65%\n",
            "Epoch [3/10], Training Loss: 0.957, Validation Accuracy: 60.43%\n",
            "Epoch [4/10], Training Loss: 0.943, Validation Accuracy: 59.99%\n",
            "Epoch [5/10], Training Loss: 0.922, Validation Accuracy: 60.83%\n",
            "Epoch [6/10], Training Loss: 0.911, Validation Accuracy: 60.42%\n",
            "Epoch [7/10], Training Loss: 0.887, Validation Accuracy: 60.77%\n",
            "Epoch [8/10], Training Loss: 0.876, Validation Accuracy: 60.48%\n",
            "Epoch [9/10], Training Loss: 0.865, Validation Accuracy: 60.46%\n",
            "Epoch [10/10], Training Loss: 0.856, Validation Accuracy: 60.73%\n",
            "Epoch [1/10], Training Loss: 0.991, Validation Accuracy: 60.64%\n",
            "Epoch [2/10], Training Loss: 0.946, Validation Accuracy: 61.27%\n",
            "Epoch [3/10], Training Loss: 0.916, Validation Accuracy: 60.66%\n",
            "Epoch [4/10], Training Loss: 0.892, Validation Accuracy: 60.65%\n",
            "Epoch [5/10], Training Loss: 0.874, Validation Accuracy: 61.55%\n",
            "Epoch [6/10], Training Loss: 0.869, Validation Accuracy: 60.59%\n",
            "Epoch [7/10], Training Loss: 0.854, Validation Accuracy: 61.52%\n",
            "Epoch [8/10], Training Loss: 0.826, Validation Accuracy: 61.31%\n",
            "Epoch [9/10], Training Loss: 0.820, Validation Accuracy: 60.80%\n",
            "Epoch [10/10], Training Loss: 0.810, Validation Accuracy: 60.72%\n",
            "Epoch [1/10], Training Loss: 1.003, Validation Accuracy: 61.26%\n",
            "Epoch [2/10], Training Loss: 0.939, Validation Accuracy: 60.99%\n",
            "Epoch [3/10], Training Loss: 0.917, Validation Accuracy: 61.09%\n",
            "Epoch [4/10], Training Loss: 0.897, Validation Accuracy: 61.20%\n",
            "Epoch [5/10], Training Loss: 0.866, Validation Accuracy: 61.58%\n",
            "Epoch [6/10], Training Loss: 0.857, Validation Accuracy: 61.24%\n",
            "Epoch [7/10], Training Loss: 0.844, Validation Accuracy: 61.67%\n",
            "Epoch [8/10], Training Loss: 0.817, Validation Accuracy: 61.30%\n",
            "Epoch [9/10], Training Loss: 0.813, Validation Accuracy: 61.07%\n",
            "Epoch [10/10], Training Loss: 0.801, Validation Accuracy: 61.12%\n",
            "Epoch [1/10], Training Loss: 0.968, Validation Accuracy: 61.78%\n",
            "Epoch [2/10], Training Loss: 0.920, Validation Accuracy: 61.44%\n",
            "Epoch [3/10], Training Loss: 0.891, Validation Accuracy: 61.06%\n",
            "Epoch [4/10], Training Loss: 0.864, Validation Accuracy: 61.75%\n",
            "Epoch [5/10], Training Loss: 0.846, Validation Accuracy: 61.64%\n",
            "Epoch [6/10], Training Loss: 0.828, Validation Accuracy: 61.57%\n",
            "Epoch [7/10], Training Loss: 0.817, Validation Accuracy: 61.51%\n",
            "Epoch [8/10], Training Loss: 0.799, Validation Accuracy: 61.25%\n",
            "Epoch [9/10], Training Loss: 0.794, Validation Accuracy: 61.43%\n",
            "Epoch [10/10], Training Loss: 0.772, Validation Accuracy: 61.42%\n",
            "Epoch [1/10], Training Loss: 0.979, Validation Accuracy: 61.63%\n",
            "Epoch [2/10], Training Loss: 0.938, Validation Accuracy: 61.64%\n",
            "Epoch [3/10], Training Loss: 0.898, Validation Accuracy: 61.46%\n",
            "Epoch [4/10], Training Loss: 0.878, Validation Accuracy: 61.58%\n",
            "Epoch [5/10], Training Loss: 0.855, Validation Accuracy: 61.53%\n",
            "Epoch [6/10], Training Loss: 0.831, Validation Accuracy: 61.73%\n",
            "Epoch [7/10], Training Loss: 0.825, Validation Accuracy: 61.24%\n",
            "Epoch [8/10], Training Loss: 0.814, Validation Accuracy: 61.62%\n",
            "Epoch [9/10], Training Loss: 0.800, Validation Accuracy: 61.40%\n",
            "Epoch [10/10], Training Loss: 0.785, Validation Accuracy: 61.57%\n",
            "Epoch [1/10], Training Loss: 0.966, Validation Accuracy: 61.29%\n",
            "Epoch [2/10], Training Loss: 0.912, Validation Accuracy: 60.85%\n",
            "Epoch [3/10], Training Loss: 0.881, Validation Accuracy: 60.27%\n",
            "Epoch [4/10], Training Loss: 0.868, Validation Accuracy: 61.60%\n",
            "Epoch [5/10], Training Loss: 0.831, Validation Accuracy: 61.42%\n",
            "Epoch [6/10], Training Loss: 0.819, Validation Accuracy: 61.16%\n",
            "Epoch [7/10], Training Loss: 0.801, Validation Accuracy: 61.77%\n",
            "Epoch [8/10], Training Loss: 0.772, Validation Accuracy: 61.31%\n",
            "Epoch [9/10], Training Loss: 0.767, Validation Accuracy: 61.00%\n",
            "Epoch [10/10], Training Loss: 0.750, Validation Accuracy: 60.95%\n",
            "Epoch [1/10], Training Loss: 0.925, Validation Accuracy: 61.06%\n",
            "Epoch [2/10], Training Loss: 0.874, Validation Accuracy: 61.92%\n",
            "Epoch [3/10], Training Loss: 0.845, Validation Accuracy: 61.98%\n",
            "Epoch [4/10], Training Loss: 0.812, Validation Accuracy: 61.33%\n",
            "Epoch [5/10], Training Loss: 0.792, Validation Accuracy: 61.62%\n",
            "Epoch [6/10], Training Loss: 0.769, Validation Accuracy: 62.38%\n",
            "Epoch [7/10], Training Loss: 0.753, Validation Accuracy: 62.42%\n",
            "Epoch [8/10], Training Loss: 0.742, Validation Accuracy: 61.87%\n",
            "Epoch [9/10], Training Loss: 0.726, Validation Accuracy: 61.70%\n",
            "Epoch [10/10], Training Loss: 0.714, Validation Accuracy: 61.95%\n",
            "Epoch [1/10], Training Loss: 0.942, Validation Accuracy: 62.01%\n",
            "Epoch [2/10], Training Loss: 0.873, Validation Accuracy: 62.26%\n",
            "Epoch [3/10], Training Loss: 0.838, Validation Accuracy: 61.73%\n",
            "Epoch [4/10], Training Loss: 0.815, Validation Accuracy: 62.71%\n",
            "Epoch [5/10], Training Loss: 0.791, Validation Accuracy: 62.33%\n",
            "Epoch [6/10], Training Loss: 0.761, Validation Accuracy: 62.12%\n",
            "Epoch [7/10], Training Loss: 0.750, Validation Accuracy: 62.14%\n",
            "Epoch [8/10], Training Loss: 0.739, Validation Accuracy: 61.92%\n",
            "Epoch [9/10], Training Loss: 0.725, Validation Accuracy: 62.41%\n",
            "Epoch [10/10], Training Loss: 0.708, Validation Accuracy: 62.26%\n",
            "Epoch [1/10], Training Loss: 0.906, Validation Accuracy: 62.30%\n",
            "Epoch [2/10], Training Loss: 0.844, Validation Accuracy: 62.08%\n",
            "Epoch [3/10], Training Loss: 0.814, Validation Accuracy: 62.41%\n",
            "Epoch [4/10], Training Loss: 0.781, Validation Accuracy: 62.05%\n",
            "Epoch [5/10], Training Loss: 0.770, Validation Accuracy: 62.37%\n",
            "Epoch [6/10], Training Loss: 0.735, Validation Accuracy: 62.05%\n",
            "Epoch [7/10], Training Loss: 0.725, Validation Accuracy: 62.27%\n",
            "Epoch [8/10], Training Loss: 0.708, Validation Accuracy: 61.95%\n",
            "Epoch [9/10], Training Loss: 0.688, Validation Accuracy: 62.26%\n",
            "Epoch [10/10], Training Loss: 0.671, Validation Accuracy: 62.19%\n",
            "Epoch [1/10], Training Loss: 0.928, Validation Accuracy: 61.79%\n",
            "Epoch [2/10], Training Loss: 0.865, Validation Accuracy: 61.64%\n",
            "Epoch [3/10], Training Loss: 0.831, Validation Accuracy: 62.52%\n",
            "Epoch [4/10], Training Loss: 0.797, Validation Accuracy: 62.52%\n",
            "Epoch [5/10], Training Loss: 0.776, Validation Accuracy: 62.19%\n",
            "Epoch [6/10], Training Loss: 0.753, Validation Accuracy: 62.26%\n",
            "Epoch [7/10], Training Loss: 0.740, Validation Accuracy: 61.59%\n",
            "Epoch [8/10], Training Loss: 0.720, Validation Accuracy: 62.09%\n",
            "Epoch [9/10], Training Loss: 0.714, Validation Accuracy: 62.02%\n",
            "Epoch [10/10], Training Loss: 0.691, Validation Accuracy: 62.00%\n",
            "Epoch [1/10], Training Loss: 0.905, Validation Accuracy: 61.82%\n",
            "Epoch [2/10], Training Loss: 0.841, Validation Accuracy: 61.99%\n",
            "Epoch [3/10], Training Loss: 0.806, Validation Accuracy: 61.75%\n",
            "Epoch [4/10], Training Loss: 0.788, Validation Accuracy: 61.84%\n",
            "Epoch [5/10], Training Loss: 0.756, Validation Accuracy: 61.69%\n",
            "Epoch [6/10], Training Loss: 0.729, Validation Accuracy: 61.53%\n",
            "Epoch [7/10], Training Loss: 0.702, Validation Accuracy: 61.76%\n",
            "Epoch [8/10], Training Loss: 0.686, Validation Accuracy: 61.79%\n",
            "Epoch [9/10], Training Loss: 0.668, Validation Accuracy: 61.73%\n",
            "Epoch [10/10], Training Loss: 0.659, Validation Accuracy: 61.39%\n",
            "Epoch [1/10], Training Loss: 0.886, Validation Accuracy: 60.97%\n",
            "Epoch [2/10], Training Loss: 0.817, Validation Accuracy: 60.79%\n",
            "Epoch [3/10], Training Loss: 0.779, Validation Accuracy: 62.74%\n",
            "Epoch [4/10], Training Loss: 0.735, Validation Accuracy: 62.66%\n",
            "Epoch [5/10], Training Loss: 0.709, Validation Accuracy: 62.24%\n",
            "Epoch [6/10], Training Loss: 0.690, Validation Accuracy: 62.38%\n",
            "Epoch [7/10], Training Loss: 0.680, Validation Accuracy: 62.20%\n",
            "Epoch [8/10], Training Loss: 0.651, Validation Accuracy: 61.96%\n",
            "Epoch [9/10], Training Loss: 0.643, Validation Accuracy: 62.33%\n",
            "Epoch [10/10], Training Loss: 0.628, Validation Accuracy: 61.91%\n",
            "Epoch [1/10], Training Loss: 0.883, Validation Accuracy: 62.04%\n",
            "Epoch [2/10], Training Loss: 0.807, Validation Accuracy: 62.47%\n",
            "Epoch [3/10], Training Loss: 0.764, Validation Accuracy: 62.75%\n",
            "Epoch [4/10], Training Loss: 0.732, Validation Accuracy: 62.28%\n",
            "Epoch [5/10], Training Loss: 0.713, Validation Accuracy: 62.58%\n",
            "Epoch [6/10], Training Loss: 0.692, Validation Accuracy: 62.63%\n",
            "Epoch [7/10], Training Loss: 0.666, Validation Accuracy: 62.56%\n",
            "Epoch [8/10], Training Loss: 0.655, Validation Accuracy: 61.83%\n",
            "Epoch [9/10], Training Loss: 0.636, Validation Accuracy: 62.60%\n",
            "Epoch [10/10], Training Loss: 0.613, Validation Accuracy: 62.57%\n",
            "Epoch [1/10], Training Loss: 0.850, Validation Accuracy: 62.71%\n",
            "Epoch [2/10], Training Loss: 0.780, Validation Accuracy: 62.83%\n",
            "Epoch [3/10], Training Loss: 0.733, Validation Accuracy: 62.49%\n",
            "Epoch [4/10], Training Loss: 0.711, Validation Accuracy: 62.56%\n",
            "Epoch [5/10], Training Loss: 0.680, Validation Accuracy: 63.01%\n",
            "Epoch [6/10], Training Loss: 0.656, Validation Accuracy: 61.80%\n",
            "Epoch [7/10], Training Loss: 0.640, Validation Accuracy: 62.28%\n",
            "Epoch [8/10], Training Loss: 0.612, Validation Accuracy: 61.78%\n",
            "Epoch [9/10], Training Loss: 0.601, Validation Accuracy: 61.83%\n",
            "Epoch [10/10], Training Loss: 0.587, Validation Accuracy: 62.65%\n",
            "Epoch [1/10], Training Loss: 0.878, Validation Accuracy: 61.30%\n",
            "Epoch [2/10], Training Loss: 0.806, Validation Accuracy: 62.54%\n",
            "Epoch [3/10], Training Loss: 0.762, Validation Accuracy: 62.49%\n",
            "Epoch [4/10], Training Loss: 0.736, Validation Accuracy: 62.72%\n",
            "Epoch [5/10], Training Loss: 0.704, Validation Accuracy: 62.55%\n",
            "Epoch [6/10], Training Loss: 0.671, Validation Accuracy: 62.61%\n",
            "Epoch [7/10], Training Loss: 0.652, Validation Accuracy: 62.38%\n",
            "Epoch [8/10], Training Loss: 0.639, Validation Accuracy: 62.89%\n",
            "Epoch [9/10], Training Loss: 0.625, Validation Accuracy: 62.30%\n",
            "Epoch [10/10], Training Loss: 0.603, Validation Accuracy: 62.32%\n",
            "Epoch [1/10], Training Loss: 0.861, Validation Accuracy: 62.01%\n",
            "Epoch [2/10], Training Loss: 0.780, Validation Accuracy: 62.04%\n",
            "Epoch [3/10], Training Loss: 0.729, Validation Accuracy: 61.99%\n",
            "Epoch [4/10], Training Loss: 0.689, Validation Accuracy: 62.28%\n",
            "Epoch [5/10], Training Loss: 0.664, Validation Accuracy: 62.01%\n",
            "Epoch [6/10], Training Loss: 0.639, Validation Accuracy: 62.20%\n",
            "Epoch [7/10], Training Loss: 0.630, Validation Accuracy: 61.96%\n",
            "Epoch [8/10], Training Loss: 0.595, Validation Accuracy: 62.22%\n",
            "Epoch [9/10], Training Loss: 0.580, Validation Accuracy: 62.17%\n",
            "Epoch [10/10], Training Loss: 0.568, Validation Accuracy: 61.53%\n",
            "Epoch [1/10], Training Loss: 0.851, Validation Accuracy: 61.19%\n",
            "Epoch [2/10], Training Loss: 0.760, Validation Accuracy: 62.39%\n",
            "Epoch [3/10], Training Loss: 0.701, Validation Accuracy: 62.91%\n",
            "Epoch [4/10], Training Loss: 0.667, Validation Accuracy: 62.46%\n",
            "Epoch [5/10], Training Loss: 0.636, Validation Accuracy: 61.20%\n",
            "Epoch [6/10], Training Loss: 0.614, Validation Accuracy: 61.88%\n",
            "Epoch [7/10], Training Loss: 0.587, Validation Accuracy: 62.76%\n",
            "Epoch [8/10], Training Loss: 0.566, Validation Accuracy: 62.14%\n",
            "Epoch [9/10], Training Loss: 0.557, Validation Accuracy: 62.24%\n",
            "Epoch [10/10], Training Loss: 0.531, Validation Accuracy: 61.98%\n",
            "Epoch [1/10], Training Loss: 0.852, Validation Accuracy: 61.97%\n",
            "Epoch [2/10], Training Loss: 0.747, Validation Accuracy: 62.37%\n",
            "Epoch [3/10], Training Loss: 0.699, Validation Accuracy: 62.15%\n",
            "Epoch [4/10], Training Loss: 0.664, Validation Accuracy: 63.23%\n",
            "Epoch [5/10], Training Loss: 0.638, Validation Accuracy: 62.64%\n",
            "Epoch [6/10], Training Loss: 0.608, Validation Accuracy: 62.59%\n",
            "Epoch [7/10], Training Loss: 0.592, Validation Accuracy: 62.41%\n",
            "Epoch [8/10], Training Loss: 0.564, Validation Accuracy: 62.35%\n",
            "Epoch [9/10], Training Loss: 0.547, Validation Accuracy: 61.55%\n",
            "Epoch [10/10], Training Loss: 0.523, Validation Accuracy: 62.09%\n",
            "Epoch [1/10], Training Loss: 0.810, Validation Accuracy: 62.39%\n",
            "Epoch [2/10], Training Loss: 0.718, Validation Accuracy: 62.26%\n",
            "Epoch [3/10], Training Loss: 0.669, Validation Accuracy: 61.72%\n",
            "Epoch [4/10], Training Loss: 0.635, Validation Accuracy: 62.57%\n",
            "Epoch [5/10], Training Loss: 0.605, Validation Accuracy: 62.24%\n",
            "Epoch [6/10], Training Loss: 0.581, Validation Accuracy: 62.37%\n",
            "Epoch [7/10], Training Loss: 0.555, Validation Accuracy: 61.51%\n",
            "Epoch [8/10], Training Loss: 0.533, Validation Accuracy: 62.03%\n",
            "Epoch [9/10], Training Loss: 0.509, Validation Accuracy: 62.37%\n",
            "Epoch [10/10], Training Loss: 0.502, Validation Accuracy: 62.19%\n",
            "Epoch [1/10], Training Loss: 0.845, Validation Accuracy: 61.62%\n",
            "Epoch [2/10], Training Loss: 0.745, Validation Accuracy: 62.68%\n",
            "Epoch [3/10], Training Loss: 0.688, Validation Accuracy: 62.79%\n",
            "Epoch [4/10], Training Loss: 0.649, Validation Accuracy: 62.41%\n",
            "Epoch [5/10], Training Loss: 0.626, Validation Accuracy: 62.71%\n",
            "Epoch [6/10], Training Loss: 0.594, Validation Accuracy: 62.47%\n",
            "Epoch [7/10], Training Loss: 0.579, Validation Accuracy: 62.10%\n",
            "Epoch [8/10], Training Loss: 0.555, Validation Accuracy: 62.07%\n",
            "Epoch [9/10], Training Loss: 0.537, Validation Accuracy: 62.03%\n",
            "Epoch [10/10], Training Loss: 0.522, Validation Accuracy: 62.03%\n",
            "Epoch [1/10], Training Loss: 0.818, Validation Accuracy: 61.75%\n",
            "Epoch [2/10], Training Loss: 0.707, Validation Accuracy: 61.57%\n",
            "Epoch [3/10], Training Loss: 0.655, Validation Accuracy: 61.51%\n",
            "Epoch [4/10], Training Loss: 0.613, Validation Accuracy: 62.03%\n",
            "Epoch [5/10], Training Loss: 0.587, Validation Accuracy: 61.37%\n",
            "Epoch [6/10], Training Loss: 0.554, Validation Accuracy: 61.75%\n",
            "Epoch [7/10], Training Loss: 0.536, Validation Accuracy: 61.63%\n",
            "Epoch [8/10], Training Loss: 0.512, Validation Accuracy: 61.29%\n",
            "Epoch [9/10], Training Loss: 0.500, Validation Accuracy: 61.56%\n",
            "Epoch [10/10], Training Loss: 0.482, Validation Accuracy: 61.26%\n",
            "Epoch [1/10], Training Loss: 0.815, Validation Accuracy: 60.75%\n",
            "Epoch [2/10], Training Loss: 0.699, Validation Accuracy: 62.14%\n",
            "Epoch [3/10], Training Loss: 0.648, Validation Accuracy: 61.22%\n",
            "Epoch [4/10], Training Loss: 0.604, Validation Accuracy: 62.17%\n",
            "Epoch [5/10], Training Loss: 0.567, Validation Accuracy: 62.27%\n",
            "Epoch [6/10], Training Loss: 0.538, Validation Accuracy: 61.80%\n",
            "Epoch [7/10], Training Loss: 0.523, Validation Accuracy: 61.35%\n",
            "Epoch [8/10], Training Loss: 0.498, Validation Accuracy: 61.91%\n",
            "Epoch [9/10], Training Loss: 0.477, Validation Accuracy: 61.52%\n",
            "Epoch [10/10], Training Loss: 0.458, Validation Accuracy: 61.81%\n",
            "Epoch [1/10], Training Loss: 0.807, Validation Accuracy: 62.30%\n",
            "Epoch [2/10], Training Loss: 0.701, Validation Accuracy: 61.89%\n",
            "Epoch [3/10], Training Loss: 0.636, Validation Accuracy: 61.92%\n",
            "Epoch [4/10], Training Loss: 0.602, Validation Accuracy: 62.56%\n",
            "Epoch [5/10], Training Loss: 0.570, Validation Accuracy: 62.24%\n",
            "Epoch [6/10], Training Loss: 0.537, Validation Accuracy: 61.79%\n",
            "Epoch [7/10], Training Loss: 0.515, Validation Accuracy: 62.15%\n",
            "Epoch [8/10], Training Loss: 0.487, Validation Accuracy: 61.78%\n",
            "Epoch [9/10], Training Loss: 0.470, Validation Accuracy: 61.73%\n",
            "Epoch [10/10], Training Loss: 0.456, Validation Accuracy: 62.23%\n",
            "Confusion Matrix:\n",
            "[[659  43  41  19  30   7  18  23 116  44]\n",
            " [ 22 794  13  13   8   7  17   8  41  77]\n",
            " [ 67  12 479  73 120  83  79  54  21  12]\n",
            " [ 25  17  88 409  76 194  76  64  24  27]\n",
            " [ 33  14  97  61 545  58  64 104  18   6]\n",
            " [  6  10  51 166  78 515  40 106  12  16]\n",
            " [  6  13  49  67  46  36 749  13  11  10]\n",
            " [ 13  13  34  35  68  87  12 718   4  16]\n",
            " [ 61  69  18  15  14   8   7   9 769  30]\n",
            " [ 37 174  20  16  14  15  16  38  50 620]]\n",
            "Test Accuracy: 62.57%\n",
            "True Positives (TP): [659 794 479 409 545 515 749 718 769 620]\n",
            "False Positives (FP): [270 365 411 465 454 495 329 419 297 238]\n",
            "True Negatives (TN): [8730 8635 8589 8535 8546 8505 8671 8581 8703 8762]\n",
            "False Negatives (FN): [341 206 521 591 455 485 251 282 231 380]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.70936491 0.68507334 0.53820225 0.46796339 0.54554555 0.50990099\n",
            " 0.69480519 0.63148637 0.72138837 0.72261072]\n",
            "Recall: [0.659 0.794 0.479 0.409 0.545 0.515 0.749 0.718 0.769 0.62 ]\n",
            "F1 Score: [0.68325557 0.73552571 0.50687831 0.43649947 0.54527264 0.51243781\n",
            " 0.72088547 0.67197005 0.74443369 0.66738428]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 1):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=1):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE,  distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average =  augmented_data_truncated\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=1)\n",
        "\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beta=3"
      ],
      "metadata": {
        "id": "JtHkBjC7y2xN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DtPbY-gpy41x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f57adfe-a7c8-46f2-96a6-91dca5338b7a",
        "id": "h7E1-0p3y5Zg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:10<00:00, 16.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 3):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=3):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE,  distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average =  augmented_data_truncated\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=3)\n",
        "\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    }
  ]
}