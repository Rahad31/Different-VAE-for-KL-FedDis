{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahad31/Different-VAE-for-KL-FedDis/blob/main/Beta_Ord_Trunc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpJbi0RbDhy5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QXsfWnMDwza"
      },
      "source": [
        "Beta=.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Noujn05rDvDQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMFSzAJYkag9",
        "outputId": "f6bac336-d04c-49b3-d21f-8625cd3447e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 48.1MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Images per Class: [5996 5958 5954 5965 6011 6034 6086 5912 5996 6088]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1-1355293134.py:281: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Training Loss: 2.304, Validation Accuracy: 9.90%\n",
            "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 10.30%\n",
            "Epoch [3/10], Training Loss: 2.302, Validation Accuracy: 10.51%\n",
            "Epoch [4/10], Training Loss: 2.302, Validation Accuracy: 10.93%\n",
            "Epoch [5/10], Training Loss: 2.301, Validation Accuracy: 11.54%\n",
            "Epoch [6/10], Training Loss: 2.300, Validation Accuracy: 11.20%\n",
            "Epoch [7/10], Training Loss: 2.300, Validation Accuracy: 11.46%\n",
            "Epoch [8/10], Training Loss: 2.299, Validation Accuracy: 11.72%\n",
            "Epoch [9/10], Training Loss: 2.298, Validation Accuracy: 12.03%\n",
            "Epoch [10/10], Training Loss: 2.297, Validation Accuracy: 12.85%\n",
            "Epoch [1/10], Training Loss: 2.297, Validation Accuracy: 13.43%\n",
            "Epoch [2/10], Training Loss: 2.295, Validation Accuracy: 14.59%\n",
            "Epoch [3/10], Training Loss: 2.293, Validation Accuracy: 15.73%\n",
            "Epoch [4/10], Training Loss: 2.289, Validation Accuracy: 16.18%\n",
            "Epoch [5/10], Training Loss: 2.285, Validation Accuracy: 16.20%\n",
            "Epoch [6/10], Training Loss: 2.278, Validation Accuracy: 18.79%\n",
            "Epoch [7/10], Training Loss: 2.267, Validation Accuracy: 17.16%\n",
            "Epoch [8/10], Training Loss: 2.251, Validation Accuracy: 15.96%\n",
            "Epoch [9/10], Training Loss: 2.232, Validation Accuracy: 15.65%\n",
            "Epoch [10/10], Training Loss: 2.211, Validation Accuracy: 16.20%\n",
            "Epoch [1/10], Training Loss: 2.189, Validation Accuracy: 16.61%\n",
            "Epoch [2/10], Training Loss: 2.168, Validation Accuracy: 17.47%\n",
            "Epoch [3/10], Training Loss: 2.151, Validation Accuracy: 18.33%\n",
            "Epoch [4/10], Training Loss: 2.136, Validation Accuracy: 19.80%\n",
            "Epoch [5/10], Training Loss: 2.116, Validation Accuracy: 20.81%\n",
            "Epoch [6/10], Training Loss: 2.094, Validation Accuracy: 22.97%\n",
            "Epoch [7/10], Training Loss: 2.071, Validation Accuracy: 23.64%\n",
            "Epoch [8/10], Training Loss: 2.044, Validation Accuracy: 24.88%\n",
            "Epoch [9/10], Training Loss: 2.019, Validation Accuracy: 26.26%\n",
            "Epoch [10/10], Training Loss: 1.994, Validation Accuracy: 27.36%\n",
            "Epoch [1/10], Training Loss: 1.985, Validation Accuracy: 27.68%\n",
            "Epoch [2/10], Training Loss: 1.962, Validation Accuracy: 28.72%\n",
            "Epoch [3/10], Training Loss: 1.936, Validation Accuracy: 28.52%\n",
            "Epoch [4/10], Training Loss: 1.910, Validation Accuracy: 28.93%\n",
            "Epoch [5/10], Training Loss: 1.888, Validation Accuracy: 31.12%\n",
            "Epoch [6/10], Training Loss: 1.865, Validation Accuracy: 31.50%\n",
            "Epoch [7/10], Training Loss: 1.841, Validation Accuracy: 31.80%\n",
            "Epoch [8/10], Training Loss: 1.818, Validation Accuracy: 32.72%\n",
            "Epoch [9/10], Training Loss: 1.801, Validation Accuracy: 33.18%\n",
            "Epoch [10/10], Training Loss: 1.782, Validation Accuracy: 33.15%\n",
            "Epoch [1/10], Training Loss: 1.779, Validation Accuracy: 34.29%\n",
            "Epoch [2/10], Training Loss: 1.759, Validation Accuracy: 35.03%\n",
            "Epoch [3/10], Training Loss: 1.743, Validation Accuracy: 36.85%\n",
            "Epoch [4/10], Training Loss: 1.730, Validation Accuracy: 36.16%\n",
            "Epoch [5/10], Training Loss: 1.716, Validation Accuracy: 37.79%\n",
            "Epoch [6/10], Training Loss: 1.707, Validation Accuracy: 37.61%\n",
            "Epoch [7/10], Training Loss: 1.692, Validation Accuracy: 38.42%\n",
            "Epoch [8/10], Training Loss: 1.677, Validation Accuracy: 38.40%\n",
            "Epoch [9/10], Training Loss: 1.666, Validation Accuracy: 38.92%\n",
            "Epoch [10/10], Training Loss: 1.654, Validation Accuracy: 38.96%\n",
            "Epoch [1/10], Training Loss: 1.667, Validation Accuracy: 39.43%\n",
            "Epoch [2/10], Training Loss: 1.649, Validation Accuracy: 39.79%\n",
            "Epoch [3/10], Training Loss: 1.641, Validation Accuracy: 40.68%\n",
            "Epoch [4/10], Training Loss: 1.635, Validation Accuracy: 39.51%\n",
            "Epoch [5/10], Training Loss: 1.625, Validation Accuracy: 41.20%\n",
            "Epoch [6/10], Training Loss: 1.612, Validation Accuracy: 40.60%\n",
            "Epoch [7/10], Training Loss: 1.603, Validation Accuracy: 41.84%\n",
            "Epoch [8/10], Training Loss: 1.590, Validation Accuracy: 41.92%\n",
            "Epoch [9/10], Training Loss: 1.583, Validation Accuracy: 42.02%\n",
            "Epoch [10/10], Training Loss: 1.574, Validation Accuracy: 42.30%\n",
            "Epoch [1/10], Training Loss: 1.567, Validation Accuracy: 41.81%\n",
            "Epoch [2/10], Training Loss: 1.552, Validation Accuracy: 42.40%\n",
            "Epoch [3/10], Training Loss: 1.535, Validation Accuracy: 42.42%\n",
            "Epoch [4/10], Training Loss: 1.535, Validation Accuracy: 42.40%\n",
            "Epoch [5/10], Training Loss: 1.527, Validation Accuracy: 43.09%\n",
            "Epoch [6/10], Training Loss: 1.513, Validation Accuracy: 43.31%\n",
            "Epoch [7/10], Training Loss: 1.505, Validation Accuracy: 43.75%\n",
            "Epoch [8/10], Training Loss: 1.493, Validation Accuracy: 44.65%\n",
            "Epoch [9/10], Training Loss: 1.485, Validation Accuracy: 44.77%\n",
            "Epoch [10/10], Training Loss: 1.478, Validation Accuracy: 44.21%\n",
            "Epoch [1/10], Training Loss: 1.499, Validation Accuracy: 44.29%\n",
            "Epoch [2/10], Training Loss: 1.489, Validation Accuracy: 45.13%\n",
            "Epoch [3/10], Training Loss: 1.475, Validation Accuracy: 45.54%\n",
            "Epoch [4/10], Training Loss: 1.462, Validation Accuracy: 45.29%\n",
            "Epoch [5/10], Training Loss: 1.452, Validation Accuracy: 46.37%\n",
            "Epoch [6/10], Training Loss: 1.439, Validation Accuracy: 46.57%\n",
            "Epoch [7/10], Training Loss: 1.435, Validation Accuracy: 46.62%\n",
            "Epoch [8/10], Training Loss: 1.423, Validation Accuracy: 46.57%\n",
            "Epoch [9/10], Training Loss: 1.417, Validation Accuracy: 46.85%\n",
            "Epoch [10/10], Training Loss: 1.406, Validation Accuracy: 47.09%\n",
            "Epoch [1/10], Training Loss: 1.452, Validation Accuracy: 47.17%\n",
            "Epoch [2/10], Training Loss: 1.433, Validation Accuracy: 47.63%\n",
            "Epoch [3/10], Training Loss: 1.428, Validation Accuracy: 47.41%\n",
            "Epoch [4/10], Training Loss: 1.420, Validation Accuracy: 47.54%\n",
            "Epoch [5/10], Training Loss: 1.407, Validation Accuracy: 48.34%\n",
            "Epoch [6/10], Training Loss: 1.399, Validation Accuracy: 48.24%\n",
            "Epoch [7/10], Training Loss: 1.393, Validation Accuracy: 46.97%\n",
            "Epoch [8/10], Training Loss: 1.387, Validation Accuracy: 47.71%\n",
            "Epoch [9/10], Training Loss: 1.384, Validation Accuracy: 48.72%\n",
            "Epoch [10/10], Training Loss: 1.378, Validation Accuracy: 48.84%\n",
            "Epoch [1/10], Training Loss: 1.420, Validation Accuracy: 48.71%\n",
            "Epoch [2/10], Training Loss: 1.412, Validation Accuracy: 48.66%\n",
            "Epoch [3/10], Training Loss: 1.396, Validation Accuracy: 49.16%\n",
            "Epoch [4/10], Training Loss: 1.388, Validation Accuracy: 49.57%\n",
            "Epoch [5/10], Training Loss: 1.383, Validation Accuracy: 49.48%\n",
            "Epoch [6/10], Training Loss: 1.375, Validation Accuracy: 49.36%\n",
            "Epoch [7/10], Training Loss: 1.367, Validation Accuracy: 49.14%\n",
            "Epoch [8/10], Training Loss: 1.369, Validation Accuracy: 49.19%\n",
            "Epoch [9/10], Training Loss: 1.351, Validation Accuracy: 49.68%\n",
            "Epoch [10/10], Training Loss: 1.342, Validation Accuracy: 49.91%\n",
            "Epoch [1/10], Training Loss: 1.398, Validation Accuracy: 50.42%\n",
            "Epoch [2/10], Training Loss: 1.384, Validation Accuracy: 50.28%\n",
            "Epoch [3/10], Training Loss: 1.374, Validation Accuracy: 50.35%\n",
            "Epoch [4/10], Training Loss: 1.365, Validation Accuracy: 50.67%\n",
            "Epoch [5/10], Training Loss: 1.352, Validation Accuracy: 50.36%\n",
            "Epoch [6/10], Training Loss: 1.342, Validation Accuracy: 50.56%\n",
            "Epoch [7/10], Training Loss: 1.337, Validation Accuracy: 50.66%\n",
            "Epoch [8/10], Training Loss: 1.327, Validation Accuracy: 51.08%\n",
            "Epoch [9/10], Training Loss: 1.321, Validation Accuracy: 51.20%\n",
            "Epoch [10/10], Training Loss: 1.314, Validation Accuracy: 51.75%\n",
            "Epoch [1/10], Training Loss: 1.343, Validation Accuracy: 51.62%\n",
            "Epoch [2/10], Training Loss: 1.327, Validation Accuracy: 50.25%\n",
            "Epoch [3/10], Training Loss: 1.318, Validation Accuracy: 51.70%\n",
            "Epoch [4/10], Training Loss: 1.309, Validation Accuracy: 51.34%\n",
            "Epoch [5/10], Training Loss: 1.298, Validation Accuracy: 51.47%\n",
            "Epoch [6/10], Training Loss: 1.292, Validation Accuracy: 51.39%\n",
            "Epoch [7/10], Training Loss: 1.283, Validation Accuracy: 51.81%\n",
            "Epoch [8/10], Training Loss: 1.271, Validation Accuracy: 51.43%\n",
            "Epoch [9/10], Training Loss: 1.261, Validation Accuracy: 51.93%\n",
            "Epoch [10/10], Training Loss: 1.260, Validation Accuracy: 52.23%\n",
            "Epoch [1/10], Training Loss: 1.316, Validation Accuracy: 51.80%\n",
            "Epoch [2/10], Training Loss: 1.291, Validation Accuracy: 52.28%\n",
            "Epoch [3/10], Training Loss: 1.275, Validation Accuracy: 52.57%\n",
            "Epoch [4/10], Training Loss: 1.276, Validation Accuracy: 52.66%\n",
            "Epoch [5/10], Training Loss: 1.259, Validation Accuracy: 52.65%\n",
            "Epoch [6/10], Training Loss: 1.254, Validation Accuracy: 52.07%\n",
            "Epoch [7/10], Training Loss: 1.242, Validation Accuracy: 53.39%\n",
            "Epoch [8/10], Training Loss: 1.234, Validation Accuracy: 52.99%\n",
            "Epoch [9/10], Training Loss: 1.225, Validation Accuracy: 52.52%\n",
            "Epoch [10/10], Training Loss: 1.222, Validation Accuracy: 52.51%\n",
            "Epoch [1/10], Training Loss: 1.304, Validation Accuracy: 53.45%\n",
            "Epoch [2/10], Training Loss: 1.275, Validation Accuracy: 52.97%\n",
            "Epoch [3/10], Training Loss: 1.257, Validation Accuracy: 53.38%\n",
            "Epoch [4/10], Training Loss: 1.248, Validation Accuracy: 53.34%\n",
            "Epoch [5/10], Training Loss: 1.245, Validation Accuracy: 53.81%\n",
            "Epoch [6/10], Training Loss: 1.239, Validation Accuracy: 53.01%\n",
            "Epoch [7/10], Training Loss: 1.224, Validation Accuracy: 53.54%\n",
            "Epoch [8/10], Training Loss: 1.216, Validation Accuracy: 54.20%\n",
            "Epoch [9/10], Training Loss: 1.201, Validation Accuracy: 54.26%\n",
            "Epoch [10/10], Training Loss: 1.194, Validation Accuracy: 54.27%\n",
            "Epoch [1/10], Training Loss: 1.285, Validation Accuracy: 54.02%\n",
            "Epoch [2/10], Training Loss: 1.257, Validation Accuracy: 54.19%\n",
            "Epoch [3/10], Training Loss: 1.239, Validation Accuracy: 54.25%\n",
            "Epoch [4/10], Training Loss: 1.238, Validation Accuracy: 53.72%\n",
            "Epoch [5/10], Training Loss: 1.222, Validation Accuracy: 54.60%\n",
            "Epoch [6/10], Training Loss: 1.217, Validation Accuracy: 54.48%\n",
            "Epoch [7/10], Training Loss: 1.204, Validation Accuracy: 53.80%\n",
            "Epoch [8/10], Training Loss: 1.203, Validation Accuracy: 54.80%\n",
            "Epoch [9/10], Training Loss: 1.188, Validation Accuracy: 54.70%\n",
            "Epoch [10/10], Training Loss: 1.181, Validation Accuracy: 54.89%\n",
            "Epoch [1/10], Training Loss: 1.260, Validation Accuracy: 55.05%\n",
            "Epoch [2/10], Training Loss: 1.246, Validation Accuracy: 55.07%\n",
            "Epoch [3/10], Training Loss: 1.225, Validation Accuracy: 54.70%\n",
            "Epoch [4/10], Training Loss: 1.213, Validation Accuracy: 55.25%\n",
            "Epoch [5/10], Training Loss: 1.202, Validation Accuracy: 55.97%\n",
            "Epoch [6/10], Training Loss: 1.196, Validation Accuracy: 55.46%\n",
            "Epoch [7/10], Training Loss: 1.186, Validation Accuracy: 55.93%\n",
            "Epoch [8/10], Training Loss: 1.174, Validation Accuracy: 56.15%\n",
            "Epoch [9/10], Training Loss: 1.170, Validation Accuracy: 55.50%\n",
            "Epoch [10/10], Training Loss: 1.158, Validation Accuracy: 55.51%\n",
            "Epoch [1/10], Training Loss: 1.226, Validation Accuracy: 55.54%\n",
            "Epoch [2/10], Training Loss: 1.198, Validation Accuracy: 55.50%\n",
            "Epoch [3/10], Training Loss: 1.177, Validation Accuracy: 56.08%\n",
            "Epoch [4/10], Training Loss: 1.167, Validation Accuracy: 55.35%\n",
            "Epoch [5/10], Training Loss: 1.160, Validation Accuracy: 55.02%\n",
            "Epoch [6/10], Training Loss: 1.147, Validation Accuracy: 55.34%\n",
            "Epoch [7/10], Training Loss: 1.139, Validation Accuracy: 56.51%\n",
            "Epoch [8/10], Training Loss: 1.133, Validation Accuracy: 55.96%\n",
            "Epoch [9/10], Training Loss: 1.125, Validation Accuracy: 55.70%\n",
            "Epoch [10/10], Training Loss: 1.129, Validation Accuracy: 55.72%\n",
            "Epoch [1/10], Training Loss: 1.193, Validation Accuracy: 56.46%\n",
            "Epoch [2/10], Training Loss: 1.164, Validation Accuracy: 56.50%\n",
            "Epoch [3/10], Training Loss: 1.157, Validation Accuracy: 56.00%\n",
            "Epoch [4/10], Training Loss: 1.132, Validation Accuracy: 56.37%\n",
            "Epoch [5/10], Training Loss: 1.120, Validation Accuracy: 56.88%\n",
            "Epoch [6/10], Training Loss: 1.112, Validation Accuracy: 56.36%\n",
            "Epoch [7/10], Training Loss: 1.093, Validation Accuracy: 56.92%\n",
            "Epoch [8/10], Training Loss: 1.088, Validation Accuracy: 56.02%\n",
            "Epoch [9/10], Training Loss: 1.078, Validation Accuracy: 56.27%\n",
            "Epoch [10/10], Training Loss: 1.072, Validation Accuracy: 55.44%\n",
            "Epoch [1/10], Training Loss: 1.185, Validation Accuracy: 56.41%\n",
            "Epoch [2/10], Training Loss: 1.154, Validation Accuracy: 57.26%\n",
            "Epoch [3/10], Training Loss: 1.135, Validation Accuracy: 57.37%\n",
            "Epoch [4/10], Training Loss: 1.121, Validation Accuracy: 57.41%\n",
            "Epoch [5/10], Training Loss: 1.106, Validation Accuracy: 57.04%\n",
            "Epoch [6/10], Training Loss: 1.107, Validation Accuracy: 57.18%\n",
            "Epoch [7/10], Training Loss: 1.097, Validation Accuracy: 57.20%\n",
            "Epoch [8/10], Training Loss: 1.094, Validation Accuracy: 57.23%\n",
            "Epoch [9/10], Training Loss: 1.074, Validation Accuracy: 57.48%\n",
            "Epoch [10/10], Training Loss: 1.068, Validation Accuracy: 57.21%\n",
            "Epoch [1/10], Training Loss: 1.172, Validation Accuracy: 57.41%\n",
            "Epoch [2/10], Training Loss: 1.144, Validation Accuracy: 57.64%\n",
            "Epoch [3/10], Training Loss: 1.131, Validation Accuracy: 57.47%\n",
            "Epoch [4/10], Training Loss: 1.104, Validation Accuracy: 57.40%\n",
            "Epoch [5/10], Training Loss: 1.102, Validation Accuracy: 58.35%\n",
            "Epoch [6/10], Training Loss: 1.087, Validation Accuracy: 57.27%\n",
            "Epoch [7/10], Training Loss: 1.074, Validation Accuracy: 57.50%\n",
            "Epoch [8/10], Training Loss: 1.070, Validation Accuracy: 57.99%\n",
            "Epoch [9/10], Training Loss: 1.062, Validation Accuracy: 57.33%\n",
            "Epoch [10/10], Training Loss: 1.050, Validation Accuracy: 57.27%\n",
            "Epoch [1/10], Training Loss: 1.159, Validation Accuracy: 57.77%\n",
            "Epoch [2/10], Training Loss: 1.135, Validation Accuracy: 58.01%\n",
            "Epoch [3/10], Training Loss: 1.116, Validation Accuracy: 57.13%\n",
            "Epoch [4/10], Training Loss: 1.097, Validation Accuracy: 57.78%\n",
            "Epoch [5/10], Training Loss: 1.087, Validation Accuracy: 58.51%\n",
            "Epoch [6/10], Training Loss: 1.080, Validation Accuracy: 57.41%\n",
            "Epoch [7/10], Training Loss: 1.064, Validation Accuracy: 58.39%\n",
            "Epoch [8/10], Training Loss: 1.054, Validation Accuracy: 58.10%\n",
            "Epoch [9/10], Training Loss: 1.038, Validation Accuracy: 58.16%\n",
            "Epoch [10/10], Training Loss: 1.029, Validation Accuracy: 57.82%\n",
            "Epoch [1/10], Training Loss: 1.134, Validation Accuracy: 58.19%\n",
            "Epoch [2/10], Training Loss: 1.096, Validation Accuracy: 58.98%\n",
            "Epoch [3/10], Training Loss: 1.076, Validation Accuracy: 58.56%\n",
            "Epoch [4/10], Training Loss: 1.065, Validation Accuracy: 58.46%\n",
            "Epoch [5/10], Training Loss: 1.047, Validation Accuracy: 58.91%\n",
            "Epoch [6/10], Training Loss: 1.034, Validation Accuracy: 58.64%\n",
            "Epoch [7/10], Training Loss: 1.022, Validation Accuracy: 58.51%\n",
            "Epoch [8/10], Training Loss: 1.020, Validation Accuracy: 58.25%\n",
            "Epoch [9/10], Training Loss: 1.001, Validation Accuracy: 58.29%\n",
            "Epoch [10/10], Training Loss: 0.993, Validation Accuracy: 58.65%\n",
            "Epoch [1/10], Training Loss: 1.101, Validation Accuracy: 58.50%\n",
            "Epoch [2/10], Training Loss: 1.057, Validation Accuracy: 59.02%\n",
            "Epoch [3/10], Training Loss: 1.040, Validation Accuracy: 59.29%\n",
            "Epoch [4/10], Training Loss: 1.027, Validation Accuracy: 58.51%\n",
            "Epoch [5/10], Training Loss: 1.015, Validation Accuracy: 59.17%\n",
            "Epoch [6/10], Training Loss: 0.998, Validation Accuracy: 58.52%\n",
            "Epoch [7/10], Training Loss: 0.990, Validation Accuracy: 57.76%\n",
            "Epoch [8/10], Training Loss: 0.986, Validation Accuracy: 58.63%\n",
            "Epoch [9/10], Training Loss: 0.964, Validation Accuracy: 59.29%\n",
            "Epoch [10/10], Training Loss: 0.944, Validation Accuracy: 58.62%\n",
            "Epoch [1/10], Training Loss: 1.100, Validation Accuracy: 58.97%\n",
            "Epoch [2/10], Training Loss: 1.072, Validation Accuracy: 58.80%\n",
            "Epoch [3/10], Training Loss: 1.052, Validation Accuracy: 59.24%\n",
            "Epoch [4/10], Training Loss: 1.019, Validation Accuracy: 59.49%\n",
            "Epoch [5/10], Training Loss: 1.011, Validation Accuracy: 58.87%\n",
            "Epoch [6/10], Training Loss: 0.994, Validation Accuracy: 59.12%\n",
            "Epoch [7/10], Training Loss: 0.980, Validation Accuracy: 58.89%\n",
            "Epoch [8/10], Training Loss: 0.982, Validation Accuracy: 59.16%\n",
            "Epoch [9/10], Training Loss: 0.964, Validation Accuracy: 59.39%\n",
            "Epoch [10/10], Training Loss: 0.946, Validation Accuracy: 58.92%\n",
            "Epoch [1/10], Training Loss: 1.100, Validation Accuracy: 59.66%\n",
            "Epoch [2/10], Training Loss: 1.060, Validation Accuracy: 59.83%\n",
            "Epoch [3/10], Training Loss: 1.039, Validation Accuracy: 59.21%\n",
            "Epoch [4/10], Training Loss: 1.011, Validation Accuracy: 59.84%\n",
            "Epoch [5/10], Training Loss: 1.004, Validation Accuracy: 59.33%\n",
            "Epoch [6/10], Training Loss: 0.993, Validation Accuracy: 59.61%\n",
            "Epoch [7/10], Training Loss: 0.973, Validation Accuracy: 58.96%\n",
            "Epoch [8/10], Training Loss: 0.960, Validation Accuracy: 59.82%\n",
            "Epoch [9/10], Training Loss: 0.947, Validation Accuracy: 59.80%\n",
            "Epoch [10/10], Training Loss: 0.940, Validation Accuracy: 59.58%\n",
            "Epoch [1/10], Training Loss: 1.088, Validation Accuracy: 60.04%\n",
            "Epoch [2/10], Training Loss: 1.047, Validation Accuracy: 59.69%\n",
            "Epoch [3/10], Training Loss: 1.023, Validation Accuracy: 60.31%\n",
            "Epoch [4/10], Training Loss: 1.005, Validation Accuracy: 59.67%\n",
            "Epoch [5/10], Training Loss: 0.995, Validation Accuracy: 60.07%\n",
            "Epoch [6/10], Training Loss: 0.971, Validation Accuracy: 59.74%\n",
            "Epoch [7/10], Training Loss: 0.957, Validation Accuracy: 60.50%\n",
            "Epoch [8/10], Training Loss: 0.946, Validation Accuracy: 60.21%\n",
            "Epoch [9/10], Training Loss: 0.934, Validation Accuracy: 60.03%\n",
            "Epoch [10/10], Training Loss: 0.917, Validation Accuracy: 60.13%\n",
            "Epoch [1/10], Training Loss: 1.056, Validation Accuracy: 60.09%\n",
            "Epoch [2/10], Training Loss: 1.009, Validation Accuracy: 59.58%\n",
            "Epoch [3/10], Training Loss: 0.996, Validation Accuracy: 60.31%\n",
            "Epoch [4/10], Training Loss: 0.973, Validation Accuracy: 59.91%\n",
            "Epoch [5/10], Training Loss: 0.953, Validation Accuracy: 60.12%\n",
            "Epoch [6/10], Training Loss: 0.936, Validation Accuracy: 59.62%\n",
            "Epoch [7/10], Training Loss: 0.930, Validation Accuracy: 60.27%\n",
            "Epoch [8/10], Training Loss: 0.913, Validation Accuracy: 59.72%\n",
            "Epoch [9/10], Training Loss: 0.904, Validation Accuracy: 59.44%\n",
            "Epoch [10/10], Training Loss: 0.893, Validation Accuracy: 60.14%\n",
            "Epoch [1/10], Training Loss: 1.033, Validation Accuracy: 60.24%\n",
            "Epoch [2/10], Training Loss: 0.985, Validation Accuracy: 59.38%\n",
            "Epoch [3/10], Training Loss: 0.962, Validation Accuracy: 60.00%\n",
            "Epoch [4/10], Training Loss: 0.936, Validation Accuracy: 59.91%\n",
            "Epoch [5/10], Training Loss: 0.921, Validation Accuracy: 59.82%\n",
            "Epoch [6/10], Training Loss: 0.911, Validation Accuracy: 60.35%\n",
            "Epoch [7/10], Training Loss: 0.888, Validation Accuracy: 60.07%\n",
            "Epoch [8/10], Training Loss: 0.870, Validation Accuracy: 59.38%\n",
            "Epoch [9/10], Training Loss: 0.867, Validation Accuracy: 60.08%\n",
            "Epoch [10/10], Training Loss: 0.847, Validation Accuracy: 59.31%\n",
            "Epoch [1/10], Training Loss: 1.026, Validation Accuracy: 60.63%\n",
            "Epoch [2/10], Training Loss: 0.991, Validation Accuracy: 59.94%\n",
            "Epoch [3/10], Training Loss: 0.966, Validation Accuracy: 59.65%\n",
            "Epoch [4/10], Training Loss: 0.937, Validation Accuracy: 60.91%\n",
            "Epoch [5/10], Training Loss: 0.918, Validation Accuracy: 60.53%\n",
            "Epoch [6/10], Training Loss: 0.902, Validation Accuracy: 59.89%\n",
            "Epoch [7/10], Training Loss: 0.895, Validation Accuracy: 60.24%\n",
            "Epoch [8/10], Training Loss: 0.887, Validation Accuracy: 60.45%\n",
            "Epoch [9/10], Training Loss: 0.863, Validation Accuracy: 60.68%\n",
            "Epoch [10/10], Training Loss: 0.851, Validation Accuracy: 60.48%\n",
            "Epoch [1/10], Training Loss: 1.030, Validation Accuracy: 60.15%\n",
            "Epoch [2/10], Training Loss: 0.987, Validation Accuracy: 60.07%\n",
            "Epoch [3/10], Training Loss: 0.960, Validation Accuracy: 60.95%\n",
            "Epoch [4/10], Training Loss: 0.943, Validation Accuracy: 60.71%\n",
            "Epoch [5/10], Training Loss: 0.919, Validation Accuracy: 61.24%\n",
            "Epoch [6/10], Training Loss: 0.902, Validation Accuracy: 61.25%\n",
            "Epoch [7/10], Training Loss: 0.879, Validation Accuracy: 61.01%\n",
            "Epoch [8/10], Training Loss: 0.870, Validation Accuracy: 60.74%\n",
            "Epoch [9/10], Training Loss: 0.859, Validation Accuracy: 60.51%\n",
            "Epoch [10/10], Training Loss: 0.845, Validation Accuracy: 60.74%\n",
            "Epoch [1/10], Training Loss: 1.030, Validation Accuracy: 60.57%\n",
            "Epoch [2/10], Training Loss: 0.983, Validation Accuracy: 61.10%\n",
            "Epoch [3/10], Training Loss: 0.942, Validation Accuracy: 61.50%\n",
            "Epoch [4/10], Training Loss: 0.919, Validation Accuracy: 61.31%\n",
            "Epoch [5/10], Training Loss: 0.901, Validation Accuracy: 61.00%\n",
            "Epoch [6/10], Training Loss: 0.883, Validation Accuracy: 60.20%\n",
            "Epoch [7/10], Training Loss: 0.873, Validation Accuracy: 60.78%\n",
            "Epoch [8/10], Training Loss: 0.847, Validation Accuracy: 60.61%\n",
            "Epoch [9/10], Training Loss: 0.850, Validation Accuracy: 61.04%\n",
            "Epoch [10/10], Training Loss: 0.819, Validation Accuracy: 61.09%\n",
            "Epoch [1/10], Training Loss: 0.999, Validation Accuracy: 60.97%\n",
            "Epoch [2/10], Training Loss: 0.947, Validation Accuracy: 59.66%\n",
            "Epoch [3/10], Training Loss: 0.913, Validation Accuracy: 60.90%\n",
            "Epoch [4/10], Training Loss: 0.882, Validation Accuracy: 60.78%\n",
            "Epoch [5/10], Training Loss: 0.872, Validation Accuracy: 59.60%\n",
            "Epoch [6/10], Training Loss: 0.860, Validation Accuracy: 61.35%\n",
            "Epoch [7/10], Training Loss: 0.833, Validation Accuracy: 61.03%\n",
            "Epoch [8/10], Training Loss: 0.825, Validation Accuracy: 60.63%\n",
            "Epoch [9/10], Training Loss: 0.815, Validation Accuracy: 60.39%\n",
            "Epoch [10/10], Training Loss: 0.791, Validation Accuracy: 61.02%\n",
            "Epoch [1/10], Training Loss: 0.973, Validation Accuracy: 60.55%\n",
            "Epoch [2/10], Training Loss: 0.918, Validation Accuracy: 61.16%\n",
            "Epoch [3/10], Training Loss: 0.878, Validation Accuracy: 60.97%\n",
            "Epoch [4/10], Training Loss: 0.866, Validation Accuracy: 60.51%\n",
            "Epoch [5/10], Training Loss: 0.840, Validation Accuracy: 60.68%\n",
            "Epoch [6/10], Training Loss: 0.818, Validation Accuracy: 60.78%\n",
            "Epoch [7/10], Training Loss: 0.805, Validation Accuracy: 60.71%\n",
            "Epoch [8/10], Training Loss: 0.778, Validation Accuracy: 60.38%\n",
            "Epoch [9/10], Training Loss: 0.769, Validation Accuracy: 60.76%\n",
            "Epoch [10/10], Training Loss: 0.751, Validation Accuracy: 60.47%\n",
            "Epoch [1/10], Training Loss: 0.981, Validation Accuracy: 59.76%\n",
            "Epoch [2/10], Training Loss: 0.923, Validation Accuracy: 60.53%\n",
            "Epoch [3/10], Training Loss: 0.882, Validation Accuracy: 60.71%\n",
            "Epoch [4/10], Training Loss: 0.855, Validation Accuracy: 60.84%\n",
            "Epoch [5/10], Training Loss: 0.835, Validation Accuracy: 60.96%\n",
            "Epoch [6/10], Training Loss: 0.815, Validation Accuracy: 60.62%\n",
            "Epoch [7/10], Training Loss: 0.794, Validation Accuracy: 60.73%\n",
            "Epoch [8/10], Training Loss: 0.788, Validation Accuracy: 61.12%\n",
            "Epoch [9/10], Training Loss: 0.769, Validation Accuracy: 60.76%\n",
            "Epoch [10/10], Training Loss: 0.753, Validation Accuracy: 60.20%\n",
            "Epoch [1/10], Training Loss: 0.991, Validation Accuracy: 61.30%\n",
            "Epoch [2/10], Training Loss: 0.924, Validation Accuracy: 61.33%\n",
            "Epoch [3/10], Training Loss: 0.879, Validation Accuracy: 60.70%\n",
            "Epoch [4/10], Training Loss: 0.857, Validation Accuracy: 61.41%\n",
            "Epoch [5/10], Training Loss: 0.832, Validation Accuracy: 61.22%\n",
            "Epoch [6/10], Training Loss: 0.819, Validation Accuracy: 60.41%\n",
            "Epoch [7/10], Training Loss: 0.800, Validation Accuracy: 61.48%\n",
            "Epoch [8/10], Training Loss: 0.790, Validation Accuracy: 61.06%\n",
            "Epoch [9/10], Training Loss: 0.767, Validation Accuracy: 61.32%\n",
            "Epoch [10/10], Training Loss: 0.747, Validation Accuracy: 61.45%\n",
            "Epoch [1/10], Training Loss: 0.965, Validation Accuracy: 61.73%\n",
            "Epoch [2/10], Training Loss: 0.908, Validation Accuracy: 61.95%\n",
            "Epoch [3/10], Training Loss: 0.864, Validation Accuracy: 61.51%\n",
            "Epoch [4/10], Training Loss: 0.841, Validation Accuracy: 62.16%\n",
            "Epoch [5/10], Training Loss: 0.813, Validation Accuracy: 61.72%\n",
            "Epoch [6/10], Training Loss: 0.793, Validation Accuracy: 61.49%\n",
            "Epoch [7/10], Training Loss: 0.774, Validation Accuracy: 61.30%\n",
            "Epoch [8/10], Training Loss: 0.760, Validation Accuracy: 61.22%\n",
            "Epoch [9/10], Training Loss: 0.753, Validation Accuracy: 61.35%\n",
            "Epoch [10/10], Training Loss: 0.727, Validation Accuracy: 60.77%\n",
            "Epoch [1/10], Training Loss: 0.961, Validation Accuracy: 61.25%\n",
            "Epoch [2/10], Training Loss: 0.884, Validation Accuracy: 61.55%\n",
            "Epoch [3/10], Training Loss: 0.844, Validation Accuracy: 61.73%\n",
            "Epoch [4/10], Training Loss: 0.817, Validation Accuracy: 61.65%\n",
            "Epoch [5/10], Training Loss: 0.791, Validation Accuracy: 61.43%\n",
            "Epoch [6/10], Training Loss: 0.772, Validation Accuracy: 62.27%\n",
            "Epoch [7/10], Training Loss: 0.755, Validation Accuracy: 61.83%\n",
            "Epoch [8/10], Training Loss: 0.732, Validation Accuracy: 61.18%\n",
            "Epoch [9/10], Training Loss: 0.723, Validation Accuracy: 61.12%\n",
            "Epoch [10/10], Training Loss: 0.702, Validation Accuracy: 61.14%\n",
            "Epoch [1/10], Training Loss: 0.920, Validation Accuracy: 61.81%\n",
            "Epoch [2/10], Training Loss: 0.847, Validation Accuracy: 61.39%\n",
            "Epoch [3/10], Training Loss: 0.805, Validation Accuracy: 61.48%\n",
            "Epoch [4/10], Training Loss: 0.778, Validation Accuracy: 61.49%\n",
            "Epoch [5/10], Training Loss: 0.763, Validation Accuracy: 61.36%\n",
            "Epoch [6/10], Training Loss: 0.731, Validation Accuracy: 60.77%\n",
            "Epoch [7/10], Training Loss: 0.713, Validation Accuracy: 61.27%\n",
            "Epoch [8/10], Training Loss: 0.691, Validation Accuracy: 61.57%\n",
            "Epoch [9/10], Training Loss: 0.670, Validation Accuracy: 61.63%\n",
            "Epoch [10/10], Training Loss: 0.655, Validation Accuracy: 60.48%\n",
            "Epoch [1/10], Training Loss: 0.919, Validation Accuracy: 61.39%\n",
            "Epoch [2/10], Training Loss: 0.848, Validation Accuracy: 60.84%\n",
            "Epoch [3/10], Training Loss: 0.814, Validation Accuracy: 61.51%\n",
            "Epoch [4/10], Training Loss: 0.777, Validation Accuracy: 61.45%\n",
            "Epoch [5/10], Training Loss: 0.753, Validation Accuracy: 60.87%\n",
            "Epoch [6/10], Training Loss: 0.738, Validation Accuracy: 61.02%\n",
            "Epoch [7/10], Training Loss: 0.707, Validation Accuracy: 60.72%\n",
            "Epoch [8/10], Training Loss: 0.691, Validation Accuracy: 61.16%\n",
            "Epoch [9/10], Training Loss: 0.678, Validation Accuracy: 61.40%\n",
            "Epoch [10/10], Training Loss: 0.659, Validation Accuracy: 61.04%\n",
            "Epoch [1/10], Training Loss: 0.939, Validation Accuracy: 61.46%\n",
            "Epoch [2/10], Training Loss: 0.868, Validation Accuracy: 61.85%\n",
            "Epoch [3/10], Training Loss: 0.822, Validation Accuracy: 61.67%\n",
            "Epoch [4/10], Training Loss: 0.788, Validation Accuracy: 61.62%\n",
            "Epoch [5/10], Training Loss: 0.761, Validation Accuracy: 61.60%\n",
            "Epoch [6/10], Training Loss: 0.738, Validation Accuracy: 61.05%\n",
            "Epoch [7/10], Training Loss: 0.718, Validation Accuracy: 61.64%\n",
            "Epoch [8/10], Training Loss: 0.694, Validation Accuracy: 61.07%\n",
            "Epoch [9/10], Training Loss: 0.686, Validation Accuracy: 61.71%\n",
            "Epoch [10/10], Training Loss: 0.662, Validation Accuracy: 60.71%\n",
            "Epoch [1/10], Training Loss: 0.933, Validation Accuracy: 60.73%\n",
            "Epoch [2/10], Training Loss: 0.852, Validation Accuracy: 61.52%\n",
            "Epoch [3/10], Training Loss: 0.806, Validation Accuracy: 62.23%\n",
            "Epoch [4/10], Training Loss: 0.772, Validation Accuracy: 61.88%\n",
            "Epoch [5/10], Training Loss: 0.735, Validation Accuracy: 61.41%\n",
            "Epoch [6/10], Training Loss: 0.711, Validation Accuracy: 62.08%\n",
            "Epoch [7/10], Training Loss: 0.696, Validation Accuracy: 61.85%\n",
            "Epoch [8/10], Training Loss: 0.669, Validation Accuracy: 62.11%\n",
            "Epoch [9/10], Training Loss: 0.653, Validation Accuracy: 61.70%\n",
            "Epoch [10/10], Training Loss: 0.633, Validation Accuracy: 61.42%\n",
            "Epoch [1/10], Training Loss: 0.907, Validation Accuracy: 61.94%\n",
            "Epoch [2/10], Training Loss: 0.825, Validation Accuracy: 61.69%\n",
            "Epoch [3/10], Training Loss: 0.777, Validation Accuracy: 61.13%\n",
            "Epoch [4/10], Training Loss: 0.746, Validation Accuracy: 62.49%\n",
            "Epoch [5/10], Training Loss: 0.718, Validation Accuracy: 61.30%\n",
            "Epoch [6/10], Training Loss: 0.708, Validation Accuracy: 61.39%\n",
            "Epoch [7/10], Training Loss: 0.684, Validation Accuracy: 61.55%\n",
            "Epoch [8/10], Training Loss: 0.658, Validation Accuracy: 61.55%\n",
            "Epoch [9/10], Training Loss: 0.640, Validation Accuracy: 61.47%\n",
            "Epoch [10/10], Training Loss: 0.627, Validation Accuracy: 60.20%\n",
            "Epoch [1/10], Training Loss: 0.882, Validation Accuracy: 61.06%\n",
            "Epoch [2/10], Training Loss: 0.788, Validation Accuracy: 62.08%\n",
            "Epoch [3/10], Training Loss: 0.734, Validation Accuracy: 61.27%\n",
            "Epoch [4/10], Training Loss: 0.706, Validation Accuracy: 61.82%\n",
            "Epoch [5/10], Training Loss: 0.680, Validation Accuracy: 61.75%\n",
            "Epoch [6/10], Training Loss: 0.648, Validation Accuracy: 61.27%\n",
            "Epoch [7/10], Training Loss: 0.634, Validation Accuracy: 60.95%\n",
            "Epoch [8/10], Training Loss: 0.605, Validation Accuracy: 61.21%\n",
            "Epoch [9/10], Training Loss: 0.584, Validation Accuracy: 61.07%\n",
            "Epoch [10/10], Training Loss: 0.566, Validation Accuracy: 61.07%\n",
            "Epoch [1/10], Training Loss: 0.886, Validation Accuracy: 61.06%\n",
            "Epoch [2/10], Training Loss: 0.791, Validation Accuracy: 60.45%\n",
            "Epoch [3/10], Training Loss: 0.743, Validation Accuracy: 61.35%\n",
            "Epoch [4/10], Training Loss: 0.702, Validation Accuracy: 61.73%\n",
            "Epoch [5/10], Training Loss: 0.664, Validation Accuracy: 61.79%\n",
            "Epoch [6/10], Training Loss: 0.647, Validation Accuracy: 61.39%\n",
            "Epoch [7/10], Training Loss: 0.626, Validation Accuracy: 61.43%\n",
            "Epoch [8/10], Training Loss: 0.602, Validation Accuracy: 60.90%\n",
            "Epoch [9/10], Training Loss: 0.580, Validation Accuracy: 61.53%\n",
            "Epoch [10/10], Training Loss: 0.560, Validation Accuracy: 61.21%\n",
            "Epoch [1/10], Training Loss: 0.898, Validation Accuracy: 60.55%\n",
            "Epoch [2/10], Training Loss: 0.801, Validation Accuracy: 61.82%\n",
            "Epoch [3/10], Training Loss: 0.749, Validation Accuracy: 61.99%\n",
            "Epoch [4/10], Training Loss: 0.710, Validation Accuracy: 62.52%\n",
            "Epoch [5/10], Training Loss: 0.678, Validation Accuracy: 61.06%\n",
            "Epoch [6/10], Training Loss: 0.648, Validation Accuracy: 61.15%\n",
            "Epoch [7/10], Training Loss: 0.643, Validation Accuracy: 61.65%\n",
            "Epoch [8/10], Training Loss: 0.614, Validation Accuracy: 61.52%\n",
            "Epoch [9/10], Training Loss: 0.591, Validation Accuracy: 61.15%\n",
            "Epoch [10/10], Training Loss: 0.582, Validation Accuracy: 61.44%\n",
            "Epoch [1/10], Training Loss: 0.881, Validation Accuracy: 60.73%\n",
            "Epoch [2/10], Training Loss: 0.785, Validation Accuracy: 62.11%\n",
            "Epoch [3/10], Training Loss: 0.720, Validation Accuracy: 62.07%\n",
            "Epoch [4/10], Training Loss: 0.697, Validation Accuracy: 61.95%\n",
            "Epoch [5/10], Training Loss: 0.660, Validation Accuracy: 62.07%\n",
            "Epoch [6/10], Training Loss: 0.632, Validation Accuracy: 61.06%\n",
            "Epoch [7/10], Training Loss: 0.612, Validation Accuracy: 61.36%\n",
            "Epoch [8/10], Training Loss: 0.584, Validation Accuracy: 61.72%\n",
            "Epoch [9/10], Training Loss: 0.564, Validation Accuracy: 61.83%\n",
            "Epoch [10/10], Training Loss: 0.555, Validation Accuracy: 62.00%\n",
            "Epoch [1/10], Training Loss: 0.855, Validation Accuracy: 61.09%\n",
            "Epoch [2/10], Training Loss: 0.765, Validation Accuracy: 61.09%\n",
            "Epoch [3/10], Training Loss: 0.721, Validation Accuracy: 62.20%\n",
            "Epoch [4/10], Training Loss: 0.668, Validation Accuracy: 61.36%\n",
            "Epoch [5/10], Training Loss: 0.644, Validation Accuracy: 61.31%\n",
            "Epoch [6/10], Training Loss: 0.625, Validation Accuracy: 61.82%\n",
            "Epoch [7/10], Training Loss: 0.589, Validation Accuracy: 61.47%\n",
            "Epoch [8/10], Training Loss: 0.573, Validation Accuracy: 61.30%\n",
            "Epoch [9/10], Training Loss: 0.544, Validation Accuracy: 61.05%\n",
            "Epoch [10/10], Training Loss: 0.541, Validation Accuracy: 61.03%\n",
            "Epoch [1/10], Training Loss: 0.849, Validation Accuracy: 61.23%\n",
            "Epoch [2/10], Training Loss: 0.732, Validation Accuracy: 61.42%\n",
            "Epoch [3/10], Training Loss: 0.671, Validation Accuracy: 61.76%\n",
            "Epoch [4/10], Training Loss: 0.632, Validation Accuracy: 61.55%\n",
            "Epoch [5/10], Training Loss: 0.597, Validation Accuracy: 61.37%\n",
            "Epoch [6/10], Training Loss: 0.565, Validation Accuracy: 61.18%\n",
            "Epoch [7/10], Training Loss: 0.540, Validation Accuracy: 61.24%\n",
            "Epoch [8/10], Training Loss: 0.521, Validation Accuracy: 60.95%\n",
            "Epoch [9/10], Training Loss: 0.500, Validation Accuracy: 61.07%\n",
            "Epoch [10/10], Training Loss: 0.486, Validation Accuracy: 60.19%\n",
            "Epoch [1/10], Training Loss: 0.857, Validation Accuracy: 60.58%\n",
            "Epoch [2/10], Training Loss: 0.730, Validation Accuracy: 61.07%\n",
            "Epoch [3/10], Training Loss: 0.666, Validation Accuracy: 61.79%\n",
            "Epoch [4/10], Training Loss: 0.618, Validation Accuracy: 61.77%\n",
            "Epoch [5/10], Training Loss: 0.585, Validation Accuracy: 61.20%\n",
            "Epoch [6/10], Training Loss: 0.569, Validation Accuracy: 61.01%\n",
            "Epoch [7/10], Training Loss: 0.537, Validation Accuracy: 60.53%\n",
            "Epoch [8/10], Training Loss: 0.517, Validation Accuracy: 61.02%\n",
            "Epoch [9/10], Training Loss: 0.495, Validation Accuracy: 60.96%\n",
            "Epoch [10/10], Training Loss: 0.467, Validation Accuracy: 60.86%\n",
            "Epoch [1/10], Training Loss: 0.862, Validation Accuracy: 60.98%\n",
            "Epoch [2/10], Training Loss: 0.741, Validation Accuracy: 61.11%\n",
            "Epoch [3/10], Training Loss: 0.689, Validation Accuracy: 61.53%\n",
            "Epoch [4/10], Training Loss: 0.638, Validation Accuracy: 61.47%\n",
            "Epoch [5/10], Training Loss: 0.621, Validation Accuracy: 61.56%\n",
            "Epoch [6/10], Training Loss: 0.581, Validation Accuracy: 61.24%\n",
            "Epoch [7/10], Training Loss: 0.548, Validation Accuracy: 61.01%\n",
            "Epoch [8/10], Training Loss: 0.534, Validation Accuracy: 60.89%\n",
            "Epoch [9/10], Training Loss: 0.510, Validation Accuracy: 61.35%\n",
            "Epoch [10/10], Training Loss: 0.494, Validation Accuracy: 60.93%\n",
            "Confusion Matrix:\n",
            "[[698  17  41  10  29  11  18  10 104  62]\n",
            " [ 49 615  15  11   8  15  13   4  65 205]\n",
            " [ 78   8 471  47 130  92  85  46  26  17]\n",
            " [ 21   8  82 331 105 213 107  53  35  45]\n",
            " [ 23   6  65  47 593  50  88  85  27  16]\n",
            " [ 18   8  68 126  72 545  54  72  17  20]\n",
            " [ 10   7  47  54  75  32 731  14  10  20]\n",
            " [ 20   9  34  36  95  89  18 657   4  38]\n",
            " [ 94  35  14   9  10   9  10   8 751  60]\n",
            " [ 58  57  16  19  21  12  18  32  56 711]]\n",
            "Test Accuracy: 61.03%\n",
            "True Positives (TP): [698 615 471 331 593 545 731 657 751 711]\n",
            "False Positives (FP): [371 155 382 359 545 523 411 324 344 483]\n",
            "True Negatives (TN): [8629 8845 8618 8641 8455 8477 8589 8676 8656 8517]\n",
            "False Negatives (FN): [302 385 529 669 407 455 269 343 249 289]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.65294668 0.7987013  0.55216882 0.47971014 0.52108963 0.51029963\n",
            " 0.64010508 0.66972477 0.68584475 0.59547739]\n",
            "Recall: [0.698 0.615 0.471 0.331 0.593 0.545 0.731 0.657 0.751 0.711]\n",
            "F1 Score: [0.67472209 0.69491525 0.50836481 0.39171598 0.55472404 0.5270793\n",
            " 0.68253968 0.66330136 0.71694511 0.64813127]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 0.1):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=0.1):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_normal: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = ( augmented_data_normal + augmented_data_truncated) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=0.1)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"normal\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iUz3Q723B3z"
      },
      "source": [
        "Beta =.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbKkmn683E5V",
        "outputId": "c4235139-2a59-4e49-ac2e-68442b8049f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Images per Class: [5936 6016 6061 5973 5968 5981 5979 6046 6013 6027]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<timed exec>:281: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Training Loss: 2.308, Validation Accuracy: 10.06%\n",
            "Epoch [2/10], Training Loss: 2.307, Validation Accuracy: 10.06%\n",
            "Epoch [3/10], Training Loss: 2.305, Validation Accuracy: 10.06%\n",
            "Epoch [4/10], Training Loss: 2.304, Validation Accuracy: 10.05%\n",
            "Epoch [5/10], Training Loss: 2.303, Validation Accuracy: 10.03%\n",
            "Epoch [6/10], Training Loss: 2.302, Validation Accuracy: 10.31%\n",
            "Epoch [7/10], Training Loss: 2.301, Validation Accuracy: 10.93%\n",
            "Epoch [8/10], Training Loss: 2.301, Validation Accuracy: 11.08%\n",
            "Epoch [9/10], Training Loss: 2.300, Validation Accuracy: 10.64%\n",
            "Epoch [10/10], Training Loss: 2.299, Validation Accuracy: 10.64%\n",
            "Epoch [1/10], Training Loss: 2.300, Validation Accuracy: 10.82%\n",
            "Epoch [2/10], Training Loss: 2.299, Validation Accuracy: 10.83%\n",
            "Epoch [3/10], Training Loss: 2.298, Validation Accuracy: 11.23%\n",
            "Epoch [4/10], Training Loss: 2.297, Validation Accuracy: 14.84%\n",
            "Epoch [5/10], Training Loss: 2.295, Validation Accuracy: 17.06%\n",
            "Epoch [6/10], Training Loss: 2.293, Validation Accuracy: 17.39%\n",
            "Epoch [7/10], Training Loss: 2.291, Validation Accuracy: 16.72%\n",
            "Epoch [8/10], Training Loss: 2.288, Validation Accuracy: 16.11%\n",
            "Epoch [9/10], Training Loss: 2.283, Validation Accuracy: 15.50%\n",
            "Epoch [10/10], Training Loss: 2.276, Validation Accuracy: 15.32%\n",
            "Epoch [1/10], Training Loss: 2.268, Validation Accuracy: 15.98%\n",
            "Epoch [2/10], Training Loss: 2.252, Validation Accuracy: 16.26%\n",
            "Epoch [3/10], Training Loss: 2.227, Validation Accuracy: 16.58%\n",
            "Epoch [4/10], Training Loss: 2.195, Validation Accuracy: 18.40%\n",
            "Epoch [5/10], Training Loss: 2.164, Validation Accuracy: 20.55%\n",
            "Epoch [6/10], Training Loss: 2.143, Validation Accuracy: 22.32%\n",
            "Epoch [7/10], Training Loss: 2.128, Validation Accuracy: 23.06%\n",
            "Epoch [8/10], Training Loss: 2.116, Validation Accuracy: 23.70%\n",
            "Epoch [9/10], Training Loss: 2.107, Validation Accuracy: 24.39%\n",
            "Epoch [10/10], Training Loss: 2.095, Validation Accuracy: 25.41%\n",
            "Epoch [1/10], Training Loss: 2.091, Validation Accuracy: 25.56%\n",
            "Epoch [2/10], Training Loss: 2.078, Validation Accuracy: 26.63%\n",
            "Epoch [3/10], Training Loss: 2.063, Validation Accuracy: 27.41%\n",
            "Epoch [4/10], Training Loss: 2.044, Validation Accuracy: 27.61%\n",
            "Epoch [5/10], Training Loss: 2.026, Validation Accuracy: 27.72%\n",
            "Epoch [6/10], Training Loss: 1.999, Validation Accuracy: 29.00%\n",
            "Epoch [7/10], Training Loss: 1.969, Validation Accuracy: 29.80%\n",
            "Epoch [8/10], Training Loss: 1.944, Validation Accuracy: 31.33%\n",
            "Epoch [9/10], Training Loss: 1.920, Validation Accuracy: 31.23%\n",
            "Epoch [10/10], Training Loss: 1.903, Validation Accuracy: 31.18%\n",
            "Epoch [1/10], Training Loss: 1.908, Validation Accuracy: 32.51%\n",
            "Epoch [2/10], Training Loss: 1.891, Validation Accuracy: 32.91%\n",
            "Epoch [3/10], Training Loss: 1.873, Validation Accuracy: 33.38%\n",
            "Epoch [4/10], Training Loss: 1.859, Validation Accuracy: 34.21%\n",
            "Epoch [5/10], Training Loss: 1.841, Validation Accuracy: 34.48%\n",
            "Epoch [6/10], Training Loss: 1.833, Validation Accuracy: 34.66%\n",
            "Epoch [7/10], Training Loss: 1.812, Validation Accuracy: 35.43%\n",
            "Epoch [8/10], Training Loss: 1.792, Validation Accuracy: 35.42%\n",
            "Epoch [9/10], Training Loss: 1.781, Validation Accuracy: 35.63%\n",
            "Epoch [10/10], Training Loss: 1.762, Validation Accuracy: 36.92%\n",
            "Epoch [1/10], Training Loss: 1.779, Validation Accuracy: 36.69%\n",
            "Epoch [2/10], Training Loss: 1.759, Validation Accuracy: 37.04%\n",
            "Epoch [3/10], Training Loss: 1.738, Validation Accuracy: 37.58%\n",
            "Epoch [4/10], Training Loss: 1.724, Validation Accuracy: 37.43%\n",
            "Epoch [5/10], Training Loss: 1.717, Validation Accuracy: 36.52%\n",
            "Epoch [6/10], Training Loss: 1.712, Validation Accuracy: 37.93%\n",
            "Epoch [7/10], Training Loss: 1.691, Validation Accuracy: 38.90%\n",
            "Epoch [8/10], Training Loss: 1.682, Validation Accuracy: 38.86%\n",
            "Epoch [9/10], Training Loss: 1.673, Validation Accuracy: 39.19%\n",
            "Epoch [10/10], Training Loss: 1.667, Validation Accuracy: 39.81%\n",
            "Epoch [1/10], Training Loss: 1.653, Validation Accuracy: 38.27%\n",
            "Epoch [2/10], Training Loss: 1.644, Validation Accuracy: 39.52%\n",
            "Epoch [3/10], Training Loss: 1.619, Validation Accuracy: 40.24%\n",
            "Epoch [4/10], Training Loss: 1.612, Validation Accuracy: 40.72%\n",
            "Epoch [5/10], Training Loss: 1.607, Validation Accuracy: 39.83%\n",
            "Epoch [6/10], Training Loss: 1.590, Validation Accuracy: 41.13%\n",
            "Epoch [7/10], Training Loss: 1.577, Validation Accuracy: 41.87%\n",
            "Epoch [8/10], Training Loss: 1.562, Validation Accuracy: 41.75%\n",
            "Epoch [9/10], Training Loss: 1.558, Validation Accuracy: 41.10%\n",
            "Epoch [10/10], Training Loss: 1.546, Validation Accuracy: 42.65%\n",
            "Epoch [1/10], Training Loss: 1.575, Validation Accuracy: 42.87%\n",
            "Epoch [2/10], Training Loss: 1.560, Validation Accuracy: 41.60%\n",
            "Epoch [3/10], Training Loss: 1.551, Validation Accuracy: 43.60%\n",
            "Epoch [4/10], Training Loss: 1.538, Validation Accuracy: 43.70%\n",
            "Epoch [5/10], Training Loss: 1.527, Validation Accuracy: 44.37%\n",
            "Epoch [6/10], Training Loss: 1.520, Validation Accuracy: 44.37%\n",
            "Epoch [7/10], Training Loss: 1.512, Validation Accuracy: 44.10%\n",
            "Epoch [8/10], Training Loss: 1.506, Validation Accuracy: 43.48%\n",
            "Epoch [9/10], Training Loss: 1.494, Validation Accuracy: 45.11%\n",
            "Epoch [10/10], Training Loss: 1.482, Validation Accuracy: 45.56%\n",
            "Epoch [1/10], Training Loss: 1.500, Validation Accuracy: 44.66%\n",
            "Epoch [2/10], Training Loss: 1.480, Validation Accuracy: 45.51%\n",
            "Epoch [3/10], Training Loss: 1.478, Validation Accuracy: 45.13%\n",
            "Epoch [4/10], Training Loss: 1.455, Validation Accuracy: 46.26%\n",
            "Epoch [5/10], Training Loss: 1.454, Validation Accuracy: 46.15%\n",
            "Epoch [6/10], Training Loss: 1.435, Validation Accuracy: 46.50%\n",
            "Epoch [7/10], Training Loss: 1.431, Validation Accuracy: 47.00%\n",
            "Epoch [8/10], Training Loss: 1.422, Validation Accuracy: 46.77%\n",
            "Epoch [9/10], Training Loss: 1.412, Validation Accuracy: 47.42%\n",
            "Epoch [10/10], Training Loss: 1.401, Validation Accuracy: 47.76%\n",
            "Epoch [1/10], Training Loss: 1.455, Validation Accuracy: 47.52%\n",
            "Epoch [2/10], Training Loss: 1.433, Validation Accuracy: 47.52%\n",
            "Epoch [3/10], Training Loss: 1.427, Validation Accuracy: 46.02%\n",
            "Epoch [4/10], Training Loss: 1.421, Validation Accuracy: 47.38%\n",
            "Epoch [5/10], Training Loss: 1.405, Validation Accuracy: 48.07%\n",
            "Epoch [6/10], Training Loss: 1.396, Validation Accuracy: 47.45%\n",
            "Epoch [7/10], Training Loss: 1.391, Validation Accuracy: 47.97%\n",
            "Epoch [8/10], Training Loss: 1.383, Validation Accuracy: 48.20%\n",
            "Epoch [9/10], Training Loss: 1.374, Validation Accuracy: 47.63%\n",
            "Epoch [10/10], Training Loss: 1.368, Validation Accuracy: 48.27%\n",
            "Epoch [1/10], Training Loss: 1.428, Validation Accuracy: 48.66%\n",
            "Epoch [2/10], Training Loss: 1.404, Validation Accuracy: 48.57%\n",
            "Epoch [3/10], Training Loss: 1.388, Validation Accuracy: 49.22%\n",
            "Epoch [4/10], Training Loss: 1.386, Validation Accuracy: 49.98%\n",
            "Epoch [5/10], Training Loss: 1.377, Validation Accuracy: 49.83%\n",
            "Epoch [6/10], Training Loss: 1.359, Validation Accuracy: 49.74%\n",
            "Epoch [7/10], Training Loss: 1.350, Validation Accuracy: 49.92%\n",
            "Epoch [8/10], Training Loss: 1.351, Validation Accuracy: 50.03%\n",
            "Epoch [9/10], Training Loss: 1.340, Validation Accuracy: 49.16%\n",
            "Epoch [10/10], Training Loss: 1.329, Validation Accuracy: 49.99%\n",
            "Epoch [1/10], Training Loss: 1.361, Validation Accuracy: 50.72%\n",
            "Epoch [2/10], Training Loss: 1.342, Validation Accuracy: 50.68%\n",
            "Epoch [3/10], Training Loss: 1.326, Validation Accuracy: 50.34%\n",
            "Epoch [4/10], Training Loss: 1.322, Validation Accuracy: 50.97%\n",
            "Epoch [5/10], Training Loss: 1.309, Validation Accuracy: 50.73%\n",
            "Epoch [6/10], Training Loss: 1.301, Validation Accuracy: 51.09%\n",
            "Epoch [7/10], Training Loss: 1.296, Validation Accuracy: 51.37%\n",
            "Epoch [8/10], Training Loss: 1.277, Validation Accuracy: 51.14%\n",
            "Epoch [9/10], Training Loss: 1.272, Validation Accuracy: 51.63%\n",
            "Epoch [10/10], Training Loss: 1.263, Validation Accuracy: 51.12%\n",
            "Epoch [1/10], Training Loss: 1.337, Validation Accuracy: 51.65%\n",
            "Epoch [2/10], Training Loss: 1.315, Validation Accuracy: 52.34%\n",
            "Epoch [3/10], Training Loss: 1.302, Validation Accuracy: 52.66%\n",
            "Epoch [4/10], Training Loss: 1.286, Validation Accuracy: 52.24%\n",
            "Epoch [5/10], Training Loss: 1.280, Validation Accuracy: 51.89%\n",
            "Epoch [6/10], Training Loss: 1.271, Validation Accuracy: 52.82%\n",
            "Epoch [7/10], Training Loss: 1.257, Validation Accuracy: 52.88%\n",
            "Epoch [8/10], Training Loss: 1.258, Validation Accuracy: 52.26%\n",
            "Epoch [9/10], Training Loss: 1.245, Validation Accuracy: 52.28%\n",
            "Epoch [10/10], Training Loss: 1.242, Validation Accuracy: 53.53%\n",
            "Epoch [1/10], Training Loss: 1.284, Validation Accuracy: 53.21%\n",
            "Epoch [2/10], Training Loss: 1.257, Validation Accuracy: 53.78%\n",
            "Epoch [3/10], Training Loss: 1.234, Validation Accuracy: 53.65%\n",
            "Epoch [4/10], Training Loss: 1.224, Validation Accuracy: 52.95%\n",
            "Epoch [5/10], Training Loss: 1.226, Validation Accuracy: 54.28%\n",
            "Epoch [6/10], Training Loss: 1.218, Validation Accuracy: 54.11%\n",
            "Epoch [7/10], Training Loss: 1.201, Validation Accuracy: 53.86%\n",
            "Epoch [8/10], Training Loss: 1.196, Validation Accuracy: 53.61%\n",
            "Epoch [9/10], Training Loss: 1.190, Validation Accuracy: 53.03%\n",
            "Epoch [10/10], Training Loss: 1.178, Validation Accuracy: 54.10%\n",
            "Epoch [1/10], Training Loss: 1.272, Validation Accuracy: 53.84%\n",
            "Epoch [2/10], Training Loss: 1.246, Validation Accuracy: 54.76%\n",
            "Epoch [3/10], Training Loss: 1.231, Validation Accuracy: 54.57%\n",
            "Epoch [4/10], Training Loss: 1.223, Validation Accuracy: 54.26%\n",
            "Epoch [5/10], Training Loss: 1.209, Validation Accuracy: 54.79%\n",
            "Epoch [6/10], Training Loss: 1.204, Validation Accuracy: 54.96%\n",
            "Epoch [7/10], Training Loss: 1.190, Validation Accuracy: 54.49%\n",
            "Epoch [8/10], Training Loss: 1.184, Validation Accuracy: 54.74%\n",
            "Epoch [9/10], Training Loss: 1.183, Validation Accuracy: 54.16%\n",
            "Epoch [10/10], Training Loss: 1.174, Validation Accuracy: 55.14%\n",
            "Epoch [1/10], Training Loss: 1.239, Validation Accuracy: 54.76%\n",
            "Epoch [2/10], Training Loss: 1.220, Validation Accuracy: 54.51%\n",
            "Epoch [3/10], Training Loss: 1.197, Validation Accuracy: 54.36%\n",
            "Epoch [4/10], Training Loss: 1.192, Validation Accuracy: 55.54%\n",
            "Epoch [5/10], Training Loss: 1.179, Validation Accuracy: 55.51%\n",
            "Epoch [6/10], Training Loss: 1.174, Validation Accuracy: 53.24%\n",
            "Epoch [7/10], Training Loss: 1.164, Validation Accuracy: 55.42%\n",
            "Epoch [8/10], Training Loss: 1.147, Validation Accuracy: 54.41%\n",
            "Epoch [9/10], Training Loss: 1.137, Validation Accuracy: 55.67%\n",
            "Epoch [10/10], Training Loss: 1.133, Validation Accuracy: 55.18%\n",
            "Epoch [1/10], Training Loss: 1.207, Validation Accuracy: 55.84%\n",
            "Epoch [2/10], Training Loss: 1.176, Validation Accuracy: 56.07%\n",
            "Epoch [3/10], Training Loss: 1.162, Validation Accuracy: 55.94%\n",
            "Epoch [4/10], Training Loss: 1.140, Validation Accuracy: 55.99%\n",
            "Epoch [5/10], Training Loss: 1.132, Validation Accuracy: 56.66%\n",
            "Epoch [6/10], Training Loss: 1.119, Validation Accuracy: 55.99%\n",
            "Epoch [7/10], Training Loss: 1.109, Validation Accuracy: 55.78%\n",
            "Epoch [8/10], Training Loss: 1.102, Validation Accuracy: 56.57%\n",
            "Epoch [9/10], Training Loss: 1.097, Validation Accuracy: 56.55%\n",
            "Epoch [10/10], Training Loss: 1.082, Validation Accuracy: 56.66%\n",
            "Epoch [1/10], Training Loss: 1.194, Validation Accuracy: 55.55%\n",
            "Epoch [2/10], Training Loss: 1.158, Validation Accuracy: 57.23%\n",
            "Epoch [3/10], Training Loss: 1.141, Validation Accuracy: 57.50%\n",
            "Epoch [4/10], Training Loss: 1.124, Validation Accuracy: 57.09%\n",
            "Epoch [5/10], Training Loss: 1.107, Validation Accuracy: 56.86%\n",
            "Epoch [6/10], Training Loss: 1.099, Validation Accuracy: 57.64%\n",
            "Epoch [7/10], Training Loss: 1.090, Validation Accuracy: 57.79%\n",
            "Epoch [8/10], Training Loss: 1.080, Validation Accuracy: 57.02%\n",
            "Epoch [9/10], Training Loss: 1.074, Validation Accuracy: 56.92%\n",
            "Epoch [10/10], Training Loss: 1.059, Validation Accuracy: 58.12%\n",
            "Epoch [1/10], Training Loss: 1.158, Validation Accuracy: 58.02%\n",
            "Epoch [2/10], Training Loss: 1.126, Validation Accuracy: 57.98%\n",
            "Epoch [3/10], Training Loss: 1.108, Validation Accuracy: 57.90%\n",
            "Epoch [4/10], Training Loss: 1.102, Validation Accuracy: 58.50%\n",
            "Epoch [5/10], Training Loss: 1.077, Validation Accuracy: 58.07%\n",
            "Epoch [6/10], Training Loss: 1.063, Validation Accuracy: 57.98%\n",
            "Epoch [7/10], Training Loss: 1.050, Validation Accuracy: 57.40%\n",
            "Epoch [8/10], Training Loss: 1.048, Validation Accuracy: 57.82%\n",
            "Epoch [9/10], Training Loss: 1.037, Validation Accuracy: 57.19%\n",
            "Epoch [10/10], Training Loss: 1.022, Validation Accuracy: 57.12%\n",
            "Epoch [1/10], Training Loss: 1.156, Validation Accuracy: 57.00%\n",
            "Epoch [2/10], Training Loss: 1.123, Validation Accuracy: 58.98%\n",
            "Epoch [3/10], Training Loss: 1.099, Validation Accuracy: 57.78%\n",
            "Epoch [4/10], Training Loss: 1.085, Validation Accuracy: 58.77%\n",
            "Epoch [5/10], Training Loss: 1.074, Validation Accuracy: 58.49%\n",
            "Epoch [6/10], Training Loss: 1.064, Validation Accuracy: 58.64%\n",
            "Epoch [7/10], Training Loss: 1.048, Validation Accuracy: 58.42%\n",
            "Epoch [8/10], Training Loss: 1.040, Validation Accuracy: 58.72%\n",
            "Epoch [9/10], Training Loss: 1.026, Validation Accuracy: 58.38%\n",
            "Epoch [10/10], Training Loss: 1.012, Validation Accuracy: 58.90%\n",
            "Epoch [1/10], Training Loss: 1.130, Validation Accuracy: 58.13%\n",
            "Epoch [2/10], Training Loss: 1.090, Validation Accuracy: 58.81%\n",
            "Epoch [3/10], Training Loss: 1.076, Validation Accuracy: 59.09%\n",
            "Epoch [4/10], Training Loss: 1.051, Validation Accuracy: 59.09%\n",
            "Epoch [5/10], Training Loss: 1.039, Validation Accuracy: 59.36%\n",
            "Epoch [6/10], Training Loss: 1.023, Validation Accuracy: 58.93%\n",
            "Epoch [7/10], Training Loss: 1.011, Validation Accuracy: 58.40%\n",
            "Epoch [8/10], Training Loss: 1.003, Validation Accuracy: 58.73%\n",
            "Epoch [9/10], Training Loss: 0.999, Validation Accuracy: 59.48%\n",
            "Epoch [10/10], Training Loss: 0.983, Validation Accuracy: 59.74%\n",
            "Epoch [1/10], Training Loss: 1.106, Validation Accuracy: 58.23%\n",
            "Epoch [2/10], Training Loss: 1.065, Validation Accuracy: 59.02%\n",
            "Epoch [3/10], Training Loss: 1.048, Validation Accuracy: 58.58%\n",
            "Epoch [4/10], Training Loss: 1.027, Validation Accuracy: 60.08%\n",
            "Epoch [5/10], Training Loss: 1.012, Validation Accuracy: 59.53%\n",
            "Epoch [6/10], Training Loss: 0.997, Validation Accuracy: 59.38%\n",
            "Epoch [7/10], Training Loss: 0.992, Validation Accuracy: 59.29%\n",
            "Epoch [8/10], Training Loss: 0.978, Validation Accuracy: 59.27%\n",
            "Epoch [9/10], Training Loss: 0.965, Validation Accuracy: 58.91%\n",
            "Epoch [10/10], Training Loss: 0.958, Validation Accuracy: 60.00%\n",
            "Epoch [1/10], Training Loss: 1.075, Validation Accuracy: 59.61%\n",
            "Epoch [2/10], Training Loss: 1.039, Validation Accuracy: 60.07%\n",
            "Epoch [3/10], Training Loss: 1.015, Validation Accuracy: 59.72%\n",
            "Epoch [4/10], Training Loss: 0.997, Validation Accuracy: 60.32%\n",
            "Epoch [5/10], Training Loss: 0.980, Validation Accuracy: 59.80%\n",
            "Epoch [6/10], Training Loss: 0.965, Validation Accuracy: 60.12%\n",
            "Epoch [7/10], Training Loss: 0.950, Validation Accuracy: 60.04%\n",
            "Epoch [8/10], Training Loss: 0.941, Validation Accuracy: 59.64%\n",
            "Epoch [9/10], Training Loss: 0.933, Validation Accuracy: 60.21%\n",
            "Epoch [10/10], Training Loss: 0.923, Validation Accuracy: 60.32%\n",
            "Epoch [1/10], Training Loss: 1.050, Validation Accuracy: 60.73%\n",
            "Epoch [2/10], Training Loss: 1.016, Validation Accuracy: 60.79%\n",
            "Epoch [3/10], Training Loss: 0.994, Validation Accuracy: 60.43%\n",
            "Epoch [4/10], Training Loss: 0.974, Validation Accuracy: 59.88%\n",
            "Epoch [5/10], Training Loss: 0.964, Validation Accuracy: 60.17%\n",
            "Epoch [6/10], Training Loss: 0.947, Validation Accuracy: 60.52%\n",
            "Epoch [7/10], Training Loss: 0.931, Validation Accuracy: 60.44%\n",
            "Epoch [8/10], Training Loss: 0.919, Validation Accuracy: 60.15%\n",
            "Epoch [9/10], Training Loss: 0.909, Validation Accuracy: 60.37%\n",
            "Epoch [10/10], Training Loss: 0.901, Validation Accuracy: 60.55%\n",
            "Epoch [1/10], Training Loss: 1.065, Validation Accuracy: 60.85%\n",
            "Epoch [2/10], Training Loss: 1.025, Validation Accuracy: 60.39%\n",
            "Epoch [3/10], Training Loss: 0.993, Validation Accuracy: 61.24%\n",
            "Epoch [4/10], Training Loss: 0.978, Validation Accuracy: 60.74%\n",
            "Epoch [5/10], Training Loss: 0.965, Validation Accuracy: 60.73%\n",
            "Epoch [6/10], Training Loss: 0.943, Validation Accuracy: 60.34%\n",
            "Epoch [7/10], Training Loss: 0.925, Validation Accuracy: 61.39%\n",
            "Epoch [8/10], Training Loss: 0.914, Validation Accuracy: 60.39%\n",
            "Epoch [9/10], Training Loss: 0.909, Validation Accuracy: 60.64%\n",
            "Epoch [10/10], Training Loss: 0.891, Validation Accuracy: 61.08%\n",
            "Epoch [1/10], Training Loss: 1.045, Validation Accuracy: 61.42%\n",
            "Epoch [2/10], Training Loss: 0.997, Validation Accuracy: 60.88%\n",
            "Epoch [3/10], Training Loss: 0.968, Validation Accuracy: 60.81%\n",
            "Epoch [4/10], Training Loss: 0.952, Validation Accuracy: 60.88%\n",
            "Epoch [5/10], Training Loss: 0.939, Validation Accuracy: 60.51%\n",
            "Epoch [6/10], Training Loss: 0.915, Validation Accuracy: 60.86%\n",
            "Epoch [7/10], Training Loss: 0.904, Validation Accuracy: 60.70%\n",
            "Epoch [8/10], Training Loss: 0.893, Validation Accuracy: 61.28%\n",
            "Epoch [9/10], Training Loss: 0.873, Validation Accuracy: 60.80%\n",
            "Epoch [10/10], Training Loss: 0.872, Validation Accuracy: 59.44%\n",
            "Epoch [1/10], Training Loss: 1.035, Validation Accuracy: 61.09%\n",
            "Epoch [2/10], Training Loss: 0.979, Validation Accuracy: 60.60%\n",
            "Epoch [3/10], Training Loss: 0.955, Validation Accuracy: 60.80%\n",
            "Epoch [4/10], Training Loss: 0.932, Validation Accuracy: 61.05%\n",
            "Epoch [5/10], Training Loss: 0.914, Validation Accuracy: 61.15%\n",
            "Epoch [6/10], Training Loss: 0.896, Validation Accuracy: 61.20%\n",
            "Epoch [7/10], Training Loss: 0.889, Validation Accuracy: 61.59%\n",
            "Epoch [8/10], Training Loss: 0.864, Validation Accuracy: 60.96%\n",
            "Epoch [9/10], Training Loss: 0.849, Validation Accuracy: 61.16%\n",
            "Epoch [10/10], Training Loss: 0.844, Validation Accuracy: 61.13%\n",
            "Epoch [1/10], Training Loss: 0.988, Validation Accuracy: 61.24%\n",
            "Epoch [2/10], Training Loss: 0.949, Validation Accuracy: 61.95%\n",
            "Epoch [3/10], Training Loss: 0.923, Validation Accuracy: 62.03%\n",
            "Epoch [4/10], Training Loss: 0.900, Validation Accuracy: 62.57%\n",
            "Epoch [5/10], Training Loss: 0.874, Validation Accuracy: 62.40%\n",
            "Epoch [6/10], Training Loss: 0.856, Validation Accuracy: 62.33%\n",
            "Epoch [7/10], Training Loss: 0.850, Validation Accuracy: 62.06%\n",
            "Epoch [8/10], Training Loss: 0.836, Validation Accuracy: 62.62%\n",
            "Epoch [9/10], Training Loss: 0.829, Validation Accuracy: 62.18%\n",
            "Epoch [10/10], Training Loss: 0.808, Validation Accuracy: 61.74%\n",
            "Epoch [1/10], Training Loss: 0.980, Validation Accuracy: 61.57%\n",
            "Epoch [2/10], Training Loss: 0.932, Validation Accuracy: 62.36%\n",
            "Epoch [3/10], Training Loss: 0.906, Validation Accuracy: 62.14%\n",
            "Epoch [4/10], Training Loss: 0.883, Validation Accuracy: 62.47%\n",
            "Epoch [5/10], Training Loss: 0.859, Validation Accuracy: 61.79%\n",
            "Epoch [6/10], Training Loss: 0.851, Validation Accuracy: 61.99%\n",
            "Epoch [7/10], Training Loss: 0.832, Validation Accuracy: 62.08%\n",
            "Epoch [8/10], Training Loss: 0.817, Validation Accuracy: 62.17%\n",
            "Epoch [9/10], Training Loss: 0.799, Validation Accuracy: 61.71%\n",
            "Epoch [10/10], Training Loss: 0.787, Validation Accuracy: 61.71%\n",
            "Epoch [1/10], Training Loss: 0.994, Validation Accuracy: 61.46%\n",
            "Epoch [2/10], Training Loss: 0.934, Validation Accuracy: 61.17%\n",
            "Epoch [3/10], Training Loss: 0.905, Validation Accuracy: 62.15%\n",
            "Epoch [4/10], Training Loss: 0.875, Validation Accuracy: 62.20%\n",
            "Epoch [5/10], Training Loss: 0.861, Validation Accuracy: 61.85%\n",
            "Epoch [6/10], Training Loss: 0.850, Validation Accuracy: 62.31%\n",
            "Epoch [7/10], Training Loss: 0.828, Validation Accuracy: 61.87%\n",
            "Epoch [8/10], Training Loss: 0.804, Validation Accuracy: 62.74%\n",
            "Epoch [9/10], Training Loss: 0.798, Validation Accuracy: 61.92%\n",
            "Epoch [10/10], Training Loss: 0.779, Validation Accuracy: 61.48%\n",
            "Epoch [1/10], Training Loss: 0.972, Validation Accuracy: 61.85%\n",
            "Epoch [2/10], Training Loss: 0.910, Validation Accuracy: 61.94%\n",
            "Epoch [3/10], Training Loss: 0.884, Validation Accuracy: 61.73%\n",
            "Epoch [4/10], Training Loss: 0.867, Validation Accuracy: 62.61%\n",
            "Epoch [5/10], Training Loss: 0.837, Validation Accuracy: 62.22%\n",
            "Epoch [6/10], Training Loss: 0.819, Validation Accuracy: 62.17%\n",
            "Epoch [7/10], Training Loss: 0.804, Validation Accuracy: 62.41%\n",
            "Epoch [8/10], Training Loss: 0.787, Validation Accuracy: 61.05%\n",
            "Epoch [9/10], Training Loss: 0.783, Validation Accuracy: 62.44%\n",
            "Epoch [10/10], Training Loss: 0.753, Validation Accuracy: 61.80%\n",
            "Epoch [1/10], Training Loss: 0.952, Validation Accuracy: 62.54%\n",
            "Epoch [2/10], Training Loss: 0.899, Validation Accuracy: 62.24%\n",
            "Epoch [3/10], Training Loss: 0.863, Validation Accuracy: 62.31%\n",
            "Epoch [4/10], Training Loss: 0.843, Validation Accuracy: 62.83%\n",
            "Epoch [5/10], Training Loss: 0.822, Validation Accuracy: 62.10%\n",
            "Epoch [6/10], Training Loss: 0.814, Validation Accuracy: 62.96%\n",
            "Epoch [7/10], Training Loss: 0.785, Validation Accuracy: 62.19%\n",
            "Epoch [8/10], Training Loss: 0.777, Validation Accuracy: 62.27%\n",
            "Epoch [9/10], Training Loss: 0.751, Validation Accuracy: 62.28%\n",
            "Epoch [10/10], Training Loss: 0.734, Validation Accuracy: 61.74%\n",
            "Epoch [1/10], Training Loss: 0.938, Validation Accuracy: 61.79%\n",
            "Epoch [2/10], Training Loss: 0.880, Validation Accuracy: 63.21%\n",
            "Epoch [3/10], Training Loss: 0.835, Validation Accuracy: 63.33%\n",
            "Epoch [4/10], Training Loss: 0.812, Validation Accuracy: 63.17%\n",
            "Epoch [5/10], Training Loss: 0.795, Validation Accuracy: 62.98%\n",
            "Epoch [6/10], Training Loss: 0.779, Validation Accuracy: 63.27%\n",
            "Epoch [7/10], Training Loss: 0.754, Validation Accuracy: 62.54%\n",
            "Epoch [8/10], Training Loss: 0.733, Validation Accuracy: 62.89%\n",
            "Epoch [9/10], Training Loss: 0.727, Validation Accuracy: 63.09%\n",
            "Epoch [10/10], Training Loss: 0.711, Validation Accuracy: 63.34%\n",
            "Epoch [1/10], Training Loss: 0.913, Validation Accuracy: 62.75%\n",
            "Epoch [2/10], Training Loss: 0.863, Validation Accuracy: 63.34%\n",
            "Epoch [3/10], Training Loss: 0.824, Validation Accuracy: 61.56%\n",
            "Epoch [4/10], Training Loss: 0.791, Validation Accuracy: 63.47%\n",
            "Epoch [5/10], Training Loss: 0.777, Validation Accuracy: 63.16%\n",
            "Epoch [6/10], Training Loss: 0.754, Validation Accuracy: 63.31%\n",
            "Epoch [7/10], Training Loss: 0.735, Validation Accuracy: 63.06%\n",
            "Epoch [8/10], Training Loss: 0.717, Validation Accuracy: 62.82%\n",
            "Epoch [9/10], Training Loss: 0.706, Validation Accuracy: 62.95%\n",
            "Epoch [10/10], Training Loss: 0.685, Validation Accuracy: 62.74%\n",
            "Epoch [1/10], Training Loss: 0.913, Validation Accuracy: 62.90%\n",
            "Epoch [2/10], Training Loss: 0.846, Validation Accuracy: 62.83%\n",
            "Epoch [3/10], Training Loss: 0.811, Validation Accuracy: 63.20%\n",
            "Epoch [4/10], Training Loss: 0.779, Validation Accuracy: 62.53%\n",
            "Epoch [5/10], Training Loss: 0.763, Validation Accuracy: 62.41%\n",
            "Epoch [6/10], Training Loss: 0.746, Validation Accuracy: 63.05%\n",
            "Epoch [7/10], Training Loss: 0.731, Validation Accuracy: 61.45%\n",
            "Epoch [8/10], Training Loss: 0.707, Validation Accuracy: 62.48%\n",
            "Epoch [9/10], Training Loss: 0.692, Validation Accuracy: 62.50%\n",
            "Epoch [10/10], Training Loss: 0.672, Validation Accuracy: 62.62%\n",
            "Epoch [1/10], Training Loss: 0.913, Validation Accuracy: 62.42%\n",
            "Epoch [2/10], Training Loss: 0.846, Validation Accuracy: 62.93%\n",
            "Epoch [3/10], Training Loss: 0.820, Validation Accuracy: 62.96%\n",
            "Epoch [4/10], Training Loss: 0.778, Validation Accuracy: 62.91%\n",
            "Epoch [5/10], Training Loss: 0.759, Validation Accuracy: 62.91%\n",
            "Epoch [6/10], Training Loss: 0.727, Validation Accuracy: 62.56%\n",
            "Epoch [7/10], Training Loss: 0.713, Validation Accuracy: 62.97%\n",
            "Epoch [8/10], Training Loss: 0.702, Validation Accuracy: 62.34%\n",
            "Epoch [9/10], Training Loss: 0.677, Validation Accuracy: 62.19%\n",
            "Epoch [10/10], Training Loss: 0.663, Validation Accuracy: 62.80%\n",
            "Epoch [1/10], Training Loss: 0.894, Validation Accuracy: 62.84%\n",
            "Epoch [2/10], Training Loss: 0.828, Validation Accuracy: 63.04%\n",
            "Epoch [3/10], Training Loss: 0.786, Validation Accuracy: 63.47%\n",
            "Epoch [4/10], Training Loss: 0.754, Validation Accuracy: 62.89%\n",
            "Epoch [5/10], Training Loss: 0.734, Validation Accuracy: 63.02%\n",
            "Epoch [6/10], Training Loss: 0.712, Validation Accuracy: 63.04%\n",
            "Epoch [7/10], Training Loss: 0.690, Validation Accuracy: 62.60%\n",
            "Epoch [8/10], Training Loss: 0.673, Validation Accuracy: 62.06%\n",
            "Epoch [9/10], Training Loss: 0.656, Validation Accuracy: 62.04%\n",
            "Epoch [10/10], Training Loss: 0.636, Validation Accuracy: 62.25%\n",
            "Epoch [1/10], Training Loss: 0.877, Validation Accuracy: 63.25%\n",
            "Epoch [2/10], Training Loss: 0.801, Validation Accuracy: 63.82%\n",
            "Epoch [3/10], Training Loss: 0.771, Validation Accuracy: 63.37%\n",
            "Epoch [4/10], Training Loss: 0.737, Validation Accuracy: 63.77%\n",
            "Epoch [5/10], Training Loss: 0.708, Validation Accuracy: 63.20%\n",
            "Epoch [6/10], Training Loss: 0.684, Validation Accuracy: 63.56%\n",
            "Epoch [7/10], Training Loss: 0.660, Validation Accuracy: 64.21%\n",
            "Epoch [8/10], Training Loss: 0.640, Validation Accuracy: 62.89%\n",
            "Epoch [9/10], Training Loss: 0.633, Validation Accuracy: 63.41%\n",
            "Epoch [10/10], Training Loss: 0.608, Validation Accuracy: 63.18%\n",
            "Epoch [1/10], Training Loss: 0.857, Validation Accuracy: 62.79%\n",
            "Epoch [2/10], Training Loss: 0.783, Validation Accuracy: 63.35%\n",
            "Epoch [3/10], Training Loss: 0.741, Validation Accuracy: 63.55%\n",
            "Epoch [4/10], Training Loss: 0.709, Validation Accuracy: 63.09%\n",
            "Epoch [5/10], Training Loss: 0.695, Validation Accuracy: 63.15%\n",
            "Epoch [6/10], Training Loss: 0.660, Validation Accuracy: 63.03%\n",
            "Epoch [7/10], Training Loss: 0.641, Validation Accuracy: 63.22%\n",
            "Epoch [8/10], Training Loss: 0.618, Validation Accuracy: 62.86%\n",
            "Epoch [9/10], Training Loss: 0.602, Validation Accuracy: 62.80%\n",
            "Epoch [10/10], Training Loss: 0.591, Validation Accuracy: 63.05%\n",
            "Epoch [1/10], Training Loss: 0.863, Validation Accuracy: 62.96%\n",
            "Epoch [2/10], Training Loss: 0.784, Validation Accuracy: 62.40%\n",
            "Epoch [3/10], Training Loss: 0.739, Validation Accuracy: 62.89%\n",
            "Epoch [4/10], Training Loss: 0.706, Validation Accuracy: 63.21%\n",
            "Epoch [5/10], Training Loss: 0.678, Validation Accuracy: 62.95%\n",
            "Epoch [6/10], Training Loss: 0.660, Validation Accuracy: 63.12%\n",
            "Epoch [7/10], Training Loss: 0.630, Validation Accuracy: 62.27%\n",
            "Epoch [8/10], Training Loss: 0.625, Validation Accuracy: 62.31%\n",
            "Epoch [9/10], Training Loss: 0.598, Validation Accuracy: 62.46%\n",
            "Epoch [10/10], Training Loss: 0.587, Validation Accuracy: 62.94%\n",
            "Epoch [1/10], Training Loss: 0.855, Validation Accuracy: 62.16%\n",
            "Epoch [2/10], Training Loss: 0.771, Validation Accuracy: 62.38%\n",
            "Epoch [3/10], Training Loss: 0.725, Validation Accuracy: 62.32%\n",
            "Epoch [4/10], Training Loss: 0.701, Validation Accuracy: 62.67%\n",
            "Epoch [5/10], Training Loss: 0.665, Validation Accuracy: 62.92%\n",
            "Epoch [6/10], Training Loss: 0.641, Validation Accuracy: 63.08%\n",
            "Epoch [7/10], Training Loss: 0.624, Validation Accuracy: 62.55%\n",
            "Epoch [8/10], Training Loss: 0.606, Validation Accuracy: 62.39%\n",
            "Epoch [9/10], Training Loss: 0.595, Validation Accuracy: 62.75%\n",
            "Epoch [10/10], Training Loss: 0.567, Validation Accuracy: 62.76%\n",
            "Epoch [1/10], Training Loss: 0.848, Validation Accuracy: 62.22%\n",
            "Epoch [2/10], Training Loss: 0.761, Validation Accuracy: 63.41%\n",
            "Epoch [3/10], Training Loss: 0.701, Validation Accuracy: 62.57%\n",
            "Epoch [4/10], Training Loss: 0.683, Validation Accuracy: 62.72%\n",
            "Epoch [5/10], Training Loss: 0.648, Validation Accuracy: 62.64%\n",
            "Epoch [6/10], Training Loss: 0.618, Validation Accuracy: 62.52%\n",
            "Epoch [7/10], Training Loss: 0.606, Validation Accuracy: 61.68%\n",
            "Epoch [8/10], Training Loss: 0.587, Validation Accuracy: 62.20%\n",
            "Epoch [9/10], Training Loss: 0.564, Validation Accuracy: 62.02%\n",
            "Epoch [10/10], Training Loss: 0.544, Validation Accuracy: 62.78%\n",
            "Epoch [1/10], Training Loss: 0.831, Validation Accuracy: 62.28%\n",
            "Epoch [2/10], Training Loss: 0.736, Validation Accuracy: 63.68%\n",
            "Epoch [3/10], Training Loss: 0.691, Validation Accuracy: 63.26%\n",
            "Epoch [4/10], Training Loss: 0.652, Validation Accuracy: 62.42%\n",
            "Epoch [5/10], Training Loss: 0.636, Validation Accuracy: 63.62%\n",
            "Epoch [6/10], Training Loss: 0.612, Validation Accuracy: 63.72%\n",
            "Epoch [7/10], Training Loss: 0.584, Validation Accuracy: 63.54%\n",
            "Epoch [8/10], Training Loss: 0.553, Validation Accuracy: 63.75%\n",
            "Epoch [9/10], Training Loss: 0.541, Validation Accuracy: 63.86%\n",
            "Epoch [10/10], Training Loss: 0.517, Validation Accuracy: 63.65%\n",
            "Epoch [1/10], Training Loss: 0.812, Validation Accuracy: 63.43%\n",
            "Epoch [2/10], Training Loss: 0.722, Validation Accuracy: 62.98%\n",
            "Epoch [3/10], Training Loss: 0.670, Validation Accuracy: 62.39%\n",
            "Epoch [4/10], Training Loss: 0.631, Validation Accuracy: 63.41%\n",
            "Epoch [5/10], Training Loss: 0.601, Validation Accuracy: 63.28%\n",
            "Epoch [6/10], Training Loss: 0.581, Validation Accuracy: 63.32%\n",
            "Epoch [7/10], Training Loss: 0.560, Validation Accuracy: 62.90%\n",
            "Epoch [8/10], Training Loss: 0.539, Validation Accuracy: 62.71%\n",
            "Epoch [9/10], Training Loss: 0.515, Validation Accuracy: 62.84%\n",
            "Epoch [10/10], Training Loss: 0.492, Validation Accuracy: 62.60%\n",
            "Epoch [1/10], Training Loss: 0.812, Validation Accuracy: 63.21%\n",
            "Epoch [2/10], Training Loss: 0.710, Validation Accuracy: 63.39%\n",
            "Epoch [3/10], Training Loss: 0.673, Validation Accuracy: 62.60%\n",
            "Epoch [4/10], Training Loss: 0.635, Validation Accuracy: 63.07%\n",
            "Epoch [5/10], Training Loss: 0.593, Validation Accuracy: 62.64%\n",
            "Epoch [6/10], Training Loss: 0.576, Validation Accuracy: 62.98%\n",
            "Epoch [7/10], Training Loss: 0.547, Validation Accuracy: 62.79%\n",
            "Epoch [8/10], Training Loss: 0.527, Validation Accuracy: 62.72%\n",
            "Epoch [9/10], Training Loss: 0.512, Validation Accuracy: 62.38%\n",
            "Epoch [10/10], Training Loss: 0.492, Validation Accuracy: 62.76%\n",
            "Epoch [1/10], Training Loss: 0.813, Validation Accuracy: 62.41%\n",
            "Epoch [2/10], Training Loss: 0.711, Validation Accuracy: 62.81%\n",
            "Epoch [3/10], Training Loss: 0.659, Validation Accuracy: 62.83%\n",
            "Epoch [4/10], Training Loss: 0.622, Validation Accuracy: 61.81%\n",
            "Epoch [5/10], Training Loss: 0.594, Validation Accuracy: 62.93%\n",
            "Epoch [6/10], Training Loss: 0.570, Validation Accuracy: 62.40%\n",
            "Epoch [7/10], Training Loss: 0.540, Validation Accuracy: 62.93%\n",
            "Epoch [8/10], Training Loss: 0.519, Validation Accuracy: 62.82%\n",
            "Epoch [9/10], Training Loss: 0.500, Validation Accuracy: 62.22%\n",
            "Epoch [10/10], Training Loss: 0.484, Validation Accuracy: 62.58%\n",
            "Epoch [1/10], Training Loss: 0.806, Validation Accuracy: 62.42%\n",
            "Epoch [2/10], Training Loss: 0.701, Validation Accuracy: 62.21%\n",
            "Epoch [3/10], Training Loss: 0.646, Validation Accuracy: 63.09%\n",
            "Epoch [4/10], Training Loss: 0.599, Validation Accuracy: 62.76%\n",
            "Epoch [5/10], Training Loss: 0.563, Validation Accuracy: 62.37%\n",
            "Epoch [6/10], Training Loss: 0.546, Validation Accuracy: 63.00%\n",
            "Epoch [7/10], Training Loss: 0.514, Validation Accuracy: 61.95%\n",
            "Epoch [8/10], Training Loss: 0.489, Validation Accuracy: 62.74%\n",
            "Epoch [9/10], Training Loss: 0.476, Validation Accuracy: 62.01%\n",
            "Epoch [10/10], Training Loss: 0.452, Validation Accuracy: 62.55%\n",
            "Epoch [1/10], Training Loss: 0.785, Validation Accuracy: 62.76%\n",
            "Epoch [2/10], Training Loss: 0.679, Validation Accuracy: 62.89%\n",
            "Epoch [3/10], Training Loss: 0.616, Validation Accuracy: 63.67%\n",
            "Epoch [4/10], Training Loss: 0.574, Validation Accuracy: 63.56%\n",
            "Epoch [5/10], Training Loss: 0.548, Validation Accuracy: 62.54%\n",
            "Epoch [6/10], Training Loss: 0.522, Validation Accuracy: 63.36%\n",
            "Epoch [7/10], Training Loss: 0.488, Validation Accuracy: 63.33%\n",
            "Epoch [8/10], Training Loss: 0.475, Validation Accuracy: 63.58%\n",
            "Epoch [9/10], Training Loss: 0.465, Validation Accuracy: 62.77%\n",
            "Epoch [10/10], Training Loss: 0.428, Validation Accuracy: 63.09%\n",
            "Epoch [1/10], Training Loss: 0.765, Validation Accuracy: 62.65%\n",
            "Epoch [2/10], Training Loss: 0.663, Validation Accuracy: 63.36%\n",
            "Epoch [3/10], Training Loss: 0.598, Validation Accuracy: 62.96%\n",
            "Epoch [4/10], Training Loss: 0.564, Validation Accuracy: 62.80%\n",
            "Epoch [5/10], Training Loss: 0.523, Validation Accuracy: 62.14%\n",
            "Epoch [6/10], Training Loss: 0.493, Validation Accuracy: 63.05%\n",
            "Epoch [7/10], Training Loss: 0.472, Validation Accuracy: 62.61%\n",
            "Epoch [8/10], Training Loss: 0.459, Validation Accuracy: 62.67%\n",
            "Epoch [9/10], Training Loss: 0.434, Validation Accuracy: 62.14%\n",
            "Epoch [10/10], Training Loss: 0.418, Validation Accuracy: 62.34%\n",
            "Epoch [1/10], Training Loss: 0.790, Validation Accuracy: 62.16%\n",
            "Epoch [2/10], Training Loss: 0.648, Validation Accuracy: 62.52%\n",
            "Epoch [3/10], Training Loss: 0.591, Validation Accuracy: 63.49%\n",
            "Epoch [4/10], Training Loss: 0.545, Validation Accuracy: 62.51%\n",
            "Epoch [5/10], Training Loss: 0.512, Validation Accuracy: 62.24%\n",
            "Epoch [6/10], Training Loss: 0.490, Validation Accuracy: 62.13%\n",
            "Epoch [7/10], Training Loss: 0.459, Validation Accuracy: 62.72%\n",
            "Epoch [8/10], Training Loss: 0.445, Validation Accuracy: 62.43%\n",
            "Epoch [9/10], Training Loss: 0.422, Validation Accuracy: 62.43%\n",
            "Epoch [10/10], Training Loss: 0.397, Validation Accuracy: 61.95%\n",
            "Confusion Matrix:\n",
            "[[650  33 101  34  28   5  15   8  84  42]\n",
            " [ 23 766  14  16   5   6   6   7  45 112]\n",
            " [ 45  10 543 105  92  53  75  41  20  16]\n",
            " [ 13  14 114 496  64 125  83  45  24  22]\n",
            " [ 23   2 115  99 537  39  63  97  22   3]\n",
            " [ 15   5  86 245  60 458  35  79   9   8]\n",
            " [ 12   7  60  93  68  23 698  22  11   6]\n",
            " [ 19   8  68  64  75  66  10 670   2  18]\n",
            " [103  52  23  34  15   4   7   7 725  30]\n",
            " [ 54 124  16  38  15   8   9  32  55 649]]\n",
            "Test Accuracy: 61.92%\n",
            "True Positives (TP): [650 766 543 496 537 458 698 670 725 649]\n",
            "False Positives (FP): [307 255 597 728 422 329 303 338 272 257]\n",
            "True Negatives (TN): [8693 8745 8403 8272 8578 8671 8697 8662 8728 8743]\n",
            "False Negatives (FN): [350 234 457 504 463 542 302 330 275 351]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.67920585 0.75024486 0.47631579 0.40522876 0.55995829 0.5819568\n",
            " 0.6973027  0.66468254 0.72718154 0.71633554]\n",
            "Recall: [0.65  0.766 0.543 0.496 0.537 0.458 0.698 0.67  0.725 0.649]\n",
            "F1 Score: [0.66428206 0.75804057 0.50747664 0.44604317 0.5482389  0.51259093\n",
            " 0.69765117 0.66733068 0.72608913 0.68100735]\n",
            "CPU times: user 3h 2min 12s, sys: 1min 17s, total: 3h 3min 30s\n",
            "Wall time: 3h 18min 10s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 0.5):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=0.5):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_normal: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = ( augmented_data_normal + augmented_data_truncated) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=0.5)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"normal\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs-pp8pRSXy-"
      },
      "source": [
        "Beta=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-D7fSt4SZbw",
        "outputId": "be52663b-8bdf-4ba9-88a6-7daab38c0277"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 61.2MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Images per Class: [5960 6073 5991 6056 6048 5929 6006 5973 5940 6024]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<timed exec>:281: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Training Loss: 2.304, Validation Accuracy: 7.90%\n",
            "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 9.24%\n",
            "Epoch [3/10], Training Loss: 2.303, Validation Accuracy: 10.46%\n",
            "Epoch [4/10], Training Loss: 2.302, Validation Accuracy: 11.80%\n",
            "Epoch [5/10], Training Loss: 2.302, Validation Accuracy: 12.91%\n",
            "Epoch [6/10], Training Loss: 2.301, Validation Accuracy: 13.52%\n",
            "Epoch [7/10], Training Loss: 2.300, Validation Accuracy: 13.52%\n",
            "Epoch [8/10], Training Loss: 2.299, Validation Accuracy: 14.20%\n",
            "Epoch [9/10], Training Loss: 2.298, Validation Accuracy: 14.56%\n",
            "Epoch [10/10], Training Loss: 2.297, Validation Accuracy: 14.87%\n",
            "Epoch [1/10], Training Loss: 2.297, Validation Accuracy: 16.06%\n",
            "Epoch [2/10], Training Loss: 2.295, Validation Accuracy: 16.74%\n",
            "Epoch [3/10], Training Loss: 2.292, Validation Accuracy: 17.41%\n",
            "Epoch [4/10], Training Loss: 2.289, Validation Accuracy: 17.58%\n",
            "Epoch [5/10], Training Loss: 2.284, Validation Accuracy: 16.10%\n",
            "Epoch [6/10], Training Loss: 2.276, Validation Accuracy: 15.82%\n",
            "Epoch [7/10], Training Loss: 2.264, Validation Accuracy: 15.54%\n",
            "Epoch [8/10], Training Loss: 2.246, Validation Accuracy: 17.51%\n",
            "Epoch [9/10], Training Loss: 2.220, Validation Accuracy: 18.58%\n",
            "Epoch [10/10], Training Loss: 2.188, Validation Accuracy: 19.59%\n",
            "Epoch [1/10], Training Loss: 2.147, Validation Accuracy: 20.97%\n",
            "Epoch [2/10], Training Loss: 2.111, Validation Accuracy: 23.73%\n",
            "Epoch [3/10], Training Loss: 2.084, Validation Accuracy: 23.70%\n",
            "Epoch [4/10], Training Loss: 2.061, Validation Accuracy: 24.81%\n",
            "Epoch [5/10], Training Loss: 2.043, Validation Accuracy: 25.87%\n",
            "Epoch [6/10], Training Loss: 2.027, Validation Accuracy: 25.86%\n",
            "Epoch [7/10], Training Loss: 2.012, Validation Accuracy: 27.09%\n",
            "Epoch [8/10], Training Loss: 2.000, Validation Accuracy: 27.11%\n",
            "Epoch [9/10], Training Loss: 1.986, Validation Accuracy: 27.26%\n",
            "Epoch [10/10], Training Loss: 1.973, Validation Accuracy: 27.70%\n",
            "Epoch [1/10], Training Loss: 1.963, Validation Accuracy: 27.94%\n",
            "Epoch [2/10], Training Loss: 1.951, Validation Accuracy: 28.84%\n",
            "Epoch [3/10], Training Loss: 1.940, Validation Accuracy: 29.20%\n",
            "Epoch [4/10], Training Loss: 1.931, Validation Accuracy: 29.16%\n",
            "Epoch [5/10], Training Loss: 1.917, Validation Accuracy: 29.85%\n",
            "Epoch [6/10], Training Loss: 1.907, Validation Accuracy: 30.17%\n",
            "Epoch [7/10], Training Loss: 1.898, Validation Accuracy: 30.95%\n",
            "Epoch [8/10], Training Loss: 1.885, Validation Accuracy: 31.86%\n",
            "Epoch [9/10], Training Loss: 1.870, Validation Accuracy: 31.89%\n",
            "Epoch [10/10], Training Loss: 1.861, Validation Accuracy: 32.13%\n",
            "Epoch [1/10], Training Loss: 1.860, Validation Accuracy: 32.09%\n",
            "Epoch [2/10], Training Loss: 1.847, Validation Accuracy: 33.81%\n",
            "Epoch [3/10], Training Loss: 1.829, Validation Accuracy: 34.18%\n",
            "Epoch [4/10], Training Loss: 1.811, Validation Accuracy: 35.09%\n",
            "Epoch [5/10], Training Loss: 1.794, Validation Accuracy: 35.47%\n",
            "Epoch [6/10], Training Loss: 1.779, Validation Accuracy: 36.31%\n",
            "Epoch [7/10], Training Loss: 1.759, Validation Accuracy: 35.98%\n",
            "Epoch [8/10], Training Loss: 1.743, Validation Accuracy: 36.60%\n",
            "Epoch [9/10], Training Loss: 1.727, Validation Accuracy: 37.28%\n",
            "Epoch [10/10], Training Loss: 1.709, Validation Accuracy: 37.75%\n",
            "Epoch [1/10], Training Loss: 1.706, Validation Accuracy: 38.60%\n",
            "Epoch [2/10], Training Loss: 1.686, Validation Accuracy: 39.51%\n",
            "Epoch [3/10], Training Loss: 1.669, Validation Accuracy: 39.88%\n",
            "Epoch [4/10], Training Loss: 1.650, Validation Accuracy: 39.64%\n",
            "Epoch [5/10], Training Loss: 1.637, Validation Accuracy: 39.27%\n",
            "Epoch [6/10], Training Loss: 1.620, Validation Accuracy: 40.43%\n",
            "Epoch [7/10], Training Loss: 1.599, Validation Accuracy: 41.41%\n",
            "Epoch [8/10], Training Loss: 1.591, Validation Accuracy: 42.01%\n",
            "Epoch [9/10], Training Loss: 1.572, Validation Accuracy: 41.79%\n",
            "Epoch [10/10], Training Loss: 1.561, Validation Accuracy: 42.63%\n",
            "Epoch [1/10], Training Loss: 1.603, Validation Accuracy: 42.76%\n",
            "Epoch [2/10], Training Loss: 1.587, Validation Accuracy: 43.57%\n",
            "Epoch [3/10], Training Loss: 1.566, Validation Accuracy: 44.00%\n",
            "Epoch [4/10], Training Loss: 1.550, Validation Accuracy: 44.06%\n",
            "Epoch [5/10], Training Loss: 1.539, Validation Accuracy: 44.80%\n",
            "Epoch [6/10], Training Loss: 1.528, Validation Accuracy: 45.11%\n",
            "Epoch [7/10], Training Loss: 1.520, Validation Accuracy: 45.55%\n",
            "Epoch [8/10], Training Loss: 1.504, Validation Accuracy: 45.22%\n",
            "Epoch [9/10], Training Loss: 1.497, Validation Accuracy: 44.95%\n",
            "Epoch [10/10], Training Loss: 1.494, Validation Accuracy: 46.34%\n",
            "Epoch [1/10], Training Loss: 1.508, Validation Accuracy: 46.42%\n",
            "Epoch [2/10], Training Loss: 1.487, Validation Accuracy: 46.55%\n",
            "Epoch [3/10], Training Loss: 1.473, Validation Accuracy: 46.97%\n",
            "Epoch [4/10], Training Loss: 1.465, Validation Accuracy: 47.06%\n",
            "Epoch [5/10], Training Loss: 1.455, Validation Accuracy: 46.95%\n",
            "Epoch [6/10], Training Loss: 1.445, Validation Accuracy: 47.35%\n",
            "Epoch [7/10], Training Loss: 1.438, Validation Accuracy: 47.82%\n",
            "Epoch [8/10], Training Loss: 1.428, Validation Accuracy: 48.10%\n",
            "Epoch [9/10], Training Loss: 1.414, Validation Accuracy: 47.42%\n",
            "Epoch [10/10], Training Loss: 1.411, Validation Accuracy: 47.34%\n",
            "Epoch [1/10], Training Loss: 1.435, Validation Accuracy: 48.39%\n",
            "Epoch [2/10], Training Loss: 1.418, Validation Accuracy: 48.62%\n",
            "Epoch [3/10], Training Loss: 1.404, Validation Accuracy: 47.90%\n",
            "Epoch [4/10], Training Loss: 1.398, Validation Accuracy: 48.83%\n",
            "Epoch [5/10], Training Loss: 1.384, Validation Accuracy: 48.69%\n",
            "Epoch [6/10], Training Loss: 1.385, Validation Accuracy: 48.64%\n",
            "Epoch [7/10], Training Loss: 1.368, Validation Accuracy: 50.30%\n",
            "Epoch [8/10], Training Loss: 1.363, Validation Accuracy: 49.22%\n",
            "Epoch [9/10], Training Loss: 1.352, Validation Accuracy: 49.30%\n",
            "Epoch [10/10], Training Loss: 1.355, Validation Accuracy: 49.48%\n",
            "Epoch [1/10], Training Loss: 1.406, Validation Accuracy: 49.91%\n",
            "Epoch [2/10], Training Loss: 1.387, Validation Accuracy: 50.00%\n",
            "Epoch [3/10], Training Loss: 1.376, Validation Accuracy: 50.67%\n",
            "Epoch [4/10], Training Loss: 1.364, Validation Accuracy: 50.95%\n",
            "Epoch [5/10], Training Loss: 1.352, Validation Accuracy: 50.75%\n",
            "Epoch [6/10], Training Loss: 1.349, Validation Accuracy: 49.75%\n",
            "Epoch [7/10], Training Loss: 1.342, Validation Accuracy: 50.91%\n",
            "Epoch [8/10], Training Loss: 1.327, Validation Accuracy: 50.73%\n",
            "Epoch [9/10], Training Loss: 1.322, Validation Accuracy: 50.49%\n",
            "Epoch [10/10], Training Loss: 1.321, Validation Accuracy: 50.76%\n",
            "Epoch [1/10], Training Loss: 1.356, Validation Accuracy: 51.86%\n",
            "Epoch [2/10], Training Loss: 1.327, Validation Accuracy: 51.14%\n",
            "Epoch [3/10], Training Loss: 1.320, Validation Accuracy: 51.69%\n",
            "Epoch [4/10], Training Loss: 1.305, Validation Accuracy: 51.82%\n",
            "Epoch [5/10], Training Loss: 1.297, Validation Accuracy: 51.79%\n",
            "Epoch [6/10], Training Loss: 1.281, Validation Accuracy: 52.88%\n",
            "Epoch [7/10], Training Loss: 1.276, Validation Accuracy: 52.08%\n",
            "Epoch [8/10], Training Loss: 1.268, Validation Accuracy: 52.55%\n",
            "Epoch [9/10], Training Loss: 1.255, Validation Accuracy: 52.61%\n",
            "Epoch [10/10], Training Loss: 1.248, Validation Accuracy: 52.26%\n",
            "Epoch [1/10], Training Loss: 1.344, Validation Accuracy: 52.72%\n",
            "Epoch [2/10], Training Loss: 1.321, Validation Accuracy: 52.77%\n",
            "Epoch [3/10], Training Loss: 1.307, Validation Accuracy: 52.69%\n",
            "Epoch [4/10], Training Loss: 1.296, Validation Accuracy: 52.34%\n",
            "Epoch [5/10], Training Loss: 1.300, Validation Accuracy: 53.09%\n",
            "Epoch [6/10], Training Loss: 1.276, Validation Accuracy: 52.88%\n",
            "Epoch [7/10], Training Loss: 1.265, Validation Accuracy: 52.87%\n",
            "Epoch [8/10], Training Loss: 1.258, Validation Accuracy: 53.04%\n",
            "Epoch [9/10], Training Loss: 1.252, Validation Accuracy: 52.33%\n",
            "Epoch [10/10], Training Loss: 1.242, Validation Accuracy: 54.13%\n",
            "Epoch [1/10], Training Loss: 1.292, Validation Accuracy: 53.53%\n",
            "Epoch [2/10], Training Loss: 1.268, Validation Accuracy: 54.04%\n",
            "Epoch [3/10], Training Loss: 1.262, Validation Accuracy: 53.71%\n",
            "Epoch [4/10], Training Loss: 1.247, Validation Accuracy: 54.18%\n",
            "Epoch [5/10], Training Loss: 1.238, Validation Accuracy: 54.01%\n",
            "Epoch [6/10], Training Loss: 1.230, Validation Accuracy: 54.17%\n",
            "Epoch [7/10], Training Loss: 1.216, Validation Accuracy: 54.84%\n",
            "Epoch [8/10], Training Loss: 1.209, Validation Accuracy: 54.53%\n",
            "Epoch [9/10], Training Loss: 1.202, Validation Accuracy: 54.36%\n",
            "Epoch [10/10], Training Loss: 1.199, Validation Accuracy: 54.61%\n",
            "Epoch [1/10], Training Loss: 1.268, Validation Accuracy: 53.68%\n",
            "Epoch [2/10], Training Loss: 1.243, Validation Accuracy: 54.15%\n",
            "Epoch [3/10], Training Loss: 1.229, Validation Accuracy: 54.76%\n",
            "Epoch [4/10], Training Loss: 1.222, Validation Accuracy: 55.36%\n",
            "Epoch [5/10], Training Loss: 1.199, Validation Accuracy: 55.39%\n",
            "Epoch [6/10], Training Loss: 1.194, Validation Accuracy: 54.63%\n",
            "Epoch [7/10], Training Loss: 1.187, Validation Accuracy: 55.16%\n",
            "Epoch [8/10], Training Loss: 1.178, Validation Accuracy: 54.37%\n",
            "Epoch [9/10], Training Loss: 1.171, Validation Accuracy: 55.13%\n",
            "Epoch [10/10], Training Loss: 1.158, Validation Accuracy: 54.96%\n",
            "Epoch [1/10], Training Loss: 1.250, Validation Accuracy: 55.70%\n",
            "Epoch [2/10], Training Loss: 1.216, Validation Accuracy: 55.98%\n",
            "Epoch [3/10], Training Loss: 1.202, Validation Accuracy: 56.00%\n",
            "Epoch [4/10], Training Loss: 1.197, Validation Accuracy: 55.87%\n",
            "Epoch [5/10], Training Loss: 1.175, Validation Accuracy: 56.25%\n",
            "Epoch [6/10], Training Loss: 1.173, Validation Accuracy: 55.67%\n",
            "Epoch [7/10], Training Loss: 1.162, Validation Accuracy: 55.87%\n",
            "Epoch [8/10], Training Loss: 1.149, Validation Accuracy: 55.77%\n",
            "Epoch [9/10], Training Loss: 1.138, Validation Accuracy: 55.04%\n",
            "Epoch [10/10], Training Loss: 1.134, Validation Accuracy: 56.10%\n",
            "Epoch [1/10], Training Loss: 1.200, Validation Accuracy: 56.12%\n",
            "Epoch [2/10], Training Loss: 1.169, Validation Accuracy: 56.62%\n",
            "Epoch [3/10], Training Loss: 1.160, Validation Accuracy: 55.96%\n",
            "Epoch [4/10], Training Loss: 1.144, Validation Accuracy: 57.23%\n",
            "Epoch [5/10], Training Loss: 1.133, Validation Accuracy: 56.58%\n",
            "Epoch [6/10], Training Loss: 1.120, Validation Accuracy: 56.76%\n",
            "Epoch [7/10], Training Loss: 1.106, Validation Accuracy: 56.32%\n",
            "Epoch [8/10], Training Loss: 1.105, Validation Accuracy: 56.49%\n",
            "Epoch [9/10], Training Loss: 1.094, Validation Accuracy: 57.53%\n",
            "Epoch [10/10], Training Loss: 1.077, Validation Accuracy: 56.98%\n",
            "Epoch [1/10], Training Loss: 1.209, Validation Accuracy: 56.81%\n",
            "Epoch [2/10], Training Loss: 1.184, Validation Accuracy: 56.94%\n",
            "Epoch [3/10], Training Loss: 1.163, Validation Accuracy: 56.16%\n",
            "Epoch [4/10], Training Loss: 1.148, Validation Accuracy: 57.16%\n",
            "Epoch [5/10], Training Loss: 1.136, Validation Accuracy: 57.42%\n",
            "Epoch [6/10], Training Loss: 1.119, Validation Accuracy: 56.96%\n",
            "Epoch [7/10], Training Loss: 1.114, Validation Accuracy: 56.93%\n",
            "Epoch [8/10], Training Loss: 1.094, Validation Accuracy: 56.88%\n",
            "Epoch [9/10], Training Loss: 1.094, Validation Accuracy: 56.98%\n",
            "Epoch [10/10], Training Loss: 1.082, Validation Accuracy: 57.45%\n",
            "Epoch [1/10], Training Loss: 1.180, Validation Accuracy: 57.76%\n",
            "Epoch [2/10], Training Loss: 1.147, Validation Accuracy: 57.32%\n",
            "Epoch [3/10], Training Loss: 1.126, Validation Accuracy: 58.19%\n",
            "Epoch [4/10], Training Loss: 1.111, Validation Accuracy: 58.32%\n",
            "Epoch [5/10], Training Loss: 1.092, Validation Accuracy: 57.53%\n",
            "Epoch [6/10], Training Loss: 1.089, Validation Accuracy: 58.00%\n",
            "Epoch [7/10], Training Loss: 1.072, Validation Accuracy: 57.80%\n",
            "Epoch [8/10], Training Loss: 1.059, Validation Accuracy: 58.22%\n",
            "Epoch [9/10], Training Loss: 1.055, Validation Accuracy: 57.91%\n",
            "Epoch [10/10], Training Loss: 1.038, Validation Accuracy: 58.12%\n",
            "Epoch [1/10], Training Loss: 1.160, Validation Accuracy: 57.92%\n",
            "Epoch [2/10], Training Loss: 1.129, Validation Accuracy: 57.50%\n",
            "Epoch [3/10], Training Loss: 1.100, Validation Accuracy: 58.75%\n",
            "Epoch [4/10], Training Loss: 1.087, Validation Accuracy: 58.43%\n",
            "Epoch [5/10], Training Loss: 1.080, Validation Accuracy: 57.81%\n",
            "Epoch [6/10], Training Loss: 1.061, Validation Accuracy: 57.67%\n",
            "Epoch [7/10], Training Loss: 1.054, Validation Accuracy: 58.70%\n",
            "Epoch [8/10], Training Loss: 1.029, Validation Accuracy: 58.21%\n",
            "Epoch [9/10], Training Loss: 1.021, Validation Accuracy: 57.69%\n",
            "Epoch [10/10], Training Loss: 1.017, Validation Accuracy: 58.22%\n",
            "Epoch [1/10], Training Loss: 1.145, Validation Accuracy: 57.54%\n",
            "Epoch [2/10], Training Loss: 1.106, Validation Accuracy: 58.76%\n",
            "Epoch [3/10], Training Loss: 1.082, Validation Accuracy: 58.66%\n",
            "Epoch [4/10], Training Loss: 1.068, Validation Accuracy: 58.71%\n",
            "Epoch [5/10], Training Loss: 1.054, Validation Accuracy: 59.01%\n",
            "Epoch [6/10], Training Loss: 1.053, Validation Accuracy: 59.05%\n",
            "Epoch [7/10], Training Loss: 1.027, Validation Accuracy: 58.63%\n",
            "Epoch [8/10], Training Loss: 1.009, Validation Accuracy: 58.84%\n",
            "Epoch [9/10], Training Loss: 1.003, Validation Accuracy: 58.51%\n",
            "Epoch [10/10], Training Loss: 0.996, Validation Accuracy: 58.81%\n",
            "Epoch [1/10], Training Loss: 1.096, Validation Accuracy: 59.40%\n",
            "Epoch [2/10], Training Loss: 1.067, Validation Accuracy: 58.72%\n",
            "Epoch [3/10], Training Loss: 1.042, Validation Accuracy: 59.60%\n",
            "Epoch [4/10], Training Loss: 1.029, Validation Accuracy: 59.05%\n",
            "Epoch [5/10], Training Loss: 1.013, Validation Accuracy: 59.32%\n",
            "Epoch [6/10], Training Loss: 1.005, Validation Accuracy: 58.70%\n",
            "Epoch [7/10], Training Loss: 0.983, Validation Accuracy: 59.19%\n",
            "Epoch [8/10], Training Loss: 0.981, Validation Accuracy: 58.39%\n",
            "Epoch [9/10], Training Loss: 0.966, Validation Accuracy: 59.41%\n",
            "Epoch [10/10], Training Loss: 0.956, Validation Accuracy: 58.75%\n",
            "Epoch [1/10], Training Loss: 1.126, Validation Accuracy: 59.59%\n",
            "Epoch [2/10], Training Loss: 1.083, Validation Accuracy: 59.98%\n",
            "Epoch [3/10], Training Loss: 1.052, Validation Accuracy: 58.78%\n",
            "Epoch [4/10], Training Loss: 1.041, Validation Accuracy: 59.01%\n",
            "Epoch [5/10], Training Loss: 1.021, Validation Accuracy: 58.32%\n",
            "Epoch [6/10], Training Loss: 1.014, Validation Accuracy: 59.99%\n",
            "Epoch [7/10], Training Loss: 0.994, Validation Accuracy: 59.31%\n",
            "Epoch [8/10], Training Loss: 0.985, Validation Accuracy: 59.28%\n",
            "Epoch [9/10], Training Loss: 0.982, Validation Accuracy: 59.25%\n",
            "Epoch [10/10], Training Loss: 0.959, Validation Accuracy: 59.71%\n",
            "Epoch [1/10], Training Loss: 1.097, Validation Accuracy: 60.32%\n",
            "Epoch [2/10], Training Loss: 1.049, Validation Accuracy: 59.79%\n",
            "Epoch [3/10], Training Loss: 1.031, Validation Accuracy: 60.07%\n",
            "Epoch [4/10], Training Loss: 1.006, Validation Accuracy: 59.95%\n",
            "Epoch [5/10], Training Loss: 0.986, Validation Accuracy: 60.09%\n",
            "Epoch [6/10], Training Loss: 0.974, Validation Accuracy: 59.99%\n",
            "Epoch [7/10], Training Loss: 0.958, Validation Accuracy: 59.57%\n",
            "Epoch [8/10], Training Loss: 0.944, Validation Accuracy: 60.27%\n",
            "Epoch [9/10], Training Loss: 0.932, Validation Accuracy: 60.01%\n",
            "Epoch [10/10], Training Loss: 0.929, Validation Accuracy: 59.89%\n",
            "Epoch [1/10], Training Loss: 1.069, Validation Accuracy: 60.59%\n",
            "Epoch [2/10], Training Loss: 1.024, Validation Accuracy: 60.32%\n",
            "Epoch [3/10], Training Loss: 1.010, Validation Accuracy: 60.08%\n",
            "Epoch [4/10], Training Loss: 0.981, Validation Accuracy: 60.32%\n",
            "Epoch [5/10], Training Loss: 0.971, Validation Accuracy: 60.51%\n",
            "Epoch [6/10], Training Loss: 0.954, Validation Accuracy: 60.61%\n",
            "Epoch [7/10], Training Loss: 0.939, Validation Accuracy: 60.16%\n",
            "Epoch [8/10], Training Loss: 0.925, Validation Accuracy: 60.54%\n",
            "Epoch [9/10], Training Loss: 0.915, Validation Accuracy: 60.11%\n",
            "Epoch [10/10], Training Loss: 0.904, Validation Accuracy: 60.47%\n",
            "Epoch [1/10], Training Loss: 1.062, Validation Accuracy: 59.69%\n",
            "Epoch [2/10], Training Loss: 1.023, Validation Accuracy: 60.48%\n",
            "Epoch [3/10], Training Loss: 0.991, Validation Accuracy: 59.51%\n",
            "Epoch [4/10], Training Loss: 0.969, Validation Accuracy: 61.22%\n",
            "Epoch [5/10], Training Loss: 0.954, Validation Accuracy: 59.08%\n",
            "Epoch [6/10], Training Loss: 0.938, Validation Accuracy: 59.92%\n",
            "Epoch [7/10], Training Loss: 0.930, Validation Accuracy: 59.29%\n",
            "Epoch [8/10], Training Loss: 0.919, Validation Accuracy: 58.62%\n",
            "Epoch [9/10], Training Loss: 0.904, Validation Accuracy: 60.53%\n",
            "Epoch [10/10], Training Loss: 0.889, Validation Accuracy: 60.30%\n",
            "Epoch [1/10], Training Loss: 1.034, Validation Accuracy: 60.22%\n",
            "Epoch [2/10], Training Loss: 0.978, Validation Accuracy: 60.66%\n",
            "Epoch [3/10], Training Loss: 0.959, Validation Accuracy: 60.62%\n",
            "Epoch [4/10], Training Loss: 0.930, Validation Accuracy: 60.80%\n",
            "Epoch [5/10], Training Loss: 0.915, Validation Accuracy: 60.70%\n",
            "Epoch [6/10], Training Loss: 0.896, Validation Accuracy: 60.90%\n",
            "Epoch [7/10], Training Loss: 0.882, Validation Accuracy: 60.89%\n",
            "Epoch [8/10], Training Loss: 0.870, Validation Accuracy: 60.44%\n",
            "Epoch [9/10], Training Loss: 0.852, Validation Accuracy: 60.82%\n",
            "Epoch [10/10], Training Loss: 0.839, Validation Accuracy: 60.64%\n",
            "Epoch [1/10], Training Loss: 1.047, Validation Accuracy: 60.59%\n",
            "Epoch [2/10], Training Loss: 0.999, Validation Accuracy: 60.91%\n",
            "Epoch [3/10], Training Loss: 0.975, Validation Accuracy: 60.96%\n",
            "Epoch [4/10], Training Loss: 0.945, Validation Accuracy: 61.18%\n",
            "Epoch [5/10], Training Loss: 0.928, Validation Accuracy: 61.17%\n",
            "Epoch [6/10], Training Loss: 0.904, Validation Accuracy: 61.02%\n",
            "Epoch [7/10], Training Loss: 0.898, Validation Accuracy: 61.12%\n",
            "Epoch [8/10], Training Loss: 0.880, Validation Accuracy: 61.46%\n",
            "Epoch [9/10], Training Loss: 0.867, Validation Accuracy: 60.46%\n",
            "Epoch [10/10], Training Loss: 0.849, Validation Accuracy: 60.65%\n",
            "Epoch [1/10], Training Loss: 1.021, Validation Accuracy: 61.17%\n",
            "Epoch [2/10], Training Loss: 0.968, Validation Accuracy: 60.27%\n",
            "Epoch [3/10], Training Loss: 0.935, Validation Accuracy: 61.57%\n",
            "Epoch [4/10], Training Loss: 0.912, Validation Accuracy: 61.20%\n",
            "Epoch [5/10], Training Loss: 0.887, Validation Accuracy: 61.53%\n",
            "Epoch [6/10], Training Loss: 0.872, Validation Accuracy: 60.41%\n",
            "Epoch [7/10], Training Loss: 0.856, Validation Accuracy: 60.98%\n",
            "Epoch [8/10], Training Loss: 0.847, Validation Accuracy: 60.61%\n",
            "Epoch [9/10], Training Loss: 0.829, Validation Accuracy: 61.02%\n",
            "Epoch [10/10], Training Loss: 0.823, Validation Accuracy: 60.57%\n",
            "Epoch [1/10], Training Loss: 1.005, Validation Accuracy: 61.05%\n",
            "Epoch [2/10], Training Loss: 0.951, Validation Accuracy: 61.67%\n",
            "Epoch [3/10], Training Loss: 0.913, Validation Accuracy: 61.55%\n",
            "Epoch [4/10], Training Loss: 0.899, Validation Accuracy: 61.86%\n",
            "Epoch [5/10], Training Loss: 0.878, Validation Accuracy: 60.68%\n",
            "Epoch [6/10], Training Loss: 0.854, Validation Accuracy: 61.42%\n",
            "Epoch [7/10], Training Loss: 0.839, Validation Accuracy: 61.61%\n",
            "Epoch [8/10], Training Loss: 0.819, Validation Accuracy: 61.29%\n",
            "Epoch [9/10], Training Loss: 0.812, Validation Accuracy: 60.46%\n",
            "Epoch [10/10], Training Loss: 0.807, Validation Accuracy: 61.11%\n",
            "Epoch [1/10], Training Loss: 1.001, Validation Accuracy: 61.24%\n",
            "Epoch [2/10], Training Loss: 0.943, Validation Accuracy: 61.66%\n",
            "Epoch [3/10], Training Loss: 0.914, Validation Accuracy: 61.73%\n",
            "Epoch [4/10], Training Loss: 0.882, Validation Accuracy: 61.19%\n",
            "Epoch [5/10], Training Loss: 0.859, Validation Accuracy: 61.16%\n",
            "Epoch [6/10], Training Loss: 0.845, Validation Accuracy: 61.15%\n",
            "Epoch [7/10], Training Loss: 0.831, Validation Accuracy: 61.37%\n",
            "Epoch [8/10], Training Loss: 0.805, Validation Accuracy: 61.22%\n",
            "Epoch [9/10], Training Loss: 0.797, Validation Accuracy: 61.23%\n",
            "Epoch [10/10], Training Loss: 0.786, Validation Accuracy: 61.41%\n",
            "Epoch [1/10], Training Loss: 0.964, Validation Accuracy: 61.24%\n",
            "Epoch [2/10], Training Loss: 0.908, Validation Accuracy: 60.32%\n",
            "Epoch [3/10], Training Loss: 0.870, Validation Accuracy: 61.02%\n",
            "Epoch [4/10], Training Loss: 0.849, Validation Accuracy: 61.54%\n",
            "Epoch [5/10], Training Loss: 0.818, Validation Accuracy: 61.28%\n",
            "Epoch [6/10], Training Loss: 0.803, Validation Accuracy: 61.71%\n",
            "Epoch [7/10], Training Loss: 0.791, Validation Accuracy: 61.64%\n",
            "Epoch [8/10], Training Loss: 0.763, Validation Accuracy: 61.10%\n",
            "Epoch [9/10], Training Loss: 0.749, Validation Accuracy: 61.13%\n",
            "Epoch [10/10], Training Loss: 0.734, Validation Accuracy: 60.53%\n",
            "Epoch [1/10], Training Loss: 1.001, Validation Accuracy: 61.69%\n",
            "Epoch [2/10], Training Loss: 0.931, Validation Accuracy: 61.80%\n",
            "Epoch [3/10], Training Loss: 0.888, Validation Accuracy: 61.22%\n",
            "Epoch [4/10], Training Loss: 0.863, Validation Accuracy: 61.78%\n",
            "Epoch [5/10], Training Loss: 0.837, Validation Accuracy: 61.23%\n",
            "Epoch [6/10], Training Loss: 0.826, Validation Accuracy: 61.08%\n",
            "Epoch [7/10], Training Loss: 0.800, Validation Accuracy: 61.40%\n",
            "Epoch [8/10], Training Loss: 0.786, Validation Accuracy: 61.51%\n",
            "Epoch [9/10], Training Loss: 0.764, Validation Accuracy: 61.22%\n",
            "Epoch [10/10], Training Loss: 0.753, Validation Accuracy: 61.52%\n",
            "Epoch [1/10], Training Loss: 0.963, Validation Accuracy: 61.64%\n",
            "Epoch [2/10], Training Loss: 0.896, Validation Accuracy: 60.43%\n",
            "Epoch [3/10], Training Loss: 0.876, Validation Accuracy: 61.03%\n",
            "Epoch [4/10], Training Loss: 0.824, Validation Accuracy: 62.11%\n",
            "Epoch [5/10], Training Loss: 0.795, Validation Accuracy: 61.83%\n",
            "Epoch [6/10], Training Loss: 0.787, Validation Accuracy: 61.92%\n",
            "Epoch [7/10], Training Loss: 0.755, Validation Accuracy: 62.16%\n",
            "Epoch [8/10], Training Loss: 0.737, Validation Accuracy: 61.22%\n",
            "Epoch [9/10], Training Loss: 0.727, Validation Accuracy: 60.90%\n",
            "Epoch [10/10], Training Loss: 0.710, Validation Accuracy: 60.92%\n",
            "Epoch [1/10], Training Loss: 0.956, Validation Accuracy: 62.07%\n",
            "Epoch [2/10], Training Loss: 0.884, Validation Accuracy: 62.32%\n",
            "Epoch [3/10], Training Loss: 0.847, Validation Accuracy: 62.12%\n",
            "Epoch [4/10], Training Loss: 0.812, Validation Accuracy: 62.59%\n",
            "Epoch [5/10], Training Loss: 0.783, Validation Accuracy: 61.80%\n",
            "Epoch [6/10], Training Loss: 0.775, Validation Accuracy: 62.00%\n",
            "Epoch [7/10], Training Loss: 0.747, Validation Accuracy: 61.83%\n",
            "Epoch [8/10], Training Loss: 0.734, Validation Accuracy: 61.44%\n",
            "Epoch [9/10], Training Loss: 0.711, Validation Accuracy: 61.85%\n",
            "Epoch [10/10], Training Loss: 0.698, Validation Accuracy: 61.48%\n",
            "Epoch [1/10], Training Loss: 0.942, Validation Accuracy: 62.02%\n",
            "Epoch [2/10], Training Loss: 0.867, Validation Accuracy: 61.98%\n",
            "Epoch [3/10], Training Loss: 0.829, Validation Accuracy: 61.13%\n",
            "Epoch [4/10], Training Loss: 0.796, Validation Accuracy: 62.26%\n",
            "Epoch [5/10], Training Loss: 0.777, Validation Accuracy: 62.42%\n",
            "Epoch [6/10], Training Loss: 0.748, Validation Accuracy: 61.48%\n",
            "Epoch [7/10], Training Loss: 0.730, Validation Accuracy: 61.92%\n",
            "Epoch [8/10], Training Loss: 0.713, Validation Accuracy: 61.39%\n",
            "Epoch [9/10], Training Loss: 0.697, Validation Accuracy: 61.08%\n",
            "Epoch [10/10], Training Loss: 0.682, Validation Accuracy: 61.25%\n",
            "Epoch [1/10], Training Loss: 0.912, Validation Accuracy: 60.13%\n",
            "Epoch [2/10], Training Loss: 0.843, Validation Accuracy: 61.76%\n",
            "Epoch [3/10], Training Loss: 0.792, Validation Accuracy: 62.01%\n",
            "Epoch [4/10], Training Loss: 0.761, Validation Accuracy: 61.58%\n",
            "Epoch [5/10], Training Loss: 0.729, Validation Accuracy: 61.77%\n",
            "Epoch [6/10], Training Loss: 0.709, Validation Accuracy: 61.73%\n",
            "Epoch [7/10], Training Loss: 0.694, Validation Accuracy: 61.37%\n",
            "Epoch [8/10], Training Loss: 0.672, Validation Accuracy: 61.72%\n",
            "Epoch [9/10], Training Loss: 0.651, Validation Accuracy: 61.29%\n",
            "Epoch [10/10], Training Loss: 0.635, Validation Accuracy: 61.50%\n",
            "Epoch [1/10], Training Loss: 0.939, Validation Accuracy: 61.32%\n",
            "Epoch [2/10], Training Loss: 0.867, Validation Accuracy: 61.27%\n",
            "Epoch [3/10], Training Loss: 0.818, Validation Accuracy: 61.63%\n",
            "Epoch [4/10], Training Loss: 0.786, Validation Accuracy: 61.78%\n",
            "Epoch [5/10], Training Loss: 0.751, Validation Accuracy: 61.67%\n",
            "Epoch [6/10], Training Loss: 0.725, Validation Accuracy: 62.05%\n",
            "Epoch [7/10], Training Loss: 0.710, Validation Accuracy: 61.63%\n",
            "Epoch [8/10], Training Loss: 0.688, Validation Accuracy: 61.70%\n",
            "Epoch [9/10], Training Loss: 0.667, Validation Accuracy: 61.35%\n",
            "Epoch [10/10], Training Loss: 0.653, Validation Accuracy: 61.29%\n",
            "Epoch [1/10], Training Loss: 0.895, Validation Accuracy: 61.23%\n",
            "Epoch [2/10], Training Loss: 0.822, Validation Accuracy: 61.27%\n",
            "Epoch [3/10], Training Loss: 0.779, Validation Accuracy: 61.29%\n",
            "Epoch [4/10], Training Loss: 0.751, Validation Accuracy: 61.65%\n",
            "Epoch [5/10], Training Loss: 0.710, Validation Accuracy: 61.98%\n",
            "Epoch [6/10], Training Loss: 0.684, Validation Accuracy: 61.91%\n",
            "Epoch [7/10], Training Loss: 0.665, Validation Accuracy: 61.17%\n",
            "Epoch [8/10], Training Loss: 0.649, Validation Accuracy: 61.61%\n",
            "Epoch [9/10], Training Loss: 0.635, Validation Accuracy: 61.29%\n",
            "Epoch [10/10], Training Loss: 0.609, Validation Accuracy: 60.85%\n",
            "Epoch [1/10], Training Loss: 0.902, Validation Accuracy: 61.53%\n",
            "Epoch [2/10], Training Loss: 0.819, Validation Accuracy: 61.46%\n",
            "Epoch [3/10], Training Loss: 0.778, Validation Accuracy: 62.18%\n",
            "Epoch [4/10], Training Loss: 0.739, Validation Accuracy: 62.61%\n",
            "Epoch [5/10], Training Loss: 0.702, Validation Accuracy: 62.48%\n",
            "Epoch [6/10], Training Loss: 0.679, Validation Accuracy: 62.28%\n",
            "Epoch [7/10], Training Loss: 0.660, Validation Accuracy: 61.97%\n",
            "Epoch [8/10], Training Loss: 0.634, Validation Accuracy: 61.66%\n",
            "Epoch [9/10], Training Loss: 0.612, Validation Accuracy: 62.05%\n",
            "Epoch [10/10], Training Loss: 0.608, Validation Accuracy: 61.98%\n",
            "Epoch [1/10], Training Loss: 0.902, Validation Accuracy: 61.89%\n",
            "Epoch [2/10], Training Loss: 0.816, Validation Accuracy: 61.94%\n",
            "Epoch [3/10], Training Loss: 0.757, Validation Accuracy: 61.62%\n",
            "Epoch [4/10], Training Loss: 0.719, Validation Accuracy: 62.28%\n",
            "Epoch [5/10], Training Loss: 0.691, Validation Accuracy: 61.51%\n",
            "Epoch [6/10], Training Loss: 0.661, Validation Accuracy: 61.57%\n",
            "Epoch [7/10], Training Loss: 0.640, Validation Accuracy: 61.87%\n",
            "Epoch [8/10], Training Loss: 0.620, Validation Accuracy: 61.49%\n",
            "Epoch [9/10], Training Loss: 0.599, Validation Accuracy: 61.67%\n",
            "Epoch [10/10], Training Loss: 0.591, Validation Accuracy: 61.14%\n",
            "Epoch [1/10], Training Loss: 0.875, Validation Accuracy: 61.66%\n",
            "Epoch [2/10], Training Loss: 0.773, Validation Accuracy: 61.76%\n",
            "Epoch [3/10], Training Loss: 0.714, Validation Accuracy: 61.74%\n",
            "Epoch [4/10], Training Loss: 0.689, Validation Accuracy: 61.75%\n",
            "Epoch [5/10], Training Loss: 0.656, Validation Accuracy: 62.06%\n",
            "Epoch [6/10], Training Loss: 0.633, Validation Accuracy: 62.02%\n",
            "Epoch [7/10], Training Loss: 0.597, Validation Accuracy: 61.71%\n",
            "Epoch [8/10], Training Loss: 0.575, Validation Accuracy: 60.86%\n",
            "Epoch [9/10], Training Loss: 0.557, Validation Accuracy: 61.51%\n",
            "Epoch [10/10], Training Loss: 0.533, Validation Accuracy: 61.39%\n",
            "Epoch [1/10], Training Loss: 0.907, Validation Accuracy: 59.82%\n",
            "Epoch [2/10], Training Loss: 0.806, Validation Accuracy: 61.54%\n",
            "Epoch [3/10], Training Loss: 0.743, Validation Accuracy: 61.09%\n",
            "Epoch [4/10], Training Loss: 0.711, Validation Accuracy: 61.74%\n",
            "Epoch [5/10], Training Loss: 0.670, Validation Accuracy: 61.69%\n",
            "Epoch [6/10], Training Loss: 0.643, Validation Accuracy: 61.20%\n",
            "Epoch [7/10], Training Loss: 0.623, Validation Accuracy: 61.92%\n",
            "Epoch [8/10], Training Loss: 0.599, Validation Accuracy: 60.79%\n",
            "Epoch [9/10], Training Loss: 0.578, Validation Accuracy: 61.61%\n",
            "Epoch [10/10], Training Loss: 0.560, Validation Accuracy: 61.00%\n",
            "Epoch [1/10], Training Loss: 0.874, Validation Accuracy: 60.88%\n",
            "Epoch [2/10], Training Loss: 0.768, Validation Accuracy: 61.33%\n",
            "Epoch [3/10], Training Loss: 0.720, Validation Accuracy: 61.46%\n",
            "Epoch [4/10], Training Loss: 0.664, Validation Accuracy: 61.42%\n",
            "Epoch [5/10], Training Loss: 0.633, Validation Accuracy: 61.67%\n",
            "Epoch [6/10], Training Loss: 0.613, Validation Accuracy: 61.68%\n",
            "Epoch [7/10], Training Loss: 0.576, Validation Accuracy: 61.51%\n",
            "Epoch [8/10], Training Loss: 0.559, Validation Accuracy: 61.12%\n",
            "Epoch [9/10], Training Loss: 0.535, Validation Accuracy: 61.05%\n",
            "Epoch [10/10], Training Loss: 0.526, Validation Accuracy: 61.31%\n",
            "Epoch [1/10], Training Loss: 0.864, Validation Accuracy: 61.33%\n",
            "Epoch [2/10], Training Loss: 0.755, Validation Accuracy: 61.71%\n",
            "Epoch [3/10], Training Loss: 0.689, Validation Accuracy: 62.18%\n",
            "Epoch [4/10], Training Loss: 0.651, Validation Accuracy: 61.48%\n",
            "Epoch [5/10], Training Loss: 0.619, Validation Accuracy: 62.29%\n",
            "Epoch [6/10], Training Loss: 0.595, Validation Accuracy: 61.52%\n",
            "Epoch [7/10], Training Loss: 0.563, Validation Accuracy: 60.91%\n",
            "Epoch [8/10], Training Loss: 0.540, Validation Accuracy: 61.88%\n",
            "Epoch [9/10], Training Loss: 0.518, Validation Accuracy: 61.71%\n",
            "Epoch [10/10], Training Loss: 0.506, Validation Accuracy: 61.35%\n",
            "Epoch [1/10], Training Loss: 0.865, Validation Accuracy: 61.83%\n",
            "Epoch [2/10], Training Loss: 0.745, Validation Accuracy: 61.94%\n",
            "Epoch [3/10], Training Loss: 0.687, Validation Accuracy: 62.21%\n",
            "Epoch [4/10], Training Loss: 0.647, Validation Accuracy: 61.83%\n",
            "Epoch [5/10], Training Loss: 0.609, Validation Accuracy: 62.76%\n",
            "Epoch [6/10], Training Loss: 0.580, Validation Accuracy: 61.83%\n",
            "Epoch [7/10], Training Loss: 0.562, Validation Accuracy: 61.82%\n",
            "Epoch [8/10], Training Loss: 0.536, Validation Accuracy: 61.38%\n",
            "Epoch [9/10], Training Loss: 0.513, Validation Accuracy: 61.62%\n",
            "Epoch [10/10], Training Loss: 0.491, Validation Accuracy: 61.45%\n",
            "Epoch [1/10], Training Loss: 0.834, Validation Accuracy: 61.13%\n",
            "Epoch [2/10], Training Loss: 0.704, Validation Accuracy: 61.95%\n",
            "Epoch [3/10], Training Loss: 0.645, Validation Accuracy: 61.69%\n",
            "Epoch [4/10], Training Loss: 0.611, Validation Accuracy: 61.48%\n",
            "Epoch [5/10], Training Loss: 0.571, Validation Accuracy: 61.74%\n",
            "Epoch [6/10], Training Loss: 0.537, Validation Accuracy: 61.16%\n",
            "Epoch [7/10], Training Loss: 0.507, Validation Accuracy: 61.65%\n",
            "Epoch [8/10], Training Loss: 0.491, Validation Accuracy: 61.74%\n",
            "Epoch [9/10], Training Loss: 0.463, Validation Accuracy: 61.22%\n",
            "Epoch [10/10], Training Loss: 0.445, Validation Accuracy: 61.74%\n",
            "Epoch [1/10], Training Loss: 0.859, Validation Accuracy: 61.44%\n",
            "Epoch [2/10], Training Loss: 0.733, Validation Accuracy: 61.56%\n",
            "Epoch [3/10], Training Loss: 0.660, Validation Accuracy: 61.14%\n",
            "Epoch [4/10], Training Loss: 0.629, Validation Accuracy: 61.23%\n",
            "Epoch [5/10], Training Loss: 0.597, Validation Accuracy: 61.39%\n",
            "Epoch [6/10], Training Loss: 0.558, Validation Accuracy: 61.47%\n",
            "Epoch [7/10], Training Loss: 0.524, Validation Accuracy: 61.49%\n",
            "Epoch [8/10], Training Loss: 0.511, Validation Accuracy: 60.82%\n",
            "Epoch [9/10], Training Loss: 0.481, Validation Accuracy: 60.83%\n",
            "Epoch [10/10], Training Loss: 0.474, Validation Accuracy: 60.92%\n",
            "Epoch [1/10], Training Loss: 0.841, Validation Accuracy: 61.18%\n",
            "Epoch [2/10], Training Loss: 0.705, Validation Accuracy: 60.84%\n",
            "Epoch [3/10], Training Loss: 0.639, Validation Accuracy: 61.46%\n",
            "Epoch [4/10], Training Loss: 0.593, Validation Accuracy: 61.70%\n",
            "Epoch [5/10], Training Loss: 0.552, Validation Accuracy: 60.99%\n",
            "Epoch [6/10], Training Loss: 0.522, Validation Accuracy: 61.36%\n",
            "Epoch [7/10], Training Loss: 0.495, Validation Accuracy: 61.64%\n",
            "Epoch [8/10], Training Loss: 0.470, Validation Accuracy: 61.35%\n",
            "Epoch [9/10], Training Loss: 0.452, Validation Accuracy: 60.77%\n",
            "Epoch [10/10], Training Loss: 0.434, Validation Accuracy: 61.38%\n",
            "Epoch [1/10], Training Loss: 0.829, Validation Accuracy: 60.76%\n",
            "Epoch [2/10], Training Loss: 0.697, Validation Accuracy: 61.86%\n",
            "Epoch [3/10], Training Loss: 0.620, Validation Accuracy: 60.47%\n",
            "Epoch [4/10], Training Loss: 0.579, Validation Accuracy: 60.61%\n",
            "Epoch [5/10], Training Loss: 0.534, Validation Accuracy: 61.23%\n",
            "Epoch [6/10], Training Loss: 0.512, Validation Accuracy: 61.50%\n",
            "Epoch [7/10], Training Loss: 0.480, Validation Accuracy: 61.52%\n",
            "Epoch [8/10], Training Loss: 0.455, Validation Accuracy: 61.18%\n",
            "Epoch [9/10], Training Loss: 0.435, Validation Accuracy: 60.64%\n",
            "Epoch [10/10], Training Loss: 0.418, Validation Accuracy: 61.62%\n",
            "Epoch [1/10], Training Loss: 0.846, Validation Accuracy: 61.45%\n",
            "Epoch [2/10], Training Loss: 0.701, Validation Accuracy: 61.39%\n",
            "Epoch [3/10], Training Loss: 0.617, Validation Accuracy: 61.25%\n",
            "Epoch [4/10], Training Loss: 0.578, Validation Accuracy: 61.05%\n",
            "Epoch [5/10], Training Loss: 0.534, Validation Accuracy: 61.97%\n",
            "Epoch [6/10], Training Loss: 0.500, Validation Accuracy: 61.27%\n",
            "Epoch [7/10], Training Loss: 0.483, Validation Accuracy: 61.37%\n",
            "Epoch [8/10], Training Loss: 0.456, Validation Accuracy: 61.22%\n",
            "Epoch [9/10], Training Loss: 0.430, Validation Accuracy: 61.40%\n",
            "Epoch [10/10], Training Loss: 0.409, Validation Accuracy: 61.34%\n",
            "Confusion Matrix:\n",
            "[[647  37  51  23  33  14  17  17 106  55]\n",
            " [ 31 726   7  11   6  13  11   8  43 144]\n",
            " [ 68  14 448  97 131 108  61  34  24  15]\n",
            " [ 33  13  68 386  85 232  75  43  27  38]\n",
            " [ 34  12  71  66 565  72  58  94  12  16]\n",
            " [ 23   9  47 182  71 542  40  58  13  15]\n",
            " [  7  14  56  81  82  46 683  14   8   9]\n",
            " [ 16   9  35  44  75  92  16 674   8  31]\n",
            " [ 82  59  18   9   9  16  10   5 745  47]\n",
            " [ 38 108  10  29  17  17   9  25  45 702]]\n",
            "Test Accuracy: 61.18%\n",
            "True Positives (TP): [647 726 448 386 565 542 683 674 745 702]\n",
            "False Positives (FP): [332 275 363 542 509 610 297 298 286 370]\n",
            "True Negatives (TN): [8668 8725 8637 8458 8491 8390 8703 8702 8714 8630]\n",
            "False Negatives (FN): [353 274 552 614 435 458 317 326 255 298]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.66087845 0.72527473 0.55240444 0.41594828 0.52607076 0.47048611\n",
            " 0.69693878 0.69341564 0.72259942 0.65485075]\n",
            "Recall: [0.647 0.726 0.448 0.386 0.565 0.542 0.683 0.674 0.745 0.702]\n",
            "F1 Score: [0.65386559 0.72563718 0.49475428 0.40041494 0.54484089 0.50371747\n",
            " 0.68989899 0.68356998 0.73362875 0.67760618]\n",
            "CPU times: user 3h 35min 8s, sys: 1min 25s, total: 3h 36min 34s\n",
            "Wall time: 3h 53min 40s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 1):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=1):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_normal: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = ( augmented_data_normal + augmented_data_truncated) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=1)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"normal\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4XCk9OeiUWK"
      },
      "source": [
        "Beta=2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XYytIYhiWPf",
        "outputId": "630bb7d5-a051-46a4-8bb1-4c6d0f8ad308"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 37.4MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Images per Class: [6070 6042 5865 5890 6177 6038 5914 6044 5905 6055]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<timed exec>:281: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Training Loss: 2.305, Validation Accuracy: 10.26%\n",
            "Epoch [2/10], Training Loss: 2.304, Validation Accuracy: 10.25%\n",
            "Epoch [3/10], Training Loss: 2.303, Validation Accuracy: 10.25%\n",
            "Epoch [4/10], Training Loss: 2.302, Validation Accuracy: 10.23%\n",
            "Epoch [5/10], Training Loss: 2.301, Validation Accuracy: 10.24%\n",
            "Epoch [6/10], Training Loss: 2.300, Validation Accuracy: 10.23%\n",
            "Epoch [7/10], Training Loss: 2.299, Validation Accuracy: 10.66%\n",
            "Epoch [8/10], Training Loss: 2.298, Validation Accuracy: 11.77%\n",
            "Epoch [9/10], Training Loss: 2.297, Validation Accuracy: 13.73%\n",
            "Epoch [10/10], Training Loss: 2.295, Validation Accuracy: 15.61%\n",
            "Epoch [1/10], Training Loss: 2.294, Validation Accuracy: 17.64%\n",
            "Epoch [2/10], Training Loss: 2.292, Validation Accuracy: 19.96%\n",
            "Epoch [3/10], Training Loss: 2.288, Validation Accuracy: 20.70%\n",
            "Epoch [4/10], Training Loss: 2.283, Validation Accuracy: 21.61%\n",
            "Epoch [5/10], Training Loss: 2.275, Validation Accuracy: 21.98%\n",
            "Epoch [6/10], Training Loss: 2.263, Validation Accuracy: 21.83%\n",
            "Epoch [7/10], Training Loss: 2.243, Validation Accuracy: 21.83%\n",
            "Epoch [8/10], Training Loss: 2.215, Validation Accuracy: 21.90%\n",
            "Epoch [9/10], Training Loss: 2.179, Validation Accuracy: 22.72%\n",
            "Epoch [10/10], Training Loss: 2.142, Validation Accuracy: 23.50%\n",
            "Epoch [1/10], Training Loss: 2.117, Validation Accuracy: 23.64%\n",
            "Epoch [2/10], Training Loss: 2.092, Validation Accuracy: 24.05%\n",
            "Epoch [3/10], Training Loss: 2.071, Validation Accuracy: 25.30%\n",
            "Epoch [4/10], Training Loss: 2.052, Validation Accuracy: 25.80%\n",
            "Epoch [5/10], Training Loss: 2.032, Validation Accuracy: 26.63%\n",
            "Epoch [6/10], Training Loss: 2.012, Validation Accuracy: 27.24%\n",
            "Epoch [7/10], Training Loss: 1.991, Validation Accuracy: 28.00%\n",
            "Epoch [8/10], Training Loss: 1.967, Validation Accuracy: 29.12%\n",
            "Epoch [9/10], Training Loss: 1.946, Validation Accuracy: 30.03%\n",
            "Epoch [10/10], Training Loss: 1.927, Validation Accuracy: 30.80%\n",
            "Epoch [1/10], Training Loss: 1.909, Validation Accuracy: 31.23%\n",
            "Epoch [2/10], Training Loss: 1.891, Validation Accuracy: 32.12%\n",
            "Epoch [3/10], Training Loss: 1.870, Validation Accuracy: 33.13%\n",
            "Epoch [4/10], Training Loss: 1.848, Validation Accuracy: 33.19%\n",
            "Epoch [5/10], Training Loss: 1.830, Validation Accuracy: 34.38%\n",
            "Epoch [6/10], Training Loss: 1.808, Validation Accuracy: 34.89%\n",
            "Epoch [7/10], Training Loss: 1.788, Validation Accuracy: 34.87%\n",
            "Epoch [8/10], Training Loss: 1.767, Validation Accuracy: 35.60%\n",
            "Epoch [9/10], Training Loss: 1.746, Validation Accuracy: 35.90%\n",
            "Epoch [10/10], Training Loss: 1.731, Validation Accuracy: 36.50%\n",
            "Epoch [1/10], Training Loss: 1.738, Validation Accuracy: 36.78%\n",
            "Epoch [2/10], Training Loss: 1.718, Validation Accuracy: 37.64%\n",
            "Epoch [3/10], Training Loss: 1.700, Validation Accuracy: 37.92%\n",
            "Epoch [4/10], Training Loss: 1.685, Validation Accuracy: 38.29%\n",
            "Epoch [5/10], Training Loss: 1.672, Validation Accuracy: 38.62%\n",
            "Epoch [6/10], Training Loss: 1.658, Validation Accuracy: 38.57%\n",
            "Epoch [7/10], Training Loss: 1.648, Validation Accuracy: 38.89%\n",
            "Epoch [8/10], Training Loss: 1.637, Validation Accuracy: 39.46%\n",
            "Epoch [9/10], Training Loss: 1.626, Validation Accuracy: 39.47%\n",
            "Epoch [10/10], Training Loss: 1.615, Validation Accuracy: 40.16%\n",
            "Epoch [1/10], Training Loss: 1.616, Validation Accuracy: 39.63%\n",
            "Epoch [2/10], Training Loss: 1.609, Validation Accuracy: 40.00%\n",
            "Epoch [3/10], Training Loss: 1.590, Validation Accuracy: 40.62%\n",
            "Epoch [4/10], Training Loss: 1.582, Validation Accuracy: 39.90%\n",
            "Epoch [5/10], Training Loss: 1.574, Validation Accuracy: 40.21%\n",
            "Epoch [6/10], Training Loss: 1.572, Validation Accuracy: 41.04%\n",
            "Epoch [7/10], Training Loss: 1.556, Validation Accuracy: 41.45%\n",
            "Epoch [8/10], Training Loss: 1.544, Validation Accuracy: 41.49%\n",
            "Epoch [9/10], Training Loss: 1.541, Validation Accuracy: 41.82%\n",
            "Epoch [10/10], Training Loss: 1.530, Validation Accuracy: 42.25%\n",
            "Epoch [1/10], Training Loss: 1.548, Validation Accuracy: 42.37%\n",
            "Epoch [2/10], Training Loss: 1.535, Validation Accuracy: 42.65%\n",
            "Epoch [3/10], Training Loss: 1.531, Validation Accuracy: 42.38%\n",
            "Epoch [4/10], Training Loss: 1.516, Validation Accuracy: 42.74%\n",
            "Epoch [5/10], Training Loss: 1.504, Validation Accuracy: 43.10%\n",
            "Epoch [6/10], Training Loss: 1.497, Validation Accuracy: 43.48%\n",
            "Epoch [7/10], Training Loss: 1.491, Validation Accuracy: 43.24%\n",
            "Epoch [8/10], Training Loss: 1.474, Validation Accuracy: 43.77%\n",
            "Epoch [9/10], Training Loss: 1.465, Validation Accuracy: 44.24%\n",
            "Epoch [10/10], Training Loss: 1.458, Validation Accuracy: 43.81%\n",
            "Epoch [1/10], Training Loss: 1.515, Validation Accuracy: 44.13%\n",
            "Epoch [2/10], Training Loss: 1.498, Validation Accuracy: 44.82%\n",
            "Epoch [3/10], Training Loss: 1.486, Validation Accuracy: 45.38%\n",
            "Epoch [4/10], Training Loss: 1.479, Validation Accuracy: 45.09%\n",
            "Epoch [5/10], Training Loss: 1.467, Validation Accuracy: 44.81%\n",
            "Epoch [6/10], Training Loss: 1.459, Validation Accuracy: 45.81%\n",
            "Epoch [7/10], Training Loss: 1.447, Validation Accuracy: 45.69%\n",
            "Epoch [8/10], Training Loss: 1.438, Validation Accuracy: 46.31%\n",
            "Epoch [9/10], Training Loss: 1.431, Validation Accuracy: 46.18%\n",
            "Epoch [10/10], Training Loss: 1.420, Validation Accuracy: 46.88%\n",
            "Epoch [1/10], Training Loss: 1.454, Validation Accuracy: 46.96%\n",
            "Epoch [2/10], Training Loss: 1.438, Validation Accuracy: 46.88%\n",
            "Epoch [3/10], Training Loss: 1.424, Validation Accuracy: 47.09%\n",
            "Epoch [4/10], Training Loss: 1.413, Validation Accuracy: 47.58%\n",
            "Epoch [5/10], Training Loss: 1.405, Validation Accuracy: 47.73%\n",
            "Epoch [6/10], Training Loss: 1.405, Validation Accuracy: 47.30%\n",
            "Epoch [7/10], Training Loss: 1.386, Validation Accuracy: 46.77%\n",
            "Epoch [8/10], Training Loss: 1.376, Validation Accuracy: 47.86%\n",
            "Epoch [9/10], Training Loss: 1.369, Validation Accuracy: 47.97%\n",
            "Epoch [10/10], Training Loss: 1.368, Validation Accuracy: 48.44%\n",
            "Epoch [1/10], Training Loss: 1.419, Validation Accuracy: 46.82%\n",
            "Epoch [2/10], Training Loss: 1.397, Validation Accuracy: 48.61%\n",
            "Epoch [3/10], Training Loss: 1.387, Validation Accuracy: 48.34%\n",
            "Epoch [4/10], Training Loss: 1.387, Validation Accuracy: 48.49%\n",
            "Epoch [5/10], Training Loss: 1.375, Validation Accuracy: 48.04%\n",
            "Epoch [6/10], Training Loss: 1.369, Validation Accuracy: 48.63%\n",
            "Epoch [7/10], Training Loss: 1.352, Validation Accuracy: 49.76%\n",
            "Epoch [8/10], Training Loss: 1.338, Validation Accuracy: 49.61%\n",
            "Epoch [9/10], Training Loss: 1.328, Validation Accuracy: 49.05%\n",
            "Epoch [10/10], Training Loss: 1.324, Validation Accuracy: 49.44%\n",
            "Epoch [1/10], Training Loss: 1.376, Validation Accuracy: 49.58%\n",
            "Epoch [2/10], Training Loss: 1.352, Validation Accuracy: 48.78%\n",
            "Epoch [3/10], Training Loss: 1.341, Validation Accuracy: 49.56%\n",
            "Epoch [4/10], Training Loss: 1.331, Validation Accuracy: 49.44%\n",
            "Epoch [5/10], Training Loss: 1.320, Validation Accuracy: 49.96%\n",
            "Epoch [6/10], Training Loss: 1.305, Validation Accuracy: 50.11%\n",
            "Epoch [7/10], Training Loss: 1.296, Validation Accuracy: 48.78%\n",
            "Epoch [8/10], Training Loss: 1.294, Validation Accuracy: 50.45%\n",
            "Epoch [9/10], Training Loss: 1.287, Validation Accuracy: 50.75%\n",
            "Epoch [10/10], Training Loss: 1.271, Validation Accuracy: 50.76%\n",
            "Epoch [1/10], Training Loss: 1.329, Validation Accuracy: 51.01%\n",
            "Epoch [2/10], Training Loss: 1.311, Validation Accuracy: 50.73%\n",
            "Epoch [3/10], Training Loss: 1.292, Validation Accuracy: 50.61%\n",
            "Epoch [4/10], Training Loss: 1.279, Validation Accuracy: 51.24%\n",
            "Epoch [5/10], Training Loss: 1.263, Validation Accuracy: 51.11%\n",
            "Epoch [6/10], Training Loss: 1.254, Validation Accuracy: 51.45%\n",
            "Epoch [7/10], Training Loss: 1.245, Validation Accuracy: 52.01%\n",
            "Epoch [8/10], Training Loss: 1.240, Validation Accuracy: 52.44%\n",
            "Epoch [9/10], Training Loss: 1.234, Validation Accuracy: 50.61%\n",
            "Epoch [10/10], Training Loss: 1.227, Validation Accuracy: 51.17%\n",
            "Epoch [1/10], Training Loss: 1.313, Validation Accuracy: 51.72%\n",
            "Epoch [2/10], Training Loss: 1.291, Validation Accuracy: 50.85%\n",
            "Epoch [3/10], Training Loss: 1.281, Validation Accuracy: 52.00%\n",
            "Epoch [4/10], Training Loss: 1.264, Validation Accuracy: 50.38%\n",
            "Epoch [5/10], Training Loss: 1.252, Validation Accuracy: 52.87%\n",
            "Epoch [6/10], Training Loss: 1.238, Validation Accuracy: 53.11%\n",
            "Epoch [7/10], Training Loss: 1.230, Validation Accuracy: 52.98%\n",
            "Epoch [8/10], Training Loss: 1.218, Validation Accuracy: 53.03%\n",
            "Epoch [9/10], Training Loss: 1.210, Validation Accuracy: 52.92%\n",
            "Epoch [10/10], Training Loss: 1.206, Validation Accuracy: 53.20%\n",
            "Epoch [1/10], Training Loss: 1.279, Validation Accuracy: 53.04%\n",
            "Epoch [2/10], Training Loss: 1.257, Validation Accuracy: 52.31%\n",
            "Epoch [3/10], Training Loss: 1.246, Validation Accuracy: 53.59%\n",
            "Epoch [4/10], Training Loss: 1.225, Validation Accuracy: 53.81%\n",
            "Epoch [5/10], Training Loss: 1.223, Validation Accuracy: 53.06%\n",
            "Epoch [6/10], Training Loss: 1.205, Validation Accuracy: 53.16%\n",
            "Epoch [7/10], Training Loss: 1.192, Validation Accuracy: 53.56%\n",
            "Epoch [8/10], Training Loss: 1.177, Validation Accuracy: 53.30%\n",
            "Epoch [9/10], Training Loss: 1.171, Validation Accuracy: 53.55%\n",
            "Epoch [10/10], Training Loss: 1.157, Validation Accuracy: 52.67%\n",
            "Epoch [1/10], Training Loss: 1.258, Validation Accuracy: 54.08%\n",
            "Epoch [2/10], Training Loss: 1.233, Validation Accuracy: 54.81%\n",
            "Epoch [3/10], Training Loss: 1.210, Validation Accuracy: 54.52%\n",
            "Epoch [4/10], Training Loss: 1.198, Validation Accuracy: 55.39%\n",
            "Epoch [5/10], Training Loss: 1.186, Validation Accuracy: 54.91%\n",
            "Epoch [6/10], Training Loss: 1.182, Validation Accuracy: 54.71%\n",
            "Epoch [7/10], Training Loss: 1.171, Validation Accuracy: 54.98%\n",
            "Epoch [8/10], Training Loss: 1.156, Validation Accuracy: 55.27%\n",
            "Epoch [9/10], Training Loss: 1.149, Validation Accuracy: 54.66%\n",
            "Epoch [10/10], Training Loss: 1.142, Validation Accuracy: 55.36%\n",
            "Epoch [1/10], Training Loss: 1.221, Validation Accuracy: 55.26%\n",
            "Epoch [2/10], Training Loss: 1.198, Validation Accuracy: 55.31%\n",
            "Epoch [3/10], Training Loss: 1.174, Validation Accuracy: 55.70%\n",
            "Epoch [4/10], Training Loss: 1.155, Validation Accuracy: 55.59%\n",
            "Epoch [5/10], Training Loss: 1.150, Validation Accuracy: 55.03%\n",
            "Epoch [6/10], Training Loss: 1.139, Validation Accuracy: 54.98%\n",
            "Epoch [7/10], Training Loss: 1.124, Validation Accuracy: 54.55%\n",
            "Epoch [8/10], Training Loss: 1.123, Validation Accuracy: 54.64%\n",
            "Epoch [9/10], Training Loss: 1.107, Validation Accuracy: 55.29%\n",
            "Epoch [10/10], Training Loss: 1.090, Validation Accuracy: 55.21%\n",
            "Epoch [1/10], Training Loss: 1.195, Validation Accuracy: 55.85%\n",
            "Epoch [2/10], Training Loss: 1.166, Validation Accuracy: 55.13%\n",
            "Epoch [3/10], Training Loss: 1.141, Validation Accuracy: 55.59%\n",
            "Epoch [4/10], Training Loss: 1.121, Validation Accuracy: 55.66%\n",
            "Epoch [5/10], Training Loss: 1.111, Validation Accuracy: 55.68%\n",
            "Epoch [6/10], Training Loss: 1.101, Validation Accuracy: 56.02%\n",
            "Epoch [7/10], Training Loss: 1.084, Validation Accuracy: 55.70%\n",
            "Epoch [8/10], Training Loss: 1.081, Validation Accuracy: 56.31%\n",
            "Epoch [9/10], Training Loss: 1.069, Validation Accuracy: 56.19%\n",
            "Epoch [10/10], Training Loss: 1.059, Validation Accuracy: 55.99%\n",
            "Epoch [1/10], Training Loss: 1.197, Validation Accuracy: 56.32%\n",
            "Epoch [2/10], Training Loss: 1.155, Validation Accuracy: 57.22%\n",
            "Epoch [3/10], Training Loss: 1.141, Validation Accuracy: 56.49%\n",
            "Epoch [4/10], Training Loss: 1.126, Validation Accuracy: 56.06%\n",
            "Epoch [5/10], Training Loss: 1.123, Validation Accuracy: 55.71%\n",
            "Epoch [6/10], Training Loss: 1.100, Validation Accuracy: 56.65%\n",
            "Epoch [7/10], Training Loss: 1.091, Validation Accuracy: 56.67%\n",
            "Epoch [8/10], Training Loss: 1.086, Validation Accuracy: 56.46%\n",
            "Epoch [9/10], Training Loss: 1.071, Validation Accuracy: 56.82%\n",
            "Epoch [10/10], Training Loss: 1.064, Validation Accuracy: 56.31%\n",
            "Epoch [1/10], Training Loss: 1.170, Validation Accuracy: 56.74%\n",
            "Epoch [2/10], Training Loss: 1.129, Validation Accuracy: 56.44%\n",
            "Epoch [3/10], Training Loss: 1.107, Validation Accuracy: 56.52%\n",
            "Epoch [4/10], Training Loss: 1.103, Validation Accuracy: 57.44%\n",
            "Epoch [5/10], Training Loss: 1.081, Validation Accuracy: 56.84%\n",
            "Epoch [6/10], Training Loss: 1.060, Validation Accuracy: 57.48%\n",
            "Epoch [7/10], Training Loss: 1.051, Validation Accuracy: 57.59%\n",
            "Epoch [8/10], Training Loss: 1.033, Validation Accuracy: 57.23%\n",
            "Epoch [9/10], Training Loss: 1.027, Validation Accuracy: 56.19%\n",
            "Epoch [10/10], Training Loss: 1.022, Validation Accuracy: 56.69%\n",
            "Epoch [1/10], Training Loss: 1.151, Validation Accuracy: 57.30%\n",
            "Epoch [2/10], Training Loss: 1.112, Validation Accuracy: 57.37%\n",
            "Epoch [3/10], Training Loss: 1.095, Validation Accuracy: 57.78%\n",
            "Epoch [4/10], Training Loss: 1.064, Validation Accuracy: 57.79%\n",
            "Epoch [5/10], Training Loss: 1.061, Validation Accuracy: 57.63%\n",
            "Epoch [6/10], Training Loss: 1.042, Validation Accuracy: 57.86%\n",
            "Epoch [7/10], Training Loss: 1.036, Validation Accuracy: 57.42%\n",
            "Epoch [8/10], Training Loss: 1.017, Validation Accuracy: 57.02%\n",
            "Epoch [9/10], Training Loss: 1.001, Validation Accuracy: 56.61%\n",
            "Epoch [10/10], Training Loss: 0.999, Validation Accuracy: 56.49%\n",
            "Epoch [1/10], Training Loss: 1.131, Validation Accuracy: 57.06%\n",
            "Epoch [2/10], Training Loss: 1.083, Validation Accuracy: 57.94%\n",
            "Epoch [3/10], Training Loss: 1.061, Validation Accuracy: 57.76%\n",
            "Epoch [4/10], Training Loss: 1.037, Validation Accuracy: 57.18%\n",
            "Epoch [5/10], Training Loss: 1.020, Validation Accuracy: 57.81%\n",
            "Epoch [6/10], Training Loss: 1.004, Validation Accuracy: 58.11%\n",
            "Epoch [7/10], Training Loss: 1.001, Validation Accuracy: 56.94%\n",
            "Epoch [8/10], Training Loss: 1.005, Validation Accuracy: 58.49%\n",
            "Epoch [9/10], Training Loss: 0.967, Validation Accuracy: 56.85%\n",
            "Epoch [10/10], Training Loss: 0.964, Validation Accuracy: 58.17%\n",
            "Epoch [1/10], Training Loss: 1.097, Validation Accuracy: 58.00%\n",
            "Epoch [2/10], Training Loss: 1.060, Validation Accuracy: 58.70%\n",
            "Epoch [3/10], Training Loss: 1.033, Validation Accuracy: 58.76%\n",
            "Epoch [4/10], Training Loss: 1.012, Validation Accuracy: 58.68%\n",
            "Epoch [5/10], Training Loss: 0.995, Validation Accuracy: 58.41%\n",
            "Epoch [6/10], Training Loss: 0.987, Validation Accuracy: 58.80%\n",
            "Epoch [7/10], Training Loss: 0.967, Validation Accuracy: 58.95%\n",
            "Epoch [8/10], Training Loss: 0.959, Validation Accuracy: 58.44%\n",
            "Epoch [9/10], Training Loss: 0.949, Validation Accuracy: 58.58%\n",
            "Epoch [10/10], Training Loss: 0.935, Validation Accuracy: 58.08%\n",
            "Epoch [1/10], Training Loss: 1.108, Validation Accuracy: 58.87%\n",
            "Epoch [2/10], Training Loss: 1.063, Validation Accuracy: 58.67%\n",
            "Epoch [3/10], Training Loss: 1.040, Validation Accuracy: 58.74%\n",
            "Epoch [4/10], Training Loss: 1.024, Validation Accuracy: 59.17%\n",
            "Epoch [5/10], Training Loss: 1.000, Validation Accuracy: 59.04%\n",
            "Epoch [6/10], Training Loss: 0.984, Validation Accuracy: 58.90%\n",
            "Epoch [7/10], Training Loss: 0.978, Validation Accuracy: 58.75%\n",
            "Epoch [8/10], Training Loss: 0.967, Validation Accuracy: 58.70%\n",
            "Epoch [9/10], Training Loss: 0.944, Validation Accuracy: 58.10%\n",
            "Epoch [10/10], Training Loss: 0.941, Validation Accuracy: 59.14%\n",
            "Epoch [1/10], Training Loss: 1.089, Validation Accuracy: 59.01%\n",
            "Epoch [2/10], Training Loss: 1.030, Validation Accuracy: 59.23%\n",
            "Epoch [3/10], Training Loss: 1.005, Validation Accuracy: 58.76%\n",
            "Epoch [4/10], Training Loss: 0.986, Validation Accuracy: 58.19%\n",
            "Epoch [5/10], Training Loss: 0.965, Validation Accuracy: 58.95%\n",
            "Epoch [6/10], Training Loss: 0.947, Validation Accuracy: 59.03%\n",
            "Epoch [7/10], Training Loss: 0.940, Validation Accuracy: 58.46%\n",
            "Epoch [8/10], Training Loss: 0.916, Validation Accuracy: 58.58%\n",
            "Epoch [9/10], Training Loss: 0.903, Validation Accuracy: 59.19%\n",
            "Epoch [10/10], Training Loss: 0.900, Validation Accuracy: 57.84%\n",
            "Epoch [1/10], Training Loss: 1.068, Validation Accuracy: 58.93%\n",
            "Epoch [2/10], Training Loss: 1.024, Validation Accuracy: 59.28%\n",
            "Epoch [3/10], Training Loss: 0.994, Validation Accuracy: 59.13%\n",
            "Epoch [4/10], Training Loss: 0.972, Validation Accuracy: 59.40%\n",
            "Epoch [5/10], Training Loss: 0.949, Validation Accuracy: 58.51%\n",
            "Epoch [6/10], Training Loss: 0.935, Validation Accuracy: 58.51%\n",
            "Epoch [7/10], Training Loss: 0.917, Validation Accuracy: 58.63%\n",
            "Epoch [8/10], Training Loss: 0.906, Validation Accuracy: 58.13%\n",
            "Epoch [9/10], Training Loss: 0.890, Validation Accuracy: 58.67%\n",
            "Epoch [10/10], Training Loss: 0.878, Validation Accuracy: 58.84%\n",
            "Epoch [1/10], Training Loss: 1.048, Validation Accuracy: 59.40%\n",
            "Epoch [2/10], Training Loss: 0.991, Validation Accuracy: 59.03%\n",
            "Epoch [3/10], Training Loss: 0.958, Validation Accuracy: 59.72%\n",
            "Epoch [4/10], Training Loss: 0.938, Validation Accuracy: 58.57%\n",
            "Epoch [5/10], Training Loss: 0.935, Validation Accuracy: 59.56%\n",
            "Epoch [6/10], Training Loss: 0.894, Validation Accuracy: 58.58%\n",
            "Epoch [7/10], Training Loss: 0.887, Validation Accuracy: 59.42%\n",
            "Epoch [8/10], Training Loss: 0.865, Validation Accuracy: 59.85%\n",
            "Epoch [9/10], Training Loss: 0.854, Validation Accuracy: 59.57%\n",
            "Epoch [10/10], Training Loss: 0.838, Validation Accuracy: 59.07%\n",
            "Epoch [1/10], Training Loss: 1.037, Validation Accuracy: 59.61%\n",
            "Epoch [2/10], Training Loss: 0.982, Validation Accuracy: 59.77%\n",
            "Epoch [3/10], Training Loss: 0.944, Validation Accuracy: 59.15%\n",
            "Epoch [4/10], Training Loss: 0.924, Validation Accuracy: 58.91%\n",
            "Epoch [5/10], Training Loss: 0.904, Validation Accuracy: 59.82%\n",
            "Epoch [6/10], Training Loss: 0.885, Validation Accuracy: 59.18%\n",
            "Epoch [7/10], Training Loss: 0.873, Validation Accuracy: 59.62%\n",
            "Epoch [8/10], Training Loss: 0.851, Validation Accuracy: 59.50%\n",
            "Epoch [9/10], Training Loss: 0.832, Validation Accuracy: 59.13%\n",
            "Epoch [10/10], Training Loss: 0.828, Validation Accuracy: 59.71%\n",
            "Epoch [1/10], Training Loss: 1.055, Validation Accuracy: 58.94%\n",
            "Epoch [2/10], Training Loss: 0.992, Validation Accuracy: 60.47%\n",
            "Epoch [3/10], Training Loss: 0.956, Validation Accuracy: 60.39%\n",
            "Epoch [4/10], Training Loss: 0.926, Validation Accuracy: 59.65%\n",
            "Epoch [5/10], Training Loss: 0.907, Validation Accuracy: 59.87%\n",
            "Epoch [6/10], Training Loss: 0.888, Validation Accuracy: 60.01%\n",
            "Epoch [7/10], Training Loss: 0.872, Validation Accuracy: 59.88%\n",
            "Epoch [8/10], Training Loss: 0.860, Validation Accuracy: 59.88%\n",
            "Epoch [9/10], Training Loss: 0.841, Validation Accuracy: 59.69%\n",
            "Epoch [10/10], Training Loss: 0.829, Validation Accuracy: 59.50%\n",
            "Epoch [1/10], Training Loss: 1.009, Validation Accuracy: 60.37%\n",
            "Epoch [2/10], Training Loss: 0.953, Validation Accuracy: 59.81%\n",
            "Epoch [3/10], Training Loss: 0.918, Validation Accuracy: 59.35%\n",
            "Epoch [4/10], Training Loss: 0.898, Validation Accuracy: 59.31%\n",
            "Epoch [5/10], Training Loss: 0.874, Validation Accuracy: 60.02%\n",
            "Epoch [6/10], Training Loss: 0.851, Validation Accuracy: 60.51%\n",
            "Epoch [7/10], Training Loss: 0.843, Validation Accuracy: 59.78%\n",
            "Epoch [8/10], Training Loss: 0.814, Validation Accuracy: 59.71%\n",
            "Epoch [9/10], Training Loss: 0.798, Validation Accuracy: 60.44%\n",
            "Epoch [10/10], Training Loss: 0.800, Validation Accuracy: 59.94%\n",
            "Epoch [1/10], Training Loss: 1.006, Validation Accuracy: 59.65%\n",
            "Epoch [2/10], Training Loss: 0.943, Validation Accuracy: 59.67%\n",
            "Epoch [3/10], Training Loss: 0.909, Validation Accuracy: 59.90%\n",
            "Epoch [4/10], Training Loss: 0.892, Validation Accuracy: 60.03%\n",
            "Epoch [5/10], Training Loss: 0.863, Validation Accuracy: 60.15%\n",
            "Epoch [6/10], Training Loss: 0.839, Validation Accuracy: 59.93%\n",
            "Epoch [7/10], Training Loss: 0.818, Validation Accuracy: 59.44%\n",
            "Epoch [8/10], Training Loss: 0.806, Validation Accuracy: 60.29%\n",
            "Epoch [9/10], Training Loss: 0.806, Validation Accuracy: 59.03%\n",
            "Epoch [10/10], Training Loss: 0.783, Validation Accuracy: 59.25%\n",
            "Epoch [1/10], Training Loss: 0.977, Validation Accuracy: 60.34%\n",
            "Epoch [2/10], Training Loss: 0.908, Validation Accuracy: 60.44%\n",
            "Epoch [3/10], Training Loss: 0.872, Validation Accuracy: 60.63%\n",
            "Epoch [4/10], Training Loss: 0.843, Validation Accuracy: 60.08%\n",
            "Epoch [5/10], Training Loss: 0.823, Validation Accuracy: 59.48%\n",
            "Epoch [6/10], Training Loss: 0.804, Validation Accuracy: 59.68%\n",
            "Epoch [7/10], Training Loss: 0.786, Validation Accuracy: 59.78%\n",
            "Epoch [8/10], Training Loss: 0.760, Validation Accuracy: 60.52%\n",
            "Epoch [9/10], Training Loss: 0.759, Validation Accuracy: 60.14%\n",
            "Epoch [10/10], Training Loss: 0.734, Validation Accuracy: 60.00%\n",
            "Epoch [1/10], Training Loss: 0.981, Validation Accuracy: 60.33%\n",
            "Epoch [2/10], Training Loss: 0.911, Validation Accuracy: 60.48%\n",
            "Epoch [3/10], Training Loss: 0.864, Validation Accuracy: 60.07%\n",
            "Epoch [4/10], Training Loss: 0.845, Validation Accuracy: 60.73%\n",
            "Epoch [5/10], Training Loss: 0.813, Validation Accuracy: 60.46%\n",
            "Epoch [6/10], Training Loss: 0.797, Validation Accuracy: 59.19%\n",
            "Epoch [7/10], Training Loss: 0.784, Validation Accuracy: 60.68%\n",
            "Epoch [8/10], Training Loss: 0.760, Validation Accuracy: 60.08%\n",
            "Epoch [9/10], Training Loss: 0.739, Validation Accuracy: 59.88%\n",
            "Epoch [10/10], Training Loss: 0.732, Validation Accuracy: 60.10%\n",
            "Epoch [1/10], Training Loss: 0.973, Validation Accuracy: 59.99%\n",
            "Epoch [2/10], Training Loss: 0.907, Validation Accuracy: 60.47%\n",
            "Epoch [3/10], Training Loss: 0.871, Validation Accuracy: 60.51%\n",
            "Epoch [4/10], Training Loss: 0.837, Validation Accuracy: 60.96%\n",
            "Epoch [5/10], Training Loss: 0.814, Validation Accuracy: 60.46%\n",
            "Epoch [6/10], Training Loss: 0.798, Validation Accuracy: 59.52%\n",
            "Epoch [7/10], Training Loss: 0.786, Validation Accuracy: 60.52%\n",
            "Epoch [8/10], Training Loss: 0.757, Validation Accuracy: 60.52%\n",
            "Epoch [9/10], Training Loss: 0.737, Validation Accuracy: 60.16%\n",
            "Epoch [10/10], Training Loss: 0.730, Validation Accuracy: 60.41%\n",
            "Epoch [1/10], Training Loss: 0.953, Validation Accuracy: 60.60%\n",
            "Epoch [2/10], Training Loss: 0.878, Validation Accuracy: 60.63%\n",
            "Epoch [3/10], Training Loss: 0.841, Validation Accuracy: 60.95%\n",
            "Epoch [4/10], Training Loss: 0.809, Validation Accuracy: 60.94%\n",
            "Epoch [5/10], Training Loss: 0.787, Validation Accuracy: 60.96%\n",
            "Epoch [6/10], Training Loss: 0.764, Validation Accuracy: 60.77%\n",
            "Epoch [7/10], Training Loss: 0.738, Validation Accuracy: 60.42%\n",
            "Epoch [8/10], Training Loss: 0.714, Validation Accuracy: 60.93%\n",
            "Epoch [9/10], Training Loss: 0.711, Validation Accuracy: 60.59%\n",
            "Epoch [10/10], Training Loss: 0.682, Validation Accuracy: 59.97%\n",
            "Epoch [1/10], Training Loss: 0.960, Validation Accuracy: 60.41%\n",
            "Epoch [2/10], Training Loss: 0.886, Validation Accuracy: 60.13%\n",
            "Epoch [3/10], Training Loss: 0.847, Validation Accuracy: 59.96%\n",
            "Epoch [4/10], Training Loss: 0.809, Validation Accuracy: 59.85%\n",
            "Epoch [5/10], Training Loss: 0.773, Validation Accuracy: 59.96%\n",
            "Epoch [6/10], Training Loss: 0.758, Validation Accuracy: 59.85%\n",
            "Epoch [7/10], Training Loss: 0.730, Validation Accuracy: 59.78%\n",
            "Epoch [8/10], Training Loss: 0.720, Validation Accuracy: 60.40%\n",
            "Epoch [9/10], Training Loss: 0.698, Validation Accuracy: 59.63%\n",
            "Epoch [10/10], Training Loss: 0.680, Validation Accuracy: 59.71%\n",
            "Epoch [1/10], Training Loss: 0.928, Validation Accuracy: 59.85%\n",
            "Epoch [2/10], Training Loss: 0.848, Validation Accuracy: 60.46%\n",
            "Epoch [3/10], Training Loss: 0.795, Validation Accuracy: 60.83%\n",
            "Epoch [4/10], Training Loss: 0.761, Validation Accuracy: 60.23%\n",
            "Epoch [5/10], Training Loss: 0.733, Validation Accuracy: 60.86%\n",
            "Epoch [6/10], Training Loss: 0.711, Validation Accuracy: 60.67%\n",
            "Epoch [7/10], Training Loss: 0.687, Validation Accuracy: 59.81%\n",
            "Epoch [8/10], Training Loss: 0.679, Validation Accuracy: 60.68%\n",
            "Epoch [9/10], Training Loss: 0.651, Validation Accuracy: 60.64%\n",
            "Epoch [10/10], Training Loss: 0.627, Validation Accuracy: 60.47%\n",
            "Epoch [1/10], Training Loss: 0.932, Validation Accuracy: 60.64%\n",
            "Epoch [2/10], Training Loss: 0.829, Validation Accuracy: 60.70%\n",
            "Epoch [3/10], Training Loss: 0.794, Validation Accuracy: 59.83%\n",
            "Epoch [4/10], Training Loss: 0.753, Validation Accuracy: 60.94%\n",
            "Epoch [5/10], Training Loss: 0.721, Validation Accuracy: 61.20%\n",
            "Epoch [6/10], Training Loss: 0.699, Validation Accuracy: 60.77%\n",
            "Epoch [7/10], Training Loss: 0.676, Validation Accuracy: 60.19%\n",
            "Epoch [8/10], Training Loss: 0.658, Validation Accuracy: 59.96%\n",
            "Epoch [9/10], Training Loss: 0.645, Validation Accuracy: 60.23%\n",
            "Epoch [10/10], Training Loss: 0.627, Validation Accuracy: 60.11%\n",
            "Epoch [1/10], Training Loss: 0.923, Validation Accuracy: 60.59%\n",
            "Epoch [2/10], Training Loss: 0.850, Validation Accuracy: 60.28%\n",
            "Epoch [3/10], Training Loss: 0.799, Validation Accuracy: 60.35%\n",
            "Epoch [4/10], Training Loss: 0.760, Validation Accuracy: 60.95%\n",
            "Epoch [5/10], Training Loss: 0.726, Validation Accuracy: 60.99%\n",
            "Epoch [6/10], Training Loss: 0.700, Validation Accuracy: 60.93%\n",
            "Epoch [7/10], Training Loss: 0.679, Validation Accuracy: 60.55%\n",
            "Epoch [8/10], Training Loss: 0.663, Validation Accuracy: 60.65%\n",
            "Epoch [9/10], Training Loss: 0.640, Validation Accuracy: 60.61%\n",
            "Epoch [10/10], Training Loss: 0.627, Validation Accuracy: 59.01%\n",
            "Epoch [1/10], Training Loss: 0.911, Validation Accuracy: 60.46%\n",
            "Epoch [2/10], Training Loss: 0.810, Validation Accuracy: 60.49%\n",
            "Epoch [3/10], Training Loss: 0.764, Validation Accuracy: 60.79%\n",
            "Epoch [4/10], Training Loss: 0.729, Validation Accuracy: 60.60%\n",
            "Epoch [5/10], Training Loss: 0.703, Validation Accuracy: 60.81%\n",
            "Epoch [6/10], Training Loss: 0.663, Validation Accuracy: 60.64%\n",
            "Epoch [7/10], Training Loss: 0.646, Validation Accuracy: 60.17%\n",
            "Epoch [8/10], Training Loss: 0.619, Validation Accuracy: 60.64%\n",
            "Epoch [9/10], Training Loss: 0.598, Validation Accuracy: 60.22%\n",
            "Epoch [10/10], Training Loss: 0.584, Validation Accuracy: 60.18%\n",
            "Epoch [1/10], Training Loss: 0.910, Validation Accuracy: 60.32%\n",
            "Epoch [2/10], Training Loss: 0.812, Validation Accuracy: 60.20%\n",
            "Epoch [3/10], Training Loss: 0.759, Validation Accuracy: 60.40%\n",
            "Epoch [4/10], Training Loss: 0.720, Validation Accuracy: 60.77%\n",
            "Epoch [5/10], Training Loss: 0.689, Validation Accuracy: 60.64%\n",
            "Epoch [6/10], Training Loss: 0.657, Validation Accuracy: 60.72%\n",
            "Epoch [7/10], Training Loss: 0.639, Validation Accuracy: 60.30%\n",
            "Epoch [8/10], Training Loss: 0.619, Validation Accuracy: 60.42%\n",
            "Epoch [9/10], Training Loss: 0.593, Validation Accuracy: 59.93%\n",
            "Epoch [10/10], Training Loss: 0.577, Validation Accuracy: 59.81%\n",
            "Epoch [1/10], Training Loss: 0.888, Validation Accuracy: 60.64%\n",
            "Epoch [2/10], Training Loss: 0.777, Validation Accuracy: 60.78%\n",
            "Epoch [3/10], Training Loss: 0.722, Validation Accuracy: 60.80%\n",
            "Epoch [4/10], Training Loss: 0.676, Validation Accuracy: 60.84%\n",
            "Epoch [5/10], Training Loss: 0.649, Validation Accuracy: 60.56%\n",
            "Epoch [6/10], Training Loss: 0.623, Validation Accuracy: 60.88%\n",
            "Epoch [7/10], Training Loss: 0.596, Validation Accuracy: 60.80%\n",
            "Epoch [8/10], Training Loss: 0.576, Validation Accuracy: 60.24%\n",
            "Epoch [9/10], Training Loss: 0.551, Validation Accuracy: 60.71%\n",
            "Epoch [10/10], Training Loss: 0.533, Validation Accuracy: 60.03%\n",
            "Epoch [1/10], Training Loss: 0.877, Validation Accuracy: 60.39%\n",
            "Epoch [2/10], Training Loss: 0.768, Validation Accuracy: 60.99%\n",
            "Epoch [3/10], Training Loss: 0.712, Validation Accuracy: 60.15%\n",
            "Epoch [4/10], Training Loss: 0.674, Validation Accuracy: 60.77%\n",
            "Epoch [5/10], Training Loss: 0.653, Validation Accuracy: 60.72%\n",
            "Epoch [6/10], Training Loss: 0.611, Validation Accuracy: 60.78%\n",
            "Epoch [7/10], Training Loss: 0.589, Validation Accuracy: 60.64%\n",
            "Epoch [8/10], Training Loss: 0.564, Validation Accuracy: 60.09%\n",
            "Epoch [9/10], Training Loss: 0.546, Validation Accuracy: 60.62%\n",
            "Epoch [10/10], Training Loss: 0.528, Validation Accuracy: 60.04%\n",
            "Epoch [1/10], Training Loss: 0.890, Validation Accuracy: 59.55%\n",
            "Epoch [2/10], Training Loss: 0.787, Validation Accuracy: 60.66%\n",
            "Epoch [3/10], Training Loss: 0.723, Validation Accuracy: 60.34%\n",
            "Epoch [4/10], Training Loss: 0.675, Validation Accuracy: 61.11%\n",
            "Epoch [5/10], Training Loss: 0.641, Validation Accuracy: 60.91%\n",
            "Epoch [6/10], Training Loss: 0.616, Validation Accuracy: 60.73%\n",
            "Epoch [7/10], Training Loss: 0.601, Validation Accuracy: 60.87%\n",
            "Epoch [8/10], Training Loss: 0.566, Validation Accuracy: 61.27%\n",
            "Epoch [9/10], Training Loss: 0.549, Validation Accuracy: 60.24%\n",
            "Epoch [10/10], Training Loss: 0.534, Validation Accuracy: 60.56%\n",
            "Epoch [1/10], Training Loss: 0.847, Validation Accuracy: 60.13%\n",
            "Epoch [2/10], Training Loss: 0.743, Validation Accuracy: 60.61%\n",
            "Epoch [3/10], Training Loss: 0.686, Validation Accuracy: 60.43%\n",
            "Epoch [4/10], Training Loss: 0.657, Validation Accuracy: 60.23%\n",
            "Epoch [5/10], Training Loss: 0.607, Validation Accuracy: 60.73%\n",
            "Epoch [6/10], Training Loss: 0.574, Validation Accuracy: 61.00%\n",
            "Epoch [7/10], Training Loss: 0.555, Validation Accuracy: 61.16%\n",
            "Epoch [8/10], Training Loss: 0.532, Validation Accuracy: 60.22%\n",
            "Epoch [9/10], Training Loss: 0.510, Validation Accuracy: 59.91%\n",
            "Epoch [10/10], Training Loss: 0.503, Validation Accuracy: 60.11%\n",
            "Epoch [1/10], Training Loss: 0.867, Validation Accuracy: 59.44%\n",
            "Epoch [2/10], Training Loss: 0.745, Validation Accuracy: 61.08%\n",
            "Epoch [3/10], Training Loss: 0.688, Validation Accuracy: 60.74%\n",
            "Epoch [4/10], Training Loss: 0.637, Validation Accuracy: 60.00%\n",
            "Epoch [5/10], Training Loss: 0.614, Validation Accuracy: 60.01%\n",
            "Epoch [6/10], Training Loss: 0.580, Validation Accuracy: 60.34%\n",
            "Epoch [7/10], Training Loss: 0.549, Validation Accuracy: 60.16%\n",
            "Epoch [8/10], Training Loss: 0.524, Validation Accuracy: 60.51%\n",
            "Epoch [9/10], Training Loss: 0.505, Validation Accuracy: 60.19%\n",
            "Epoch [10/10], Training Loss: 0.488, Validation Accuracy: 59.85%\n",
            "Epoch [1/10], Training Loss: 0.854, Validation Accuracy: 60.28%\n",
            "Epoch [2/10], Training Loss: 0.714, Validation Accuracy: 60.51%\n",
            "Epoch [3/10], Training Loss: 0.653, Validation Accuracy: 60.67%\n",
            "Epoch [4/10], Training Loss: 0.600, Validation Accuracy: 60.88%\n",
            "Epoch [5/10], Training Loss: 0.569, Validation Accuracy: 61.17%\n",
            "Epoch [6/10], Training Loss: 0.543, Validation Accuracy: 60.39%\n",
            "Epoch [7/10], Training Loss: 0.514, Validation Accuracy: 60.80%\n",
            "Epoch [8/10], Training Loss: 0.491, Validation Accuracy: 60.10%\n",
            "Epoch [9/10], Training Loss: 0.468, Validation Accuracy: 60.10%\n",
            "Epoch [10/10], Training Loss: 0.450, Validation Accuracy: 60.31%\n",
            "Epoch [1/10], Training Loss: 0.829, Validation Accuracy: 59.76%\n",
            "Epoch [2/10], Training Loss: 0.700, Validation Accuracy: 60.76%\n",
            "Epoch [3/10], Training Loss: 0.633, Validation Accuracy: 60.27%\n",
            "Epoch [4/10], Training Loss: 0.592, Validation Accuracy: 60.17%\n",
            "Epoch [5/10], Training Loss: 0.559, Validation Accuracy: 60.20%\n",
            "Epoch [6/10], Training Loss: 0.532, Validation Accuracy: 59.74%\n",
            "Epoch [7/10], Training Loss: 0.503, Validation Accuracy: 60.36%\n",
            "Epoch [8/10], Training Loss: 0.472, Validation Accuracy: 59.81%\n",
            "Epoch [9/10], Training Loss: 0.461, Validation Accuracy: 59.95%\n",
            "Epoch [10/10], Training Loss: 0.448, Validation Accuracy: 60.35%\n",
            "Epoch [1/10], Training Loss: 0.865, Validation Accuracy: 60.45%\n",
            "Epoch [2/10], Training Loss: 0.713, Validation Accuracy: 59.81%\n",
            "Epoch [3/10], Training Loss: 0.653, Validation Accuracy: 60.48%\n",
            "Epoch [4/10], Training Loss: 0.613, Validation Accuracy: 60.27%\n",
            "Epoch [5/10], Training Loss: 0.565, Validation Accuracy: 60.39%\n",
            "Epoch [6/10], Training Loss: 0.541, Validation Accuracy: 60.24%\n",
            "Epoch [7/10], Training Loss: 0.502, Validation Accuracy: 60.50%\n",
            "Epoch [8/10], Training Loss: 0.483, Validation Accuracy: 60.38%\n",
            "Epoch [9/10], Training Loss: 0.459, Validation Accuracy: 59.86%\n",
            "Epoch [10/10], Training Loss: 0.447, Validation Accuracy: 60.05%\n",
            "Epoch [1/10], Training Loss: 0.829, Validation Accuracy: 59.98%\n",
            "Epoch [2/10], Training Loss: 0.685, Validation Accuracy: 60.56%\n",
            "Epoch [3/10], Training Loss: 0.613, Validation Accuracy: 60.08%\n",
            "Epoch [4/10], Training Loss: 0.564, Validation Accuracy: 60.85%\n",
            "Epoch [5/10], Training Loss: 0.528, Validation Accuracy: 60.64%\n",
            "Epoch [6/10], Training Loss: 0.502, Validation Accuracy: 60.49%\n",
            "Epoch [7/10], Training Loss: 0.475, Validation Accuracy: 60.79%\n",
            "Epoch [8/10], Training Loss: 0.442, Validation Accuracy: 60.10%\n",
            "Epoch [9/10], Training Loss: 0.431, Validation Accuracy: 60.60%\n",
            "Epoch [10/10], Training Loss: 0.407, Validation Accuracy: 60.43%\n",
            "Epoch [1/10], Training Loss: 0.839, Validation Accuracy: 59.62%\n",
            "Epoch [2/10], Training Loss: 0.673, Validation Accuracy: 59.92%\n",
            "Epoch [3/10], Training Loss: 0.618, Validation Accuracy: 60.38%\n",
            "Epoch [4/10], Training Loss: 0.571, Validation Accuracy: 60.36%\n",
            "Epoch [5/10], Training Loss: 0.543, Validation Accuracy: 60.07%\n",
            "Epoch [6/10], Training Loss: 0.501, Validation Accuracy: 59.66%\n",
            "Epoch [7/10], Training Loss: 0.471, Validation Accuracy: 59.21%\n",
            "Epoch [8/10], Training Loss: 0.440, Validation Accuracy: 60.18%\n",
            "Epoch [9/10], Training Loss: 0.414, Validation Accuracy: 59.60%\n",
            "Epoch [10/10], Training Loss: 0.400, Validation Accuracy: 59.93%\n",
            "Confusion Matrix:\n",
            "[[689  39  62  27  29  10   7  14  65  58]\n",
            " [ 30 728   9   9   3   8   3  11  39 160]\n",
            " [ 77  18 472  80  92  90  64  63  23  21]\n",
            " [ 27  29  70 403  60 213  72  70   8  48]\n",
            " [ 36  13 102  97 450  66  58 145  11  22]\n",
            " [ 20  14  62 152  37 546  28 106   8  27]\n",
            " [ 16  24  48  84  62  40 679  21   7  19]\n",
            " [ 21  11  32  53  53  85   9 689   3  44]\n",
            " [ 99  81  25  23  11  13   5  10 680  53]\n",
            " [ 49 129  12  22   5  18   7  34  36 688]]\n",
            "Test Accuracy: 60.24%\n",
            "True Positives (TP): [689 728 472 403 450 546 679 689 680 688]\n",
            "False Positives (FP): [375 358 422 547 352 543 253 474 200 452]\n",
            "True Negatives (TN): [8625 8642 8578 8453 8648 8457 8747 8526 8800 8548]\n",
            "False Negatives (FN): [311 272 528 597 550 454 321 311 320 312]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.64755639 0.67034991 0.52796421 0.42421053 0.56109726 0.50137741\n",
            " 0.72854077 0.59243336 0.77272727 0.60350877]\n",
            "Recall: [0.689 0.728 0.472 0.403 0.45  0.546 0.679 0.689 0.68  0.688]\n",
            "F1 Score: [0.66763566 0.69798658 0.49841605 0.41333333 0.49944506 0.52273815\n",
            " 0.70289855 0.63707813 0.72340426 0.64299065]\n",
            "CPU times: user 3h 19min 4s, sys: 1min 27s, total: 3h 20min 31s\n",
            "Wall time: 3h 36min 26s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 2):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=2):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_normal: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = ( augmented_data_normal + augmented_data_truncated) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=2)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"normal\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0pKaKu8PnJ2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRtwnxXPPnyh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvhwCXzAPvvo",
        "outputId": "24279cc5-8beb-4979-ddf0-49bad511093d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 77.5MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Images per Class: [6134 5864 5955 5977 5945 6101 6025 6068 5967 5964]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<timed exec>:281: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Training Loss: 2.304, Validation Accuracy: 10.19%\n",
            "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 10.60%\n",
            "Epoch [3/10], Training Loss: 2.302, Validation Accuracy: 11.03%\n",
            "Epoch [4/10], Training Loss: 2.301, Validation Accuracy: 10.88%\n",
            "Epoch [5/10], Training Loss: 2.300, Validation Accuracy: 10.75%\n",
            "Epoch [6/10], Training Loss: 2.299, Validation Accuracy: 10.97%\n",
            "Epoch [7/10], Training Loss: 2.298, Validation Accuracy: 11.62%\n",
            "Epoch [8/10], Training Loss: 2.296, Validation Accuracy: 12.44%\n",
            "Epoch [9/10], Training Loss: 2.295, Validation Accuracy: 12.51%\n",
            "Epoch [10/10], Training Loss: 2.292, Validation Accuracy: 12.31%\n",
            "Epoch [1/10], Training Loss: 2.290, Validation Accuracy: 12.41%\n",
            "Epoch [2/10], Training Loss: 2.286, Validation Accuracy: 12.06%\n",
            "Epoch [3/10], Training Loss: 2.281, Validation Accuracy: 11.54%\n",
            "Epoch [4/10], Training Loss: 2.274, Validation Accuracy: 11.48%\n",
            "Epoch [5/10], Training Loss: 2.264, Validation Accuracy: 13.39%\n",
            "Epoch [6/10], Training Loss: 2.250, Validation Accuracy: 15.84%\n",
            "Epoch [7/10], Training Loss: 2.232, Validation Accuracy: 18.84%\n",
            "Epoch [8/10], Training Loss: 2.206, Validation Accuracy: 22.40%\n",
            "Epoch [9/10], Training Loss: 2.172, Validation Accuracy: 23.83%\n",
            "Epoch [10/10], Training Loss: 2.142, Validation Accuracy: 24.44%\n",
            "Epoch [1/10], Training Loss: 2.131, Validation Accuracy: 24.99%\n",
            "Epoch [2/10], Training Loss: 2.115, Validation Accuracy: 26.09%\n",
            "Epoch [3/10], Training Loss: 2.100, Validation Accuracy: 26.86%\n",
            "Epoch [4/10], Training Loss: 2.082, Validation Accuracy: 27.49%\n",
            "Epoch [5/10], Training Loss: 2.064, Validation Accuracy: 27.66%\n",
            "Epoch [6/10], Training Loss: 2.046, Validation Accuracy: 28.06%\n",
            "Epoch [7/10], Training Loss: 2.028, Validation Accuracy: 28.66%\n",
            "Epoch [8/10], Training Loss: 2.009, Validation Accuracy: 29.37%\n",
            "Epoch [9/10], Training Loss: 1.990, Validation Accuracy: 29.16%\n",
            "Epoch [10/10], Training Loss: 1.976, Validation Accuracy: 30.52%\n",
            "Epoch [1/10], Training Loss: 1.969, Validation Accuracy: 30.49%\n",
            "Epoch [2/10], Training Loss: 1.951, Validation Accuracy: 30.56%\n",
            "Epoch [3/10], Training Loss: 1.937, Validation Accuracy: 31.29%\n",
            "Epoch [4/10], Training Loss: 1.924, Validation Accuracy: 32.18%\n",
            "Epoch [5/10], Training Loss: 1.912, Validation Accuracy: 32.19%\n",
            "Epoch [6/10], Training Loss: 1.902, Validation Accuracy: 32.71%\n",
            "Epoch [7/10], Training Loss: 1.887, Validation Accuracy: 32.50%\n",
            "Epoch [8/10], Training Loss: 1.875, Validation Accuracy: 33.40%\n",
            "Epoch [9/10], Training Loss: 1.863, Validation Accuracy: 33.96%\n",
            "Epoch [10/10], Training Loss: 1.854, Validation Accuracy: 33.72%\n",
            "Epoch [1/10], Training Loss: 1.866, Validation Accuracy: 34.62%\n",
            "Epoch [2/10], Training Loss: 1.845, Validation Accuracy: 35.08%\n",
            "Epoch [3/10], Training Loss: 1.832, Validation Accuracy: 34.93%\n",
            "Epoch [4/10], Training Loss: 1.824, Validation Accuracy: 35.43%\n",
            "Epoch [5/10], Training Loss: 1.808, Validation Accuracy: 35.68%\n",
            "Epoch [6/10], Training Loss: 1.792, Validation Accuracy: 35.07%\n",
            "Epoch [7/10], Training Loss: 1.780, Validation Accuracy: 36.41%\n",
            "Epoch [8/10], Training Loss: 1.771, Validation Accuracy: 37.70%\n",
            "Epoch [9/10], Training Loss: 1.754, Validation Accuracy: 37.73%\n",
            "Epoch [10/10], Training Loss: 1.742, Validation Accuracy: 38.09%\n",
            "Epoch [1/10], Training Loss: 1.726, Validation Accuracy: 38.56%\n",
            "Epoch [2/10], Training Loss: 1.707, Validation Accuracy: 38.37%\n",
            "Epoch [3/10], Training Loss: 1.704, Validation Accuracy: 39.39%\n",
            "Epoch [4/10], Training Loss: 1.677, Validation Accuracy: 39.65%\n",
            "Epoch [5/10], Training Loss: 1.662, Validation Accuracy: 40.33%\n",
            "Epoch [6/10], Training Loss: 1.647, Validation Accuracy: 40.53%\n",
            "Epoch [7/10], Training Loss: 1.634, Validation Accuracy: 40.57%\n",
            "Epoch [8/10], Training Loss: 1.620, Validation Accuracy: 41.46%\n",
            "Epoch [9/10], Training Loss: 1.605, Validation Accuracy: 41.74%\n",
            "Epoch [10/10], Training Loss: 1.598, Validation Accuracy: 42.63%\n",
            "Epoch [1/10], Training Loss: 1.618, Validation Accuracy: 42.18%\n",
            "Epoch [2/10], Training Loss: 1.602, Validation Accuracy: 41.51%\n",
            "Epoch [3/10], Training Loss: 1.591, Validation Accuracy: 42.67%\n",
            "Epoch [4/10], Training Loss: 1.577, Validation Accuracy: 43.61%\n",
            "Epoch [5/10], Training Loss: 1.562, Validation Accuracy: 43.47%\n",
            "Epoch [6/10], Training Loss: 1.553, Validation Accuracy: 44.29%\n",
            "Epoch [7/10], Training Loss: 1.537, Validation Accuracy: 43.81%\n",
            "Epoch [8/10], Training Loss: 1.529, Validation Accuracy: 44.66%\n",
            "Epoch [9/10], Training Loss: 1.520, Validation Accuracy: 45.36%\n",
            "Epoch [10/10], Training Loss: 1.510, Validation Accuracy: 45.92%\n",
            "Epoch [1/10], Training Loss: 1.529, Validation Accuracy: 45.46%\n",
            "Epoch [2/10], Training Loss: 1.515, Validation Accuracy: 45.42%\n",
            "Epoch [3/10], Training Loss: 1.503, Validation Accuracy: 45.28%\n",
            "Epoch [4/10], Training Loss: 1.499, Validation Accuracy: 46.55%\n",
            "Epoch [5/10], Training Loss: 1.484, Validation Accuracy: 46.55%\n",
            "Epoch [6/10], Training Loss: 1.477, Validation Accuracy: 46.78%\n",
            "Epoch [7/10], Training Loss: 1.471, Validation Accuracy: 46.57%\n",
            "Epoch [8/10], Training Loss: 1.466, Validation Accuracy: 47.01%\n",
            "Epoch [9/10], Training Loss: 1.455, Validation Accuracy: 46.91%\n",
            "Epoch [10/10], Training Loss: 1.446, Validation Accuracy: 46.94%\n",
            "Epoch [1/10], Training Loss: 1.486, Validation Accuracy: 47.34%\n",
            "Epoch [2/10], Training Loss: 1.474, Validation Accuracy: 47.68%\n",
            "Epoch [3/10], Training Loss: 1.469, Validation Accuracy: 47.14%\n",
            "Epoch [4/10], Training Loss: 1.450, Validation Accuracy: 48.11%\n",
            "Epoch [5/10], Training Loss: 1.445, Validation Accuracy: 48.24%\n",
            "Epoch [6/10], Training Loss: 1.436, Validation Accuracy: 48.10%\n",
            "Epoch [7/10], Training Loss: 1.433, Validation Accuracy: 48.19%\n",
            "Epoch [8/10], Training Loss: 1.422, Validation Accuracy: 48.26%\n",
            "Epoch [9/10], Training Loss: 1.414, Validation Accuracy: 48.58%\n",
            "Epoch [10/10], Training Loss: 1.409, Validation Accuracy: 48.11%\n",
            "Epoch [1/10], Training Loss: 1.460, Validation Accuracy: 48.82%\n",
            "Epoch [2/10], Training Loss: 1.442, Validation Accuracy: 48.71%\n",
            "Epoch [3/10], Training Loss: 1.427, Validation Accuracy: 49.41%\n",
            "Epoch [4/10], Training Loss: 1.423, Validation Accuracy: 49.83%\n",
            "Epoch [5/10], Training Loss: 1.407, Validation Accuracy: 49.64%\n",
            "Epoch [6/10], Training Loss: 1.403, Validation Accuracy: 49.88%\n",
            "Epoch [7/10], Training Loss: 1.397, Validation Accuracy: 48.73%\n",
            "Epoch [8/10], Training Loss: 1.385, Validation Accuracy: 49.09%\n",
            "Epoch [9/10], Training Loss: 1.380, Validation Accuracy: 49.55%\n",
            "Epoch [10/10], Training Loss: 1.373, Validation Accuracy: 49.43%\n",
            "Epoch [1/10], Training Loss: 1.395, Validation Accuracy: 49.56%\n",
            "Epoch [2/10], Training Loss: 1.374, Validation Accuracy: 49.40%\n",
            "Epoch [3/10], Training Loss: 1.367, Validation Accuracy: 50.13%\n",
            "Epoch [4/10], Training Loss: 1.352, Validation Accuracy: 50.15%\n",
            "Epoch [5/10], Training Loss: 1.345, Validation Accuracy: 50.04%\n",
            "Epoch [6/10], Training Loss: 1.331, Validation Accuracy: 50.17%\n",
            "Epoch [7/10], Training Loss: 1.332, Validation Accuracy: 49.86%\n",
            "Epoch [8/10], Training Loss: 1.323, Validation Accuracy: 50.58%\n",
            "Epoch [9/10], Training Loss: 1.314, Validation Accuracy: 50.99%\n",
            "Epoch [10/10], Training Loss: 1.306, Validation Accuracy: 50.15%\n",
            "Epoch [1/10], Training Loss: 1.365, Validation Accuracy: 50.92%\n",
            "Epoch [2/10], Training Loss: 1.348, Validation Accuracy: 51.39%\n",
            "Epoch [3/10], Training Loss: 1.327, Validation Accuracy: 51.50%\n",
            "Epoch [4/10], Training Loss: 1.321, Validation Accuracy: 50.91%\n",
            "Epoch [5/10], Training Loss: 1.304, Validation Accuracy: 50.95%\n",
            "Epoch [6/10], Training Loss: 1.300, Validation Accuracy: 51.79%\n",
            "Epoch [7/10], Training Loss: 1.296, Validation Accuracy: 50.97%\n",
            "Epoch [8/10], Training Loss: 1.291, Validation Accuracy: 51.55%\n",
            "Epoch [9/10], Training Loss: 1.277, Validation Accuracy: 51.55%\n",
            "Epoch [10/10], Training Loss: 1.270, Validation Accuracy: 52.10%\n",
            "Epoch [1/10], Training Loss: 1.337, Validation Accuracy: 51.89%\n",
            "Epoch [2/10], Training Loss: 1.317, Validation Accuracy: 52.18%\n",
            "Epoch [3/10], Training Loss: 1.306, Validation Accuracy: 52.71%\n",
            "Epoch [4/10], Training Loss: 1.294, Validation Accuracy: 52.27%\n",
            "Epoch [5/10], Training Loss: 1.283, Validation Accuracy: 52.38%\n",
            "Epoch [6/10], Training Loss: 1.272, Validation Accuracy: 52.62%\n",
            "Epoch [7/10], Training Loss: 1.263, Validation Accuracy: 52.39%\n",
            "Epoch [8/10], Training Loss: 1.262, Validation Accuracy: 51.82%\n",
            "Epoch [9/10], Training Loss: 1.251, Validation Accuracy: 53.19%\n",
            "Epoch [10/10], Training Loss: 1.243, Validation Accuracy: 52.80%\n",
            "Epoch [1/10], Training Loss: 1.328, Validation Accuracy: 53.22%\n",
            "Epoch [2/10], Training Loss: 1.304, Validation Accuracy: 53.35%\n",
            "Epoch [3/10], Training Loss: 1.288, Validation Accuracy: 53.14%\n",
            "Epoch [4/10], Training Loss: 1.275, Validation Accuracy: 53.27%\n",
            "Epoch [5/10], Training Loss: 1.261, Validation Accuracy: 53.45%\n",
            "Epoch [6/10], Training Loss: 1.258, Validation Accuracy: 53.61%\n",
            "Epoch [7/10], Training Loss: 1.251, Validation Accuracy: 52.93%\n",
            "Epoch [8/10], Training Loss: 1.242, Validation Accuracy: 52.74%\n",
            "Epoch [9/10], Training Loss: 1.224, Validation Accuracy: 54.25%\n",
            "Epoch [10/10], Training Loss: 1.216, Validation Accuracy: 53.23%\n",
            "Epoch [1/10], Training Loss: 1.300, Validation Accuracy: 54.30%\n",
            "Epoch [2/10], Training Loss: 1.272, Validation Accuracy: 54.64%\n",
            "Epoch [3/10], Training Loss: 1.265, Validation Accuracy: 54.65%\n",
            "Epoch [4/10], Training Loss: 1.250, Validation Accuracy: 54.47%\n",
            "Epoch [5/10], Training Loss: 1.237, Validation Accuracy: 54.50%\n",
            "Epoch [6/10], Training Loss: 1.221, Validation Accuracy: 54.25%\n",
            "Epoch [7/10], Training Loss: 1.215, Validation Accuracy: 54.43%\n",
            "Epoch [8/10], Training Loss: 1.203, Validation Accuracy: 54.46%\n",
            "Epoch [9/10], Training Loss: 1.193, Validation Accuracy: 54.79%\n",
            "Epoch [10/10], Training Loss: 1.184, Validation Accuracy: 54.70%\n",
            "Epoch [1/10], Training Loss: 1.245, Validation Accuracy: 54.44%\n",
            "Epoch [2/10], Training Loss: 1.221, Validation Accuracy: 54.98%\n",
            "Epoch [3/10], Training Loss: 1.208, Validation Accuracy: 55.46%\n",
            "Epoch [4/10], Training Loss: 1.188, Validation Accuracy: 53.58%\n",
            "Epoch [5/10], Training Loss: 1.189, Validation Accuracy: 54.98%\n",
            "Epoch [6/10], Training Loss: 1.164, Validation Accuracy: 55.34%\n",
            "Epoch [7/10], Training Loss: 1.156, Validation Accuracy: 54.48%\n",
            "Epoch [8/10], Training Loss: 1.150, Validation Accuracy: 54.36%\n",
            "Epoch [9/10], Training Loss: 1.154, Validation Accuracy: 55.21%\n",
            "Epoch [10/10], Training Loss: 1.134, Validation Accuracy: 54.93%\n",
            "Epoch [1/10], Training Loss: 1.216, Validation Accuracy: 55.08%\n",
            "Epoch [2/10], Training Loss: 1.185, Validation Accuracy: 55.71%\n",
            "Epoch [3/10], Training Loss: 1.172, Validation Accuracy: 55.73%\n",
            "Epoch [4/10], Training Loss: 1.154, Validation Accuracy: 55.54%\n",
            "Epoch [5/10], Training Loss: 1.140, Validation Accuracy: 54.66%\n",
            "Epoch [6/10], Training Loss: 1.137, Validation Accuracy: 55.66%\n",
            "Epoch [7/10], Training Loss: 1.126, Validation Accuracy: 55.69%\n",
            "Epoch [8/10], Training Loss: 1.106, Validation Accuracy: 55.81%\n",
            "Epoch [9/10], Training Loss: 1.101, Validation Accuracy: 55.95%\n",
            "Epoch [10/10], Training Loss: 1.092, Validation Accuracy: 55.64%\n",
            "Epoch [1/10], Training Loss: 1.204, Validation Accuracy: 56.74%\n",
            "Epoch [2/10], Training Loss: 1.177, Validation Accuracy: 54.87%\n",
            "Epoch [3/10], Training Loss: 1.174, Validation Accuracy: 56.18%\n",
            "Epoch [4/10], Training Loss: 1.145, Validation Accuracy: 55.73%\n",
            "Epoch [5/10], Training Loss: 1.127, Validation Accuracy: 55.77%\n",
            "Epoch [6/10], Training Loss: 1.127, Validation Accuracy: 56.63%\n",
            "Epoch [7/10], Training Loss: 1.112, Validation Accuracy: 56.77%\n",
            "Epoch [8/10], Training Loss: 1.098, Validation Accuracy: 56.79%\n",
            "Epoch [9/10], Training Loss: 1.090, Validation Accuracy: 56.65%\n",
            "Epoch [10/10], Training Loss: 1.078, Validation Accuracy: 56.19%\n",
            "Epoch [1/10], Training Loss: 1.206, Validation Accuracy: 56.72%\n",
            "Epoch [2/10], Training Loss: 1.173, Validation Accuracy: 57.21%\n",
            "Epoch [3/10], Training Loss: 1.167, Validation Accuracy: 56.86%\n",
            "Epoch [4/10], Training Loss: 1.147, Validation Accuracy: 56.73%\n",
            "Epoch [5/10], Training Loss: 1.146, Validation Accuracy: 57.20%\n",
            "Epoch [6/10], Training Loss: 1.126, Validation Accuracy: 57.01%\n",
            "Epoch [7/10], Training Loss: 1.102, Validation Accuracy: 57.75%\n",
            "Epoch [8/10], Training Loss: 1.098, Validation Accuracy: 56.37%\n",
            "Epoch [9/10], Training Loss: 1.091, Validation Accuracy: 57.26%\n",
            "Epoch [10/10], Training Loss: 1.081, Validation Accuracy: 57.19%\n",
            "Epoch [1/10], Training Loss: 1.174, Validation Accuracy: 57.71%\n",
            "Epoch [2/10], Training Loss: 1.135, Validation Accuracy: 57.80%\n",
            "Epoch [3/10], Training Loss: 1.114, Validation Accuracy: 57.27%\n",
            "Epoch [4/10], Training Loss: 1.105, Validation Accuracy: 57.91%\n",
            "Epoch [5/10], Training Loss: 1.086, Validation Accuracy: 57.97%\n",
            "Epoch [6/10], Training Loss: 1.065, Validation Accuracy: 58.25%\n",
            "Epoch [7/10], Training Loss: 1.067, Validation Accuracy: 58.21%\n",
            "Epoch [8/10], Training Loss: 1.050, Validation Accuracy: 57.29%\n",
            "Epoch [9/10], Training Loss: 1.037, Validation Accuracy: 57.56%\n",
            "Epoch [10/10], Training Loss: 1.027, Validation Accuracy: 58.01%\n",
            "Epoch [1/10], Training Loss: 1.135, Validation Accuracy: 57.49%\n",
            "Epoch [2/10], Training Loss: 1.114, Validation Accuracy: 57.57%\n",
            "Epoch [3/10], Training Loss: 1.078, Validation Accuracy: 57.13%\n",
            "Epoch [4/10], Training Loss: 1.071, Validation Accuracy: 58.34%\n",
            "Epoch [5/10], Training Loss: 1.049, Validation Accuracy: 57.52%\n",
            "Epoch [6/10], Training Loss: 1.039, Validation Accuracy: 58.22%\n",
            "Epoch [7/10], Training Loss: 1.023, Validation Accuracy: 58.06%\n",
            "Epoch [8/10], Training Loss: 1.016, Validation Accuracy: 58.13%\n",
            "Epoch [9/10], Training Loss: 1.002, Validation Accuracy: 57.73%\n",
            "Epoch [10/10], Training Loss: 0.993, Validation Accuracy: 58.38%\n",
            "Epoch [1/10], Training Loss: 1.103, Validation Accuracy: 58.70%\n",
            "Epoch [2/10], Training Loss: 1.066, Validation Accuracy: 58.64%\n",
            "Epoch [3/10], Training Loss: 1.047, Validation Accuracy: 57.07%\n",
            "Epoch [4/10], Training Loss: 1.027, Validation Accuracy: 59.10%\n",
            "Epoch [5/10], Training Loss: 1.014, Validation Accuracy: 58.83%\n",
            "Epoch [6/10], Training Loss: 1.000, Validation Accuracy: 58.17%\n",
            "Epoch [7/10], Training Loss: 0.993, Validation Accuracy: 58.43%\n",
            "Epoch [8/10], Training Loss: 0.990, Validation Accuracy: 58.41%\n",
            "Epoch [9/10], Training Loss: 0.964, Validation Accuracy: 58.38%\n",
            "Epoch [10/10], Training Loss: 0.955, Validation Accuracy: 57.99%\n",
            "Epoch [1/10], Training Loss: 1.120, Validation Accuracy: 59.09%\n",
            "Epoch [2/10], Training Loss: 1.072, Validation Accuracy: 58.89%\n",
            "Epoch [3/10], Training Loss: 1.055, Validation Accuracy: 59.42%\n",
            "Epoch [4/10], Training Loss: 1.022, Validation Accuracy: 59.22%\n",
            "Epoch [5/10], Training Loss: 1.007, Validation Accuracy: 59.32%\n",
            "Epoch [6/10], Training Loss: 1.001, Validation Accuracy: 59.00%\n",
            "Epoch [7/10], Training Loss: 0.994, Validation Accuracy: 59.13%\n",
            "Epoch [8/10], Training Loss: 0.965, Validation Accuracy: 59.12%\n",
            "Epoch [9/10], Training Loss: 0.958, Validation Accuracy: 59.14%\n",
            "Epoch [10/10], Training Loss: 0.948, Validation Accuracy: 59.17%\n",
            "Epoch [1/10], Training Loss: 1.120, Validation Accuracy: 59.05%\n",
            "Epoch [2/10], Training Loss: 1.082, Validation Accuracy: 59.63%\n",
            "Epoch [3/10], Training Loss: 1.055, Validation Accuracy: 59.24%\n",
            "Epoch [4/10], Training Loss: 1.039, Validation Accuracy: 59.12%\n",
            "Epoch [5/10], Training Loss: 1.027, Validation Accuracy: 59.09%\n",
            "Epoch [6/10], Training Loss: 1.004, Validation Accuracy: 59.22%\n",
            "Epoch [7/10], Training Loss: 0.991, Validation Accuracy: 59.01%\n",
            "Epoch [8/10], Training Loss: 0.982, Validation Accuracy: 59.39%\n",
            "Epoch [9/10], Training Loss: 0.971, Validation Accuracy: 59.27%\n",
            "Epoch [10/10], Training Loss: 0.958, Validation Accuracy: 59.03%\n",
            "Epoch [1/10], Training Loss: 1.079, Validation Accuracy: 58.66%\n",
            "Epoch [2/10], Training Loss: 1.037, Validation Accuracy: 59.62%\n",
            "Epoch [3/10], Training Loss: 1.013, Validation Accuracy: 59.81%\n",
            "Epoch [4/10], Training Loss: 0.985, Validation Accuracy: 59.84%\n",
            "Epoch [5/10], Training Loss: 0.961, Validation Accuracy: 59.94%\n",
            "Epoch [6/10], Training Loss: 0.949, Validation Accuracy: 59.45%\n",
            "Epoch [7/10], Training Loss: 0.934, Validation Accuracy: 59.75%\n",
            "Epoch [8/10], Training Loss: 0.922, Validation Accuracy: 58.85%\n",
            "Epoch [9/10], Training Loss: 0.913, Validation Accuracy: 59.75%\n",
            "Epoch [10/10], Training Loss: 0.904, Validation Accuracy: 59.72%\n",
            "Epoch [1/10], Training Loss: 1.060, Validation Accuracy: 59.77%\n",
            "Epoch [2/10], Training Loss: 1.013, Validation Accuracy: 60.02%\n",
            "Epoch [3/10], Training Loss: 0.971, Validation Accuracy: 60.11%\n",
            "Epoch [4/10], Training Loss: 0.956, Validation Accuracy: 59.86%\n",
            "Epoch [5/10], Training Loss: 0.949, Validation Accuracy: 60.13%\n",
            "Epoch [6/10], Training Loss: 0.927, Validation Accuracy: 60.50%\n",
            "Epoch [7/10], Training Loss: 0.910, Validation Accuracy: 59.73%\n",
            "Epoch [8/10], Training Loss: 0.897, Validation Accuracy: 59.87%\n",
            "Epoch [9/10], Training Loss: 0.889, Validation Accuracy: 60.42%\n",
            "Epoch [10/10], Training Loss: 0.869, Validation Accuracy: 59.86%\n",
            "Epoch [1/10], Training Loss: 1.028, Validation Accuracy: 60.12%\n",
            "Epoch [2/10], Training Loss: 0.985, Validation Accuracy: 58.68%\n",
            "Epoch [3/10], Training Loss: 0.970, Validation Accuracy: 59.11%\n",
            "Epoch [4/10], Training Loss: 0.936, Validation Accuracy: 60.29%\n",
            "Epoch [5/10], Training Loss: 0.914, Validation Accuracy: 60.48%\n",
            "Epoch [6/10], Training Loss: 0.894, Validation Accuracy: 60.29%\n",
            "Epoch [7/10], Training Loss: 0.891, Validation Accuracy: 59.58%\n",
            "Epoch [8/10], Training Loss: 0.877, Validation Accuracy: 59.80%\n",
            "Epoch [9/10], Training Loss: 0.855, Validation Accuracy: 59.38%\n",
            "Epoch [10/10], Training Loss: 0.856, Validation Accuracy: 60.44%\n",
            "Epoch [1/10], Training Loss: 1.031, Validation Accuracy: 60.23%\n",
            "Epoch [2/10], Training Loss: 0.988, Validation Accuracy: 60.39%\n",
            "Epoch [3/10], Training Loss: 0.960, Validation Accuracy: 60.51%\n",
            "Epoch [4/10], Training Loss: 0.935, Validation Accuracy: 60.84%\n",
            "Epoch [5/10], Training Loss: 0.910, Validation Accuracy: 59.80%\n",
            "Epoch [6/10], Training Loss: 0.892, Validation Accuracy: 60.34%\n",
            "Epoch [7/10], Training Loss: 0.880, Validation Accuracy: 60.74%\n",
            "Epoch [8/10], Training Loss: 0.864, Validation Accuracy: 59.57%\n",
            "Epoch [9/10], Training Loss: 0.845, Validation Accuracy: 60.68%\n",
            "Epoch [10/10], Training Loss: 0.843, Validation Accuracy: 59.95%\n",
            "Epoch [1/10], Training Loss: 1.047, Validation Accuracy: 59.95%\n",
            "Epoch [2/10], Training Loss: 0.992, Validation Accuracy: 60.31%\n",
            "Epoch [3/10], Training Loss: 0.956, Validation Accuracy: 59.64%\n",
            "Epoch [4/10], Training Loss: 0.945, Validation Accuracy: 60.57%\n",
            "Epoch [5/10], Training Loss: 0.920, Validation Accuracy: 60.22%\n",
            "Epoch [6/10], Training Loss: 0.902, Validation Accuracy: 60.02%\n",
            "Epoch [7/10], Training Loss: 0.888, Validation Accuracy: 60.20%\n",
            "Epoch [8/10], Training Loss: 0.879, Validation Accuracy: 59.43%\n",
            "Epoch [9/10], Training Loss: 0.852, Validation Accuracy: 60.32%\n",
            "Epoch [10/10], Training Loss: 0.842, Validation Accuracy: 59.31%\n",
            "Epoch [1/10], Training Loss: 1.002, Validation Accuracy: 59.90%\n",
            "Epoch [2/10], Training Loss: 0.949, Validation Accuracy: 60.92%\n",
            "Epoch [3/10], Training Loss: 0.913, Validation Accuracy: 60.45%\n",
            "Epoch [4/10], Training Loss: 0.889, Validation Accuracy: 61.20%\n",
            "Epoch [5/10], Training Loss: 0.861, Validation Accuracy: 60.75%\n",
            "Epoch [6/10], Training Loss: 0.843, Validation Accuracy: 61.06%\n",
            "Epoch [7/10], Training Loss: 0.824, Validation Accuracy: 60.37%\n",
            "Epoch [8/10], Training Loss: 0.813, Validation Accuracy: 60.58%\n",
            "Epoch [9/10], Training Loss: 0.800, Validation Accuracy: 60.00%\n",
            "Epoch [10/10], Training Loss: 0.780, Validation Accuracy: 60.36%\n",
            "Epoch [1/10], Training Loss: 0.989, Validation Accuracy: 60.11%\n",
            "Epoch [2/10], Training Loss: 0.926, Validation Accuracy: 60.82%\n",
            "Epoch [3/10], Training Loss: 0.889, Validation Accuracy: 61.18%\n",
            "Epoch [4/10], Training Loss: 0.860, Validation Accuracy: 61.26%\n",
            "Epoch [5/10], Training Loss: 0.840, Validation Accuracy: 60.59%\n",
            "Epoch [6/10], Training Loss: 0.824, Validation Accuracy: 60.85%\n",
            "Epoch [7/10], Training Loss: 0.802, Validation Accuracy: 60.73%\n",
            "Epoch [8/10], Training Loss: 0.786, Validation Accuracy: 61.16%\n",
            "Epoch [9/10], Training Loss: 0.770, Validation Accuracy: 61.00%\n",
            "Epoch [10/10], Training Loss: 0.754, Validation Accuracy: 60.78%\n",
            "Epoch [1/10], Training Loss: 0.965, Validation Accuracy: 60.08%\n",
            "Epoch [2/10], Training Loss: 0.904, Validation Accuracy: 60.07%\n",
            "Epoch [3/10], Training Loss: 0.869, Validation Accuracy: 61.06%\n",
            "Epoch [4/10], Training Loss: 0.846, Validation Accuracy: 61.07%\n",
            "Epoch [5/10], Training Loss: 0.820, Validation Accuracy: 61.09%\n",
            "Epoch [6/10], Training Loss: 0.796, Validation Accuracy: 60.77%\n",
            "Epoch [7/10], Training Loss: 0.782, Validation Accuracy: 60.98%\n",
            "Epoch [8/10], Training Loss: 0.777, Validation Accuracy: 60.41%\n",
            "Epoch [9/10], Training Loss: 0.753, Validation Accuracy: 60.25%\n",
            "Epoch [10/10], Training Loss: 0.743, Validation Accuracy: 60.61%\n",
            "Epoch [1/10], Training Loss: 0.984, Validation Accuracy: 60.79%\n",
            "Epoch [2/10], Training Loss: 0.912, Validation Accuracy: 60.81%\n",
            "Epoch [3/10], Training Loss: 0.875, Validation Accuracy: 60.50%\n",
            "Epoch [4/10], Training Loss: 0.838, Validation Accuracy: 60.67%\n",
            "Epoch [5/10], Training Loss: 0.820, Validation Accuracy: 61.29%\n",
            "Epoch [6/10], Training Loss: 0.794, Validation Accuracy: 60.61%\n",
            "Epoch [7/10], Training Loss: 0.781, Validation Accuracy: 60.49%\n",
            "Epoch [8/10], Training Loss: 0.755, Validation Accuracy: 61.33%\n",
            "Epoch [9/10], Training Loss: 0.742, Validation Accuracy: 60.96%\n",
            "Epoch [10/10], Training Loss: 0.724, Validation Accuracy: 61.11%\n",
            "Epoch [1/10], Training Loss: 0.988, Validation Accuracy: 61.20%\n",
            "Epoch [2/10], Training Loss: 0.920, Validation Accuracy: 60.86%\n",
            "Epoch [3/10], Training Loss: 0.883, Validation Accuracy: 60.38%\n",
            "Epoch [4/10], Training Loss: 0.852, Validation Accuracy: 60.93%\n",
            "Epoch [5/10], Training Loss: 0.825, Validation Accuracy: 61.28%\n",
            "Epoch [6/10], Training Loss: 0.800, Validation Accuracy: 60.68%\n",
            "Epoch [7/10], Training Loss: 0.788, Validation Accuracy: 60.34%\n",
            "Epoch [8/10], Training Loss: 0.766, Validation Accuracy: 60.76%\n",
            "Epoch [9/10], Training Loss: 0.749, Validation Accuracy: 60.68%\n",
            "Epoch [10/10], Training Loss: 0.730, Validation Accuracy: 60.12%\n",
            "Epoch [1/10], Training Loss: 0.929, Validation Accuracy: 61.61%\n",
            "Epoch [2/10], Training Loss: 0.866, Validation Accuracy: 61.54%\n",
            "Epoch [3/10], Training Loss: 0.830, Validation Accuracy: 61.15%\n",
            "Epoch [4/10], Training Loss: 0.800, Validation Accuracy: 61.47%\n",
            "Epoch [5/10], Training Loss: 0.766, Validation Accuracy: 61.85%\n",
            "Epoch [6/10], Training Loss: 0.750, Validation Accuracy: 61.86%\n",
            "Epoch [7/10], Training Loss: 0.737, Validation Accuracy: 60.25%\n",
            "Epoch [8/10], Training Loss: 0.713, Validation Accuracy: 61.41%\n",
            "Epoch [9/10], Training Loss: 0.696, Validation Accuracy: 61.17%\n",
            "Epoch [10/10], Training Loss: 0.677, Validation Accuracy: 60.42%\n",
            "Epoch [1/10], Training Loss: 0.915, Validation Accuracy: 61.62%\n",
            "Epoch [2/10], Training Loss: 0.857, Validation Accuracy: 61.44%\n",
            "Epoch [3/10], Training Loss: 0.803, Validation Accuracy: 61.17%\n",
            "Epoch [4/10], Training Loss: 0.771, Validation Accuracy: 61.81%\n",
            "Epoch [5/10], Training Loss: 0.754, Validation Accuracy: 61.36%\n",
            "Epoch [6/10], Training Loss: 0.723, Validation Accuracy: 61.03%\n",
            "Epoch [7/10], Training Loss: 0.704, Validation Accuracy: 60.79%\n",
            "Epoch [8/10], Training Loss: 0.693, Validation Accuracy: 61.25%\n",
            "Epoch [9/10], Training Loss: 0.672, Validation Accuracy: 61.14%\n",
            "Epoch [10/10], Training Loss: 0.653, Validation Accuracy: 60.61%\n",
            "Epoch [1/10], Training Loss: 0.922, Validation Accuracy: 61.02%\n",
            "Epoch [2/10], Training Loss: 0.833, Validation Accuracy: 61.20%\n",
            "Epoch [3/10], Training Loss: 0.799, Validation Accuracy: 60.68%\n",
            "Epoch [4/10], Training Loss: 0.767, Validation Accuracy: 61.34%\n",
            "Epoch [5/10], Training Loss: 0.736, Validation Accuracy: 61.42%\n",
            "Epoch [6/10], Training Loss: 0.716, Validation Accuracy: 61.44%\n",
            "Epoch [7/10], Training Loss: 0.688, Validation Accuracy: 61.31%\n",
            "Epoch [8/10], Training Loss: 0.673, Validation Accuracy: 60.88%\n",
            "Epoch [9/10], Training Loss: 0.657, Validation Accuracy: 60.12%\n",
            "Epoch [10/10], Training Loss: 0.638, Validation Accuracy: 61.33%\n",
            "Epoch [1/10], Training Loss: 0.917, Validation Accuracy: 61.48%\n",
            "Epoch [2/10], Training Loss: 0.843, Validation Accuracy: 61.61%\n",
            "Epoch [3/10], Training Loss: 0.789, Validation Accuracy: 61.49%\n",
            "Epoch [4/10], Training Loss: 0.755, Validation Accuracy: 61.31%\n",
            "Epoch [5/10], Training Loss: 0.734, Validation Accuracy: 61.58%\n",
            "Epoch [6/10], Training Loss: 0.698, Validation Accuracy: 61.23%\n",
            "Epoch [7/10], Training Loss: 0.683, Validation Accuracy: 60.81%\n",
            "Epoch [8/10], Training Loss: 0.670, Validation Accuracy: 61.46%\n",
            "Epoch [9/10], Training Loss: 0.644, Validation Accuracy: 61.60%\n",
            "Epoch [10/10], Training Loss: 0.617, Validation Accuracy: 61.30%\n",
            "Epoch [1/10], Training Loss: 0.933, Validation Accuracy: 60.23%\n",
            "Epoch [2/10], Training Loss: 0.846, Validation Accuracy: 60.48%\n",
            "Epoch [3/10], Training Loss: 0.801, Validation Accuracy: 61.19%\n",
            "Epoch [4/10], Training Loss: 0.761, Validation Accuracy: 60.89%\n",
            "Epoch [5/10], Training Loss: 0.734, Validation Accuracy: 60.95%\n",
            "Epoch [6/10], Training Loss: 0.704, Validation Accuracy: 61.20%\n",
            "Epoch [7/10], Training Loss: 0.684, Validation Accuracy: 60.78%\n",
            "Epoch [8/10], Training Loss: 0.664, Validation Accuracy: 61.11%\n",
            "Epoch [9/10], Training Loss: 0.647, Validation Accuracy: 60.69%\n",
            "Epoch [10/10], Training Loss: 0.625, Validation Accuracy: 60.85%\n",
            "Epoch [1/10], Training Loss: 0.882, Validation Accuracy: 60.79%\n",
            "Epoch [2/10], Training Loss: 0.790, Validation Accuracy: 61.37%\n",
            "Epoch [3/10], Training Loss: 0.750, Validation Accuracy: 61.63%\n",
            "Epoch [4/10], Training Loss: 0.708, Validation Accuracy: 61.76%\n",
            "Epoch [5/10], Training Loss: 0.683, Validation Accuracy: 61.77%\n",
            "Epoch [6/10], Training Loss: 0.653, Validation Accuracy: 61.50%\n",
            "Epoch [7/10], Training Loss: 0.631, Validation Accuracy: 61.37%\n",
            "Epoch [8/10], Training Loss: 0.612, Validation Accuracy: 61.56%\n",
            "Epoch [9/10], Training Loss: 0.589, Validation Accuracy: 60.84%\n",
            "Epoch [10/10], Training Loss: 0.569, Validation Accuracy: 60.61%\n",
            "Epoch [1/10], Training Loss: 0.872, Validation Accuracy: 60.65%\n",
            "Epoch [2/10], Training Loss: 0.777, Validation Accuracy: 61.85%\n",
            "Epoch [3/10], Training Loss: 0.720, Validation Accuracy: 60.85%\n",
            "Epoch [4/10], Training Loss: 0.690, Validation Accuracy: 61.79%\n",
            "Epoch [5/10], Training Loss: 0.659, Validation Accuracy: 61.36%\n",
            "Epoch [6/10], Training Loss: 0.630, Validation Accuracy: 61.12%\n",
            "Epoch [7/10], Training Loss: 0.610, Validation Accuracy: 60.98%\n",
            "Epoch [8/10], Training Loss: 0.585, Validation Accuracy: 61.29%\n",
            "Epoch [9/10], Training Loss: 0.571, Validation Accuracy: 60.58%\n",
            "Epoch [10/10], Training Loss: 0.544, Validation Accuracy: 60.92%\n",
            "Epoch [1/10], Training Loss: 0.884, Validation Accuracy: 60.62%\n",
            "Epoch [2/10], Training Loss: 0.775, Validation Accuracy: 61.83%\n",
            "Epoch [3/10], Training Loss: 0.717, Validation Accuracy: 61.24%\n",
            "Epoch [4/10], Training Loss: 0.681, Validation Accuracy: 61.40%\n",
            "Epoch [5/10], Training Loss: 0.652, Validation Accuracy: 61.68%\n",
            "Epoch [6/10], Training Loss: 0.628, Validation Accuracy: 61.50%\n",
            "Epoch [7/10], Training Loss: 0.603, Validation Accuracy: 61.63%\n",
            "Epoch [8/10], Training Loss: 0.573, Validation Accuracy: 61.41%\n",
            "Epoch [9/10], Training Loss: 0.554, Validation Accuracy: 60.31%\n",
            "Epoch [10/10], Training Loss: 0.546, Validation Accuracy: 61.57%\n",
            "Epoch [1/10], Training Loss: 0.861, Validation Accuracy: 61.68%\n",
            "Epoch [2/10], Training Loss: 0.762, Validation Accuracy: 61.64%\n",
            "Epoch [3/10], Training Loss: 0.713, Validation Accuracy: 61.94%\n",
            "Epoch [4/10], Training Loss: 0.673, Validation Accuracy: 61.83%\n",
            "Epoch [5/10], Training Loss: 0.644, Validation Accuracy: 61.78%\n",
            "Epoch [6/10], Training Loss: 0.608, Validation Accuracy: 61.76%\n",
            "Epoch [7/10], Training Loss: 0.584, Validation Accuracy: 60.78%\n",
            "Epoch [8/10], Training Loss: 0.568, Validation Accuracy: 61.73%\n",
            "Epoch [9/10], Training Loss: 0.538, Validation Accuracy: 60.94%\n",
            "Epoch [10/10], Training Loss: 0.526, Validation Accuracy: 60.55%\n",
            "Epoch [1/10], Training Loss: 0.880, Validation Accuracy: 60.74%\n",
            "Epoch [2/10], Training Loss: 0.768, Validation Accuracy: 61.38%\n",
            "Epoch [3/10], Training Loss: 0.724, Validation Accuracy: 60.49%\n",
            "Epoch [4/10], Training Loss: 0.681, Validation Accuracy: 61.42%\n",
            "Epoch [5/10], Training Loss: 0.645, Validation Accuracy: 60.78%\n",
            "Epoch [6/10], Training Loss: 0.617, Validation Accuracy: 61.20%\n",
            "Epoch [7/10], Training Loss: 0.591, Validation Accuracy: 61.35%\n",
            "Epoch [8/10], Training Loss: 0.567, Validation Accuracy: 60.90%\n",
            "Epoch [9/10], Training Loss: 0.550, Validation Accuracy: 60.79%\n",
            "Epoch [10/10], Training Loss: 0.534, Validation Accuracy: 60.38%\n",
            "Epoch [1/10], Training Loss: 0.849, Validation Accuracy: 60.87%\n",
            "Epoch [2/10], Training Loss: 0.729, Validation Accuracy: 61.10%\n",
            "Epoch [3/10], Training Loss: 0.668, Validation Accuracy: 60.64%\n",
            "Epoch [4/10], Training Loss: 0.644, Validation Accuracy: 60.93%\n",
            "Epoch [5/10], Training Loss: 0.604, Validation Accuracy: 61.80%\n",
            "Epoch [6/10], Training Loss: 0.561, Validation Accuracy: 61.63%\n",
            "Epoch [7/10], Training Loss: 0.536, Validation Accuracy: 61.54%\n",
            "Epoch [8/10], Training Loss: 0.517, Validation Accuracy: 61.33%\n",
            "Epoch [9/10], Training Loss: 0.488, Validation Accuracy: 60.96%\n",
            "Epoch [10/10], Training Loss: 0.485, Validation Accuracy: 60.61%\n",
            "Epoch [1/10], Training Loss: 0.839, Validation Accuracy: 61.44%\n",
            "Epoch [2/10], Training Loss: 0.719, Validation Accuracy: 61.04%\n",
            "Epoch [3/10], Training Loss: 0.657, Validation Accuracy: 61.41%\n",
            "Epoch [4/10], Training Loss: 0.603, Validation Accuracy: 61.44%\n",
            "Epoch [5/10], Training Loss: 0.566, Validation Accuracy: 61.18%\n",
            "Epoch [6/10], Training Loss: 0.547, Validation Accuracy: 61.14%\n",
            "Epoch [7/10], Training Loss: 0.530, Validation Accuracy: 60.63%\n",
            "Epoch [8/10], Training Loss: 0.497, Validation Accuracy: 61.14%\n",
            "Epoch [9/10], Training Loss: 0.482, Validation Accuracy: 60.29%\n",
            "Epoch [10/10], Training Loss: 0.464, Validation Accuracy: 60.97%\n",
            "Epoch [1/10], Training Loss: 0.830, Validation Accuracy: 60.79%\n",
            "Epoch [2/10], Training Loss: 0.706, Validation Accuracy: 61.16%\n",
            "Epoch [3/10], Training Loss: 0.651, Validation Accuracy: 61.51%\n",
            "Epoch [4/10], Training Loss: 0.605, Validation Accuracy: 60.50%\n",
            "Epoch [5/10], Training Loss: 0.573, Validation Accuracy: 61.18%\n",
            "Epoch [6/10], Training Loss: 0.531, Validation Accuracy: 60.93%\n",
            "Epoch [7/10], Training Loss: 0.520, Validation Accuracy: 61.64%\n",
            "Epoch [8/10], Training Loss: 0.500, Validation Accuracy: 60.87%\n",
            "Epoch [9/10], Training Loss: 0.471, Validation Accuracy: 60.56%\n",
            "Epoch [10/10], Training Loss: 0.453, Validation Accuracy: 60.93%\n",
            "Epoch [1/10], Training Loss: 0.829, Validation Accuracy: 60.76%\n",
            "Epoch [2/10], Training Loss: 0.699, Validation Accuracy: 61.07%\n",
            "Epoch [3/10], Training Loss: 0.632, Validation Accuracy: 61.36%\n",
            "Epoch [4/10], Training Loss: 0.586, Validation Accuracy: 61.44%\n",
            "Epoch [5/10], Training Loss: 0.551, Validation Accuracy: 61.30%\n",
            "Epoch [6/10], Training Loss: 0.526, Validation Accuracy: 60.60%\n",
            "Epoch [7/10], Training Loss: 0.503, Validation Accuracy: 61.56%\n",
            "Epoch [8/10], Training Loss: 0.464, Validation Accuracy: 61.10%\n",
            "Epoch [9/10], Training Loss: 0.458, Validation Accuracy: 61.02%\n",
            "Epoch [10/10], Training Loss: 0.431, Validation Accuracy: 60.76%\n",
            "Epoch [1/10], Training Loss: 0.849, Validation Accuracy: 60.40%\n",
            "Epoch [2/10], Training Loss: 0.726, Validation Accuracy: 61.16%\n",
            "Epoch [3/10], Training Loss: 0.639, Validation Accuracy: 60.92%\n",
            "Epoch [4/10], Training Loss: 0.596, Validation Accuracy: 61.04%\n",
            "Epoch [5/10], Training Loss: 0.561, Validation Accuracy: 60.26%\n",
            "Epoch [6/10], Training Loss: 0.530, Validation Accuracy: 60.23%\n",
            "Epoch [7/10], Training Loss: 0.497, Validation Accuracy: 60.99%\n",
            "Epoch [8/10], Training Loss: 0.467, Validation Accuracy: 60.41%\n",
            "Epoch [9/10], Training Loss: 0.461, Validation Accuracy: 60.54%\n",
            "Epoch [10/10], Training Loss: 0.431, Validation Accuracy: 60.81%\n",
            "Epoch [1/10], Training Loss: 0.806, Validation Accuracy: 60.84%\n",
            "Epoch [2/10], Training Loss: 0.657, Validation Accuracy: 61.72%\n",
            "Epoch [3/10], Training Loss: 0.598, Validation Accuracy: 61.17%\n",
            "Epoch [4/10], Training Loss: 0.549, Validation Accuracy: 60.98%\n",
            "Epoch [5/10], Training Loss: 0.509, Validation Accuracy: 61.07%\n",
            "Epoch [6/10], Training Loss: 0.480, Validation Accuracy: 60.84%\n",
            "Epoch [7/10], Training Loss: 0.454, Validation Accuracy: 60.46%\n",
            "Epoch [8/10], Training Loss: 0.425, Validation Accuracy: 61.36%\n",
            "Epoch [9/10], Training Loss: 0.405, Validation Accuracy: 60.94%\n",
            "Epoch [10/10], Training Loss: 0.379, Validation Accuracy: 60.86%\n",
            "Confusion Matrix:\n",
            "[[627  29  69  36  39  10  16  16 102  56]\n",
            " [ 33 675  12  26   8   3  17  10  55 161]\n",
            " [ 57   8 507 121  93  59  60  43  39  13]\n",
            " [ 14  20  78 503  84 128  51  57  22  43]\n",
            " [ 23   5 108 105 565  29  42  97  16  10]\n",
            " [  9  11  72 290  57 422  28  76  21  14]\n",
            " [  7   9  86 117  62  22 635  19  16  27]\n",
            " [ 10   9  47  82  73  55   9 682  10  23]\n",
            " [ 65  45  24  29  14   4  10   6 767  36]\n",
            " [ 36  94  17  32  15   7  16  24  63 696]]\n",
            "Test Accuracy: 60.79%\n",
            "True Positives (TP): [627 675 507 503 565 422 635 682 767 696]\n",
            "False Positives (FP): [254 230 513 838 445 317 249 348 344 383]\n",
            "True Negatives (TN): [8746 8770 8487 8162 8555 8683 8751 8652 8656 8617]\n",
            "False Negatives (FN): [373 325 493 497 435 578 365 318 233 304]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.71169126 0.74585635 0.49705882 0.37509321 0.55940594 0.57104195\n",
            " 0.71832579 0.66213592 0.69036904 0.64504171]\n",
            "Recall: [0.627 0.675 0.507 0.503 0.565 0.422 0.635 0.682 0.767 0.696]\n",
            "F1 Score: [0.66666667 0.70866142 0.5019802  0.42973088 0.56218905 0.4853364\n",
            " 0.67409766 0.67192118 0.72666982 0.66955267]\n",
            "CPU times: user 3h 14min 41s, sys: 1min 16s, total: 3h 15min 57s\n",
            "Wall time: 3h 31min 10s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 3):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=3):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_normal: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = ( augmented_data_normal + augmented_data_truncated) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=3)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"normal\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bETA=4"
      ],
      "metadata": {
        "id": "pqQW5hasylVs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n7qHcJNhynMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfe82070-3c67-4c8a-a439-a30ced8a2a61",
        "id": "uUrkj-5hynpb"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Images per Class: [6006 5960 5909 6047 6045 6020 6022 5986 5973 6032]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<timed exec>:281: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Training Loss: 2.303, Validation Accuracy: 9.86%\n",
            "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 10.42%\n",
            "Epoch [3/10], Training Loss: 2.302, Validation Accuracy: 11.88%\n",
            "Epoch [4/10], Training Loss: 2.301, Validation Accuracy: 13.08%\n",
            "Epoch [5/10], Training Loss: 2.301, Validation Accuracy: 13.65%\n",
            "Epoch [6/10], Training Loss: 2.300, Validation Accuracy: 14.10%\n",
            "Epoch [7/10], Training Loss: 2.299, Validation Accuracy: 14.34%\n",
            "Epoch [8/10], Training Loss: 2.298, Validation Accuracy: 14.59%\n",
            "Epoch [9/10], Training Loss: 2.297, Validation Accuracy: 14.70%\n",
            "Epoch [10/10], Training Loss: 2.296, Validation Accuracy: 15.12%\n",
            "Epoch [1/10], Training Loss: 2.295, Validation Accuracy: 15.23%\n",
            "Epoch [2/10], Training Loss: 2.293, Validation Accuracy: 16.14%\n",
            "Epoch [3/10], Training Loss: 2.290, Validation Accuracy: 17.62%\n",
            "Epoch [4/10], Training Loss: 2.286, Validation Accuracy: 19.01%\n",
            "Epoch [5/10], Training Loss: 2.280, Validation Accuracy: 19.57%\n",
            "Epoch [6/10], Training Loss: 2.272, Validation Accuracy: 20.02%\n",
            "Epoch [7/10], Training Loss: 2.259, Validation Accuracy: 20.68%\n",
            "Epoch [8/10], Training Loss: 2.238, Validation Accuracy: 20.84%\n",
            "Epoch [9/10], Training Loss: 2.203, Validation Accuracy: 21.55%\n",
            "Epoch [10/10], Training Loss: 2.153, Validation Accuracy: 22.81%\n",
            "Epoch [1/10], Training Loss: 2.101, Validation Accuracy: 24.94%\n",
            "Epoch [2/10], Training Loss: 2.050, Validation Accuracy: 26.57%\n",
            "Epoch [3/10], Training Loss: 2.014, Validation Accuracy: 27.42%\n",
            "Epoch [4/10], Training Loss: 1.988, Validation Accuracy: 27.88%\n",
            "Epoch [5/10], Training Loss: 1.966, Validation Accuracy: 27.14%\n",
            "Epoch [6/10], Training Loss: 1.951, Validation Accuracy: 27.88%\n",
            "Epoch [7/10], Training Loss: 1.931, Validation Accuracy: 28.51%\n",
            "Epoch [8/10], Training Loss: 1.912, Validation Accuracy: 30.40%\n",
            "Epoch [9/10], Training Loss: 1.899, Validation Accuracy: 31.09%\n",
            "Epoch [10/10], Training Loss: 1.885, Validation Accuracy: 31.06%\n",
            "Epoch [1/10], Training Loss: 1.881, Validation Accuracy: 30.86%\n",
            "Epoch [2/10], Training Loss: 1.870, Validation Accuracy: 31.29%\n",
            "Epoch [3/10], Training Loss: 1.858, Validation Accuracy: 32.18%\n",
            "Epoch [4/10], Training Loss: 1.847, Validation Accuracy: 32.66%\n",
            "Epoch [5/10], Training Loss: 1.833, Validation Accuracy: 32.99%\n",
            "Epoch [6/10], Training Loss: 1.821, Validation Accuracy: 33.78%\n",
            "Epoch [7/10], Training Loss: 1.807, Validation Accuracy: 34.59%\n",
            "Epoch [8/10], Training Loss: 1.799, Validation Accuracy: 33.80%\n",
            "Epoch [9/10], Training Loss: 1.788, Validation Accuracy: 34.30%\n",
            "Epoch [10/10], Training Loss: 1.774, Validation Accuracy: 34.57%\n",
            "Epoch [1/10], Training Loss: 1.761, Validation Accuracy: 35.66%\n",
            "Epoch [2/10], Training Loss: 1.748, Validation Accuracy: 36.45%\n",
            "Epoch [3/10], Training Loss: 1.734, Validation Accuracy: 36.85%\n",
            "Epoch [4/10], Training Loss: 1.725, Validation Accuracy: 36.46%\n",
            "Epoch [5/10], Training Loss: 1.721, Validation Accuracy: 36.62%\n",
            "Epoch [6/10], Training Loss: 1.705, Validation Accuracy: 37.37%\n",
            "Epoch [7/10], Training Loss: 1.699, Validation Accuracy: 37.14%\n",
            "Epoch [8/10], Training Loss: 1.687, Validation Accuracy: 38.32%\n",
            "Epoch [9/10], Training Loss: 1.681, Validation Accuracy: 37.91%\n",
            "Epoch [10/10], Training Loss: 1.672, Validation Accuracy: 38.33%\n",
            "Epoch [1/10], Training Loss: 1.682, Validation Accuracy: 38.24%\n",
            "Epoch [2/10], Training Loss: 1.669, Validation Accuracy: 38.87%\n",
            "Epoch [3/10], Training Loss: 1.660, Validation Accuracy: 39.53%\n",
            "Epoch [4/10], Training Loss: 1.653, Validation Accuracy: 39.31%\n",
            "Epoch [5/10], Training Loss: 1.640, Validation Accuracy: 38.64%\n",
            "Epoch [6/10], Training Loss: 1.632, Validation Accuracy: 40.36%\n",
            "Epoch [7/10], Training Loss: 1.621, Validation Accuracy: 40.11%\n",
            "Epoch [8/10], Training Loss: 1.614, Validation Accuracy: 40.55%\n",
            "Epoch [9/10], Training Loss: 1.609, Validation Accuracy: 40.73%\n",
            "Epoch [10/10], Training Loss: 1.601, Validation Accuracy: 40.80%\n",
            "Epoch [1/10], Training Loss: 1.629, Validation Accuracy: 40.31%\n",
            "Epoch [2/10], Training Loss: 1.616, Validation Accuracy: 41.05%\n",
            "Epoch [3/10], Training Loss: 1.605, Validation Accuracy: 41.42%\n",
            "Epoch [4/10], Training Loss: 1.601, Validation Accuracy: 41.84%\n",
            "Epoch [5/10], Training Loss: 1.585, Validation Accuracy: 42.05%\n",
            "Epoch [6/10], Training Loss: 1.585, Validation Accuracy: 41.91%\n",
            "Epoch [7/10], Training Loss: 1.574, Validation Accuracy: 42.16%\n",
            "Epoch [8/10], Training Loss: 1.561, Validation Accuracy: 42.48%\n",
            "Epoch [9/10], Training Loss: 1.561, Validation Accuracy: 43.03%\n",
            "Epoch [10/10], Training Loss: 1.548, Validation Accuracy: 42.90%\n",
            "Epoch [1/10], Training Loss: 1.577, Validation Accuracy: 43.60%\n",
            "Epoch [2/10], Training Loss: 1.567, Validation Accuracy: 42.90%\n",
            "Epoch [3/10], Training Loss: 1.558, Validation Accuracy: 44.46%\n",
            "Epoch [4/10], Training Loss: 1.543, Validation Accuracy: 44.21%\n",
            "Epoch [5/10], Training Loss: 1.535, Validation Accuracy: 43.76%\n",
            "Epoch [6/10], Training Loss: 1.527, Validation Accuracy: 44.72%\n",
            "Epoch [7/10], Training Loss: 1.515, Validation Accuracy: 45.26%\n",
            "Epoch [8/10], Training Loss: 1.506, Validation Accuracy: 45.00%\n",
            "Epoch [9/10], Training Loss: 1.504, Validation Accuracy: 45.51%\n",
            "Epoch [10/10], Training Loss: 1.498, Validation Accuracy: 45.26%\n",
            "Epoch [1/10], Training Loss: 1.522, Validation Accuracy: 44.69%\n",
            "Epoch [2/10], Training Loss: 1.499, Validation Accuracy: 45.57%\n",
            "Epoch [3/10], Training Loss: 1.492, Validation Accuracy: 44.85%\n",
            "Epoch [4/10], Training Loss: 1.483, Validation Accuracy: 45.70%\n",
            "Epoch [5/10], Training Loss: 1.475, Validation Accuracy: 45.58%\n",
            "Epoch [6/10], Training Loss: 1.460, Validation Accuracy: 45.70%\n",
            "Epoch [7/10], Training Loss: 1.454, Validation Accuracy: 46.55%\n",
            "Epoch [8/10], Training Loss: 1.449, Validation Accuracy: 46.19%\n",
            "Epoch [9/10], Training Loss: 1.443, Validation Accuracy: 47.27%\n",
            "Epoch [10/10], Training Loss: 1.435, Validation Accuracy: 47.16%\n",
            "Epoch [1/10], Training Loss: 1.456, Validation Accuracy: 46.58%\n",
            "Epoch [2/10], Training Loss: 1.442, Validation Accuracy: 47.59%\n",
            "Epoch [3/10], Training Loss: 1.426, Validation Accuracy: 47.98%\n",
            "Epoch [4/10], Training Loss: 1.420, Validation Accuracy: 47.97%\n",
            "Epoch [5/10], Training Loss: 1.418, Validation Accuracy: 47.10%\n",
            "Epoch [6/10], Training Loss: 1.420, Validation Accuracy: 47.93%\n",
            "Epoch [7/10], Training Loss: 1.404, Validation Accuracy: 48.19%\n",
            "Epoch [8/10], Training Loss: 1.387, Validation Accuracy: 48.32%\n",
            "Epoch [9/10], Training Loss: 1.385, Validation Accuracy: 48.08%\n",
            "Epoch [10/10], Training Loss: 1.380, Validation Accuracy: 48.58%\n",
            "Epoch [1/10], Training Loss: 1.407, Validation Accuracy: 48.72%\n",
            "Epoch [2/10], Training Loss: 1.393, Validation Accuracy: 49.19%\n",
            "Epoch [3/10], Training Loss: 1.382, Validation Accuracy: 49.35%\n",
            "Epoch [4/10], Training Loss: 1.374, Validation Accuracy: 49.28%\n",
            "Epoch [5/10], Training Loss: 1.367, Validation Accuracy: 49.06%\n",
            "Epoch [6/10], Training Loss: 1.358, Validation Accuracy: 48.91%\n",
            "Epoch [7/10], Training Loss: 1.351, Validation Accuracy: 49.13%\n",
            "Epoch [8/10], Training Loss: 1.342, Validation Accuracy: 49.25%\n",
            "Epoch [9/10], Training Loss: 1.337, Validation Accuracy: 48.90%\n",
            "Epoch [10/10], Training Loss: 1.324, Validation Accuracy: 50.24%\n",
            "Epoch [1/10], Training Loss: 1.374, Validation Accuracy: 49.56%\n",
            "Epoch [2/10], Training Loss: 1.368, Validation Accuracy: 49.83%\n",
            "Epoch [3/10], Training Loss: 1.347, Validation Accuracy: 50.60%\n",
            "Epoch [4/10], Training Loss: 1.347, Validation Accuracy: 49.93%\n",
            "Epoch [5/10], Training Loss: 1.333, Validation Accuracy: 50.11%\n",
            "Epoch [6/10], Training Loss: 1.327, Validation Accuracy: 50.13%\n",
            "Epoch [7/10], Training Loss: 1.312, Validation Accuracy: 50.02%\n",
            "Epoch [8/10], Training Loss: 1.309, Validation Accuracy: 49.97%\n",
            "Epoch [9/10], Training Loss: 1.298, Validation Accuracy: 50.70%\n",
            "Epoch [10/10], Training Loss: 1.289, Validation Accuracy: 50.27%\n",
            "Epoch [1/10], Training Loss: 1.357, Validation Accuracy: 51.01%\n",
            "Epoch [2/10], Training Loss: 1.334, Validation Accuracy: 50.84%\n",
            "Epoch [3/10], Training Loss: 1.313, Validation Accuracy: 51.58%\n",
            "Epoch [4/10], Training Loss: 1.316, Validation Accuracy: 50.86%\n",
            "Epoch [5/10], Training Loss: 1.302, Validation Accuracy: 51.96%\n",
            "Epoch [6/10], Training Loss: 1.289, Validation Accuracy: 52.05%\n",
            "Epoch [7/10], Training Loss: 1.279, Validation Accuracy: 51.77%\n",
            "Epoch [8/10], Training Loss: 1.283, Validation Accuracy: 51.79%\n",
            "Epoch [9/10], Training Loss: 1.269, Validation Accuracy: 51.91%\n",
            "Epoch [10/10], Training Loss: 1.255, Validation Accuracy: 51.54%\n",
            "Epoch [1/10], Training Loss: 1.311, Validation Accuracy: 51.94%\n",
            "Epoch [2/10], Training Loss: 1.296, Validation Accuracy: 52.70%\n",
            "Epoch [3/10], Training Loss: 1.280, Validation Accuracy: 52.79%\n",
            "Epoch [4/10], Training Loss: 1.268, Validation Accuracy: 52.55%\n",
            "Epoch [5/10], Training Loss: 1.262, Validation Accuracy: 53.22%\n",
            "Epoch [6/10], Training Loss: 1.246, Validation Accuracy: 53.26%\n",
            "Epoch [7/10], Training Loss: 1.246, Validation Accuracy: 53.33%\n",
            "Epoch [8/10], Training Loss: 1.233, Validation Accuracy: 52.90%\n",
            "Epoch [9/10], Training Loss: 1.225, Validation Accuracy: 53.01%\n",
            "Epoch [10/10], Training Loss: 1.216, Validation Accuracy: 52.69%\n",
            "Epoch [1/10], Training Loss: 1.268, Validation Accuracy: 53.54%\n",
            "Epoch [2/10], Training Loss: 1.254, Validation Accuracy: 52.97%\n",
            "Epoch [3/10], Training Loss: 1.237, Validation Accuracy: 53.74%\n",
            "Epoch [4/10], Training Loss: 1.231, Validation Accuracy: 54.30%\n",
            "Epoch [5/10], Training Loss: 1.229, Validation Accuracy: 53.19%\n",
            "Epoch [6/10], Training Loss: 1.208, Validation Accuracy: 53.87%\n",
            "Epoch [7/10], Training Loss: 1.204, Validation Accuracy: 54.57%\n",
            "Epoch [8/10], Training Loss: 1.195, Validation Accuracy: 54.17%\n",
            "Epoch [9/10], Training Loss: 1.192, Validation Accuracy: 54.13%\n",
            "Epoch [10/10], Training Loss: 1.173, Validation Accuracy: 54.36%\n",
            "Epoch [1/10], Training Loss: 1.239, Validation Accuracy: 55.05%\n",
            "Epoch [2/10], Training Loss: 1.214, Validation Accuracy: 54.80%\n",
            "Epoch [3/10], Training Loss: 1.200, Validation Accuracy: 54.37%\n",
            "Epoch [4/10], Training Loss: 1.199, Validation Accuracy: 54.73%\n",
            "Epoch [5/10], Training Loss: 1.175, Validation Accuracy: 55.20%\n",
            "Epoch [6/10], Training Loss: 1.169, Validation Accuracy: 53.60%\n",
            "Epoch [7/10], Training Loss: 1.164, Validation Accuracy: 54.81%\n",
            "Epoch [8/10], Training Loss: 1.143, Validation Accuracy: 54.13%\n",
            "Epoch [9/10], Training Loss: 1.146, Validation Accuracy: 55.15%\n",
            "Epoch [10/10], Training Loss: 1.131, Validation Accuracy: 54.90%\n",
            "Epoch [1/10], Training Loss: 1.214, Validation Accuracy: 55.08%\n",
            "Epoch [2/10], Training Loss: 1.191, Validation Accuracy: 55.55%\n",
            "Epoch [3/10], Training Loss: 1.175, Validation Accuracy: 56.06%\n",
            "Epoch [4/10], Training Loss: 1.164, Validation Accuracy: 55.55%\n",
            "Epoch [5/10], Training Loss: 1.152, Validation Accuracy: 55.84%\n",
            "Epoch [6/10], Training Loss: 1.144, Validation Accuracy: 56.35%\n",
            "Epoch [7/10], Training Loss: 1.134, Validation Accuracy: 56.20%\n",
            "Epoch [8/10], Training Loss: 1.136, Validation Accuracy: 55.43%\n",
            "Epoch [9/10], Training Loss: 1.114, Validation Accuracy: 56.53%\n",
            "Epoch [10/10], Training Loss: 1.104, Validation Accuracy: 55.90%\n",
            "Epoch [1/10], Training Loss: 1.213, Validation Accuracy: 57.17%\n",
            "Epoch [2/10], Training Loss: 1.175, Validation Accuracy: 57.19%\n",
            "Epoch [3/10], Training Loss: 1.154, Validation Accuracy: 56.54%\n",
            "Epoch [4/10], Training Loss: 1.146, Validation Accuracy: 56.91%\n",
            "Epoch [5/10], Training Loss: 1.135, Validation Accuracy: 56.98%\n",
            "Epoch [6/10], Training Loss: 1.112, Validation Accuracy: 57.12%\n",
            "Epoch [7/10], Training Loss: 1.103, Validation Accuracy: 57.00%\n",
            "Epoch [8/10], Training Loss: 1.095, Validation Accuracy: 57.31%\n",
            "Epoch [9/10], Training Loss: 1.083, Validation Accuracy: 56.79%\n",
            "Epoch [10/10], Training Loss: 1.081, Validation Accuracy: 57.40%\n",
            "Epoch [1/10], Training Loss: 1.169, Validation Accuracy: 56.31%\n",
            "Epoch [2/10], Training Loss: 1.150, Validation Accuracy: 57.26%\n",
            "Epoch [3/10], Training Loss: 1.125, Validation Accuracy: 57.87%\n",
            "Epoch [4/10], Training Loss: 1.127, Validation Accuracy: 57.91%\n",
            "Epoch [5/10], Training Loss: 1.110, Validation Accuracy: 57.91%\n",
            "Epoch [6/10], Training Loss: 1.090, Validation Accuracy: 57.41%\n",
            "Epoch [7/10], Training Loss: 1.087, Validation Accuracy: 57.15%\n",
            "Epoch [8/10], Training Loss: 1.074, Validation Accuracy: 56.83%\n",
            "Epoch [9/10], Training Loss: 1.058, Validation Accuracy: 57.81%\n",
            "Epoch [10/10], Training Loss: 1.057, Validation Accuracy: 57.69%\n",
            "Epoch [1/10], Training Loss: 1.143, Validation Accuracy: 58.56%\n",
            "Epoch [2/10], Training Loss: 1.115, Validation Accuracy: 57.28%\n",
            "Epoch [3/10], Training Loss: 1.096, Validation Accuracy: 58.38%\n",
            "Epoch [4/10], Training Loss: 1.077, Validation Accuracy: 58.04%\n",
            "Epoch [5/10], Training Loss: 1.067, Validation Accuracy: 58.46%\n",
            "Epoch [6/10], Training Loss: 1.061, Validation Accuracy: 58.64%\n",
            "Epoch [7/10], Training Loss: 1.043, Validation Accuracy: 58.41%\n",
            "Epoch [8/10], Training Loss: 1.036, Validation Accuracy: 58.47%\n",
            "Epoch [9/10], Training Loss: 1.024, Validation Accuracy: 57.12%\n",
            "Epoch [10/10], Training Loss: 1.014, Validation Accuracy: 57.92%\n",
            "Epoch [1/10], Training Loss: 1.119, Validation Accuracy: 58.85%\n",
            "Epoch [2/10], Training Loss: 1.076, Validation Accuracy: 58.63%\n",
            "Epoch [3/10], Training Loss: 1.068, Validation Accuracy: 58.27%\n",
            "Epoch [4/10], Training Loss: 1.051, Validation Accuracy: 59.28%\n",
            "Epoch [5/10], Training Loss: 1.035, Validation Accuracy: 59.03%\n",
            "Epoch [6/10], Training Loss: 1.029, Validation Accuracy: 58.51%\n",
            "Epoch [7/10], Training Loss: 1.012, Validation Accuracy: 57.96%\n",
            "Epoch [8/10], Training Loss: 1.004, Validation Accuracy: 58.13%\n",
            "Epoch [9/10], Training Loss: 0.995, Validation Accuracy: 58.90%\n",
            "Epoch [10/10], Training Loss: 0.983, Validation Accuracy: 58.67%\n",
            "Epoch [1/10], Training Loss: 1.101, Validation Accuracy: 59.02%\n",
            "Epoch [2/10], Training Loss: 1.076, Validation Accuracy: 59.21%\n",
            "Epoch [3/10], Training Loss: 1.049, Validation Accuracy: 59.59%\n",
            "Epoch [4/10], Training Loss: 1.032, Validation Accuracy: 59.71%\n",
            "Epoch [5/10], Training Loss: 1.019, Validation Accuracy: 59.55%\n",
            "Epoch [6/10], Training Loss: 1.008, Validation Accuracy: 58.97%\n",
            "Epoch [7/10], Training Loss: 0.995, Validation Accuracy: 59.72%\n",
            "Epoch [8/10], Training Loss: 0.991, Validation Accuracy: 59.96%\n",
            "Epoch [9/10], Training Loss: 0.971, Validation Accuracy: 59.66%\n",
            "Epoch [10/10], Training Loss: 0.962, Validation Accuracy: 59.55%\n",
            "Epoch [1/10], Training Loss: 1.086, Validation Accuracy: 60.12%\n",
            "Epoch [2/10], Training Loss: 1.056, Validation Accuracy: 60.05%\n",
            "Epoch [3/10], Training Loss: 1.032, Validation Accuracy: 60.23%\n",
            "Epoch [4/10], Training Loss: 1.017, Validation Accuracy: 59.03%\n",
            "Epoch [5/10], Training Loss: 1.005, Validation Accuracy: 59.28%\n",
            "Epoch [6/10], Training Loss: 0.993, Validation Accuracy: 59.61%\n",
            "Epoch [7/10], Training Loss: 0.985, Validation Accuracy: 60.08%\n",
            "Epoch [8/10], Training Loss: 0.961, Validation Accuracy: 59.13%\n",
            "Epoch [9/10], Training Loss: 0.951, Validation Accuracy: 59.73%\n",
            "Epoch [10/10], Training Loss: 0.944, Validation Accuracy: 59.82%\n",
            "Epoch [1/10], Training Loss: 1.082, Validation Accuracy: 59.85%\n",
            "Epoch [2/10], Training Loss: 1.038, Validation Accuracy: 60.18%\n",
            "Epoch [3/10], Training Loss: 1.016, Validation Accuracy: 60.50%\n",
            "Epoch [4/10], Training Loss: 1.005, Validation Accuracy: 60.34%\n",
            "Epoch [5/10], Training Loss: 0.990, Validation Accuracy: 60.53%\n",
            "Epoch [6/10], Training Loss: 0.977, Validation Accuracy: 59.24%\n",
            "Epoch [7/10], Training Loss: 0.969, Validation Accuracy: 60.14%\n",
            "Epoch [8/10], Training Loss: 0.948, Validation Accuracy: 60.10%\n",
            "Epoch [9/10], Training Loss: 0.937, Validation Accuracy: 59.51%\n",
            "Epoch [10/10], Training Loss: 0.926, Validation Accuracy: 60.31%\n",
            "Epoch [1/10], Training Loss: 1.038, Validation Accuracy: 60.27%\n",
            "Epoch [2/10], Training Loss: 1.008, Validation Accuracy: 60.14%\n",
            "Epoch [3/10], Training Loss: 0.995, Validation Accuracy: 60.63%\n",
            "Epoch [4/10], Training Loss: 0.972, Validation Accuracy: 60.63%\n",
            "Epoch [5/10], Training Loss: 0.950, Validation Accuracy: 60.43%\n",
            "Epoch [6/10], Training Loss: 0.931, Validation Accuracy: 61.09%\n",
            "Epoch [7/10], Training Loss: 0.927, Validation Accuracy: 60.67%\n",
            "Epoch [8/10], Training Loss: 0.920, Validation Accuracy: 60.07%\n",
            "Epoch [9/10], Training Loss: 0.905, Validation Accuracy: 60.76%\n",
            "Epoch [10/10], Training Loss: 0.887, Validation Accuracy: 60.59%\n",
            "Epoch [1/10], Training Loss: 1.022, Validation Accuracy: 59.82%\n",
            "Epoch [2/10], Training Loss: 0.996, Validation Accuracy: 60.09%\n",
            "Epoch [3/10], Training Loss: 0.972, Validation Accuracy: 60.63%\n",
            "Epoch [4/10], Training Loss: 0.947, Validation Accuracy: 61.00%\n",
            "Epoch [5/10], Training Loss: 0.934, Validation Accuracy: 60.97%\n",
            "Epoch [6/10], Training Loss: 0.922, Validation Accuracy: 60.71%\n",
            "Epoch [7/10], Training Loss: 0.900, Validation Accuracy: 60.32%\n",
            "Epoch [8/10], Training Loss: 0.887, Validation Accuracy: 61.06%\n",
            "Epoch [9/10], Training Loss: 0.873, Validation Accuracy: 60.45%\n",
            "Epoch [10/10], Training Loss: 0.867, Validation Accuracy: 60.93%\n",
            "Epoch [1/10], Training Loss: 1.025, Validation Accuracy: 60.96%\n",
            "Epoch [2/10], Training Loss: 0.983, Validation Accuracy: 60.52%\n",
            "Epoch [3/10], Training Loss: 0.948, Validation Accuracy: 61.16%\n",
            "Epoch [4/10], Training Loss: 0.925, Validation Accuracy: 60.43%\n",
            "Epoch [5/10], Training Loss: 0.915, Validation Accuracy: 61.53%\n",
            "Epoch [6/10], Training Loss: 0.900, Validation Accuracy: 61.54%\n",
            "Epoch [7/10], Training Loss: 0.891, Validation Accuracy: 61.34%\n",
            "Epoch [8/10], Training Loss: 0.872, Validation Accuracy: 61.48%\n",
            "Epoch [9/10], Training Loss: 0.859, Validation Accuracy: 61.34%\n",
            "Epoch [10/10], Training Loss: 0.850, Validation Accuracy: 60.65%\n",
            "Epoch [1/10], Training Loss: 1.013, Validation Accuracy: 61.22%\n",
            "Epoch [2/10], Training Loss: 0.971, Validation Accuracy: 61.26%\n",
            "Epoch [3/10], Training Loss: 0.943, Validation Accuracy: 60.92%\n",
            "Epoch [4/10], Training Loss: 0.921, Validation Accuracy: 61.52%\n",
            "Epoch [5/10], Training Loss: 0.908, Validation Accuracy: 61.46%\n",
            "Epoch [6/10], Training Loss: 0.880, Validation Accuracy: 61.59%\n",
            "Epoch [7/10], Training Loss: 0.871, Validation Accuracy: 60.35%\n",
            "Epoch [8/10], Training Loss: 0.856, Validation Accuracy: 61.36%\n",
            "Epoch [9/10], Training Loss: 0.841, Validation Accuracy: 60.86%\n",
            "Epoch [10/10], Training Loss: 0.832, Validation Accuracy: 61.34%\n",
            "Epoch [1/10], Training Loss: 0.994, Validation Accuracy: 61.27%\n",
            "Epoch [2/10], Training Loss: 0.953, Validation Accuracy: 61.03%\n",
            "Epoch [3/10], Training Loss: 0.922, Validation Accuracy: 61.21%\n",
            "Epoch [4/10], Training Loss: 0.911, Validation Accuracy: 61.34%\n",
            "Epoch [5/10], Training Loss: 0.889, Validation Accuracy: 61.53%\n",
            "Epoch [6/10], Training Loss: 0.871, Validation Accuracy: 61.59%\n",
            "Epoch [7/10], Training Loss: 0.857, Validation Accuracy: 61.51%\n",
            "Epoch [8/10], Training Loss: 0.841, Validation Accuracy: 61.56%\n",
            "Epoch [9/10], Training Loss: 0.834, Validation Accuracy: 61.71%\n",
            "Epoch [10/10], Training Loss: 0.818, Validation Accuracy: 60.96%\n",
            "Epoch [1/10], Training Loss: 0.971, Validation Accuracy: 61.67%\n",
            "Epoch [2/10], Training Loss: 0.926, Validation Accuracy: 61.72%\n",
            "Epoch [3/10], Training Loss: 0.896, Validation Accuracy: 60.54%\n",
            "Epoch [4/10], Training Loss: 0.883, Validation Accuracy: 61.72%\n",
            "Epoch [5/10], Training Loss: 0.857, Validation Accuracy: 61.64%\n",
            "Epoch [6/10], Training Loss: 0.837, Validation Accuracy: 61.93%\n",
            "Epoch [7/10], Training Loss: 0.822, Validation Accuracy: 61.33%\n",
            "Epoch [8/10], Training Loss: 0.808, Validation Accuracy: 61.49%\n",
            "Epoch [9/10], Training Loss: 0.795, Validation Accuracy: 61.68%\n",
            "Epoch [10/10], Training Loss: 0.780, Validation Accuracy: 61.87%\n",
            "Epoch [1/10], Training Loss: 0.962, Validation Accuracy: 61.65%\n",
            "Epoch [2/10], Training Loss: 0.914, Validation Accuracy: 61.97%\n",
            "Epoch [3/10], Training Loss: 0.881, Validation Accuracy: 61.44%\n",
            "Epoch [4/10], Training Loss: 0.848, Validation Accuracy: 61.70%\n",
            "Epoch [5/10], Training Loss: 0.839, Validation Accuracy: 60.76%\n",
            "Epoch [6/10], Training Loss: 0.819, Validation Accuracy: 60.59%\n",
            "Epoch [7/10], Training Loss: 0.813, Validation Accuracy: 61.90%\n",
            "Epoch [8/10], Training Loss: 0.782, Validation Accuracy: 61.53%\n",
            "Epoch [9/10], Training Loss: 0.774, Validation Accuracy: 62.07%\n",
            "Epoch [10/10], Training Loss: 0.755, Validation Accuracy: 61.13%\n",
            "Epoch [1/10], Training Loss: 0.959, Validation Accuracy: 62.38%\n",
            "Epoch [2/10], Training Loss: 0.898, Validation Accuracy: 62.21%\n",
            "Epoch [3/10], Training Loss: 0.858, Validation Accuracy: 62.20%\n",
            "Epoch [4/10], Training Loss: 0.840, Validation Accuracy: 61.40%\n",
            "Epoch [5/10], Training Loss: 0.821, Validation Accuracy: 61.97%\n",
            "Epoch [6/10], Training Loss: 0.802, Validation Accuracy: 61.24%\n",
            "Epoch [7/10], Training Loss: 0.784, Validation Accuracy: 62.38%\n",
            "Epoch [8/10], Training Loss: 0.763, Validation Accuracy: 61.46%\n",
            "Epoch [9/10], Training Loss: 0.752, Validation Accuracy: 62.23%\n",
            "Epoch [10/10], Training Loss: 0.746, Validation Accuracy: 62.13%\n",
            "Epoch [1/10], Training Loss: 0.940, Validation Accuracy: 62.43%\n",
            "Epoch [2/10], Training Loss: 0.896, Validation Accuracy: 62.31%\n",
            "Epoch [3/10], Training Loss: 0.856, Validation Accuracy: 62.37%\n",
            "Epoch [4/10], Training Loss: 0.838, Validation Accuracy: 61.80%\n",
            "Epoch [5/10], Training Loss: 0.810, Validation Accuracy: 62.85%\n",
            "Epoch [6/10], Training Loss: 0.789, Validation Accuracy: 62.33%\n",
            "Epoch [7/10], Training Loss: 0.764, Validation Accuracy: 62.22%\n",
            "Epoch [8/10], Training Loss: 0.755, Validation Accuracy: 61.38%\n",
            "Epoch [9/10], Training Loss: 0.743, Validation Accuracy: 62.53%\n",
            "Epoch [10/10], Training Loss: 0.725, Validation Accuracy: 62.13%\n",
            "Epoch [1/10], Training Loss: 0.942, Validation Accuracy: 61.46%\n",
            "Epoch [2/10], Training Loss: 0.876, Validation Accuracy: 62.72%\n",
            "Epoch [3/10], Training Loss: 0.848, Validation Accuracy: 62.48%\n",
            "Epoch [4/10], Training Loss: 0.822, Validation Accuracy: 62.14%\n",
            "Epoch [5/10], Training Loss: 0.805, Validation Accuracy: 62.47%\n",
            "Epoch [6/10], Training Loss: 0.782, Validation Accuracy: 62.11%\n",
            "Epoch [7/10], Training Loss: 0.769, Validation Accuracy: 62.24%\n",
            "Epoch [8/10], Training Loss: 0.742, Validation Accuracy: 61.81%\n",
            "Epoch [9/10], Training Loss: 0.724, Validation Accuracy: 61.92%\n",
            "Epoch [10/10], Training Loss: 0.717, Validation Accuracy: 61.70%\n",
            "Epoch [1/10], Training Loss: 0.915, Validation Accuracy: 62.90%\n",
            "Epoch [2/10], Training Loss: 0.843, Validation Accuracy: 62.43%\n",
            "Epoch [3/10], Training Loss: 0.811, Validation Accuracy: 63.00%\n",
            "Epoch [4/10], Training Loss: 0.785, Validation Accuracy: 62.65%\n",
            "Epoch [5/10], Training Loss: 0.769, Validation Accuracy: 62.44%\n",
            "Epoch [6/10], Training Loss: 0.742, Validation Accuracy: 62.35%\n",
            "Epoch [7/10], Training Loss: 0.729, Validation Accuracy: 62.43%\n",
            "Epoch [8/10], Training Loss: 0.714, Validation Accuracy: 62.95%\n",
            "Epoch [9/10], Training Loss: 0.696, Validation Accuracy: 62.50%\n",
            "Epoch [10/10], Training Loss: 0.686, Validation Accuracy: 62.21%\n",
            "Epoch [1/10], Training Loss: 0.902, Validation Accuracy: 62.17%\n",
            "Epoch [2/10], Training Loss: 0.838, Validation Accuracy: 62.82%\n",
            "Epoch [3/10], Training Loss: 0.793, Validation Accuracy: 63.11%\n",
            "Epoch [4/10], Training Loss: 0.767, Validation Accuracy: 62.51%\n",
            "Epoch [5/10], Training Loss: 0.743, Validation Accuracy: 62.54%\n",
            "Epoch [6/10], Training Loss: 0.725, Validation Accuracy: 62.31%\n",
            "Epoch [7/10], Training Loss: 0.703, Validation Accuracy: 62.98%\n",
            "Epoch [8/10], Training Loss: 0.684, Validation Accuracy: 62.52%\n",
            "Epoch [9/10], Training Loss: 0.662, Validation Accuracy: 62.39%\n",
            "Epoch [10/10], Training Loss: 0.670, Validation Accuracy: 61.14%\n",
            "Epoch [1/10], Training Loss: 0.887, Validation Accuracy: 62.68%\n",
            "Epoch [2/10], Training Loss: 0.829, Validation Accuracy: 62.63%\n",
            "Epoch [3/10], Training Loss: 0.789, Validation Accuracy: 62.89%\n",
            "Epoch [4/10], Training Loss: 0.749, Validation Accuracy: 62.56%\n",
            "Epoch [5/10], Training Loss: 0.731, Validation Accuracy: 63.06%\n",
            "Epoch [6/10], Training Loss: 0.704, Validation Accuracy: 62.61%\n",
            "Epoch [7/10], Training Loss: 0.691, Validation Accuracy: 62.48%\n",
            "Epoch [8/10], Training Loss: 0.674, Validation Accuracy: 63.15%\n",
            "Epoch [9/10], Training Loss: 0.659, Validation Accuracy: 62.37%\n",
            "Epoch [10/10], Training Loss: 0.637, Validation Accuracy: 62.68%\n",
            "Epoch [1/10], Training Loss: 0.884, Validation Accuracy: 62.97%\n",
            "Epoch [2/10], Training Loss: 0.807, Validation Accuracy: 62.36%\n",
            "Epoch [3/10], Training Loss: 0.771, Validation Accuracy: 61.95%\n",
            "Epoch [4/10], Training Loss: 0.748, Validation Accuracy: 62.94%\n",
            "Epoch [5/10], Training Loss: 0.718, Validation Accuracy: 62.35%\n",
            "Epoch [6/10], Training Loss: 0.696, Validation Accuracy: 62.60%\n",
            "Epoch [7/10], Training Loss: 0.677, Validation Accuracy: 62.04%\n",
            "Epoch [8/10], Training Loss: 0.650, Validation Accuracy: 62.89%\n",
            "Epoch [9/10], Training Loss: 0.641, Validation Accuracy: 63.06%\n",
            "Epoch [10/10], Training Loss: 0.623, Validation Accuracy: 62.45%\n",
            "Epoch [1/10], Training Loss: 0.886, Validation Accuracy: 62.66%\n",
            "Epoch [2/10], Training Loss: 0.803, Validation Accuracy: 63.28%\n",
            "Epoch [3/10], Training Loss: 0.763, Validation Accuracy: 62.77%\n",
            "Epoch [4/10], Training Loss: 0.735, Validation Accuracy: 62.39%\n",
            "Epoch [5/10], Training Loss: 0.711, Validation Accuracy: 62.77%\n",
            "Epoch [6/10], Training Loss: 0.689, Validation Accuracy: 63.13%\n",
            "Epoch [7/10], Training Loss: 0.661, Validation Accuracy: 63.21%\n",
            "Epoch [8/10], Training Loss: 0.648, Validation Accuracy: 62.38%\n",
            "Epoch [9/10], Training Loss: 0.626, Validation Accuracy: 62.56%\n",
            "Epoch [10/10], Training Loss: 0.610, Validation Accuracy: 62.68%\n",
            "Epoch [1/10], Training Loss: 0.863, Validation Accuracy: 62.91%\n",
            "Epoch [2/10], Training Loss: 0.785, Validation Accuracy: 62.91%\n",
            "Epoch [3/10], Training Loss: 0.742, Validation Accuracy: 62.92%\n",
            "Epoch [4/10], Training Loss: 0.707, Validation Accuracy: 63.62%\n",
            "Epoch [5/10], Training Loss: 0.676, Validation Accuracy: 63.14%\n",
            "Epoch [6/10], Training Loss: 0.660, Validation Accuracy: 62.64%\n",
            "Epoch [7/10], Training Loss: 0.634, Validation Accuracy: 61.73%\n",
            "Epoch [8/10], Training Loss: 0.628, Validation Accuracy: 63.14%\n",
            "Epoch [9/10], Training Loss: 0.599, Validation Accuracy: 63.15%\n",
            "Epoch [10/10], Training Loss: 0.594, Validation Accuracy: 63.09%\n",
            "Epoch [1/10], Training Loss: 0.843, Validation Accuracy: 62.91%\n",
            "Epoch [2/10], Training Loss: 0.764, Validation Accuracy: 63.01%\n",
            "Epoch [3/10], Training Loss: 0.722, Validation Accuracy: 62.65%\n",
            "Epoch [4/10], Training Loss: 0.690, Validation Accuracy: 63.05%\n",
            "Epoch [5/10], Training Loss: 0.656, Validation Accuracy: 63.13%\n",
            "Epoch [6/10], Training Loss: 0.635, Validation Accuracy: 62.74%\n",
            "Epoch [7/10], Training Loss: 0.621, Validation Accuracy: 62.93%\n",
            "Epoch [8/10], Training Loss: 0.591, Validation Accuracy: 62.57%\n",
            "Epoch [9/10], Training Loss: 0.575, Validation Accuracy: 62.72%\n",
            "Epoch [10/10], Training Loss: 0.565, Validation Accuracy: 62.96%\n",
            "Epoch [1/10], Training Loss: 0.846, Validation Accuracy: 63.60%\n",
            "Epoch [2/10], Training Loss: 0.753, Validation Accuracy: 63.58%\n",
            "Epoch [3/10], Training Loss: 0.708, Validation Accuracy: 63.11%\n",
            "Epoch [4/10], Training Loss: 0.686, Validation Accuracy: 63.09%\n",
            "Epoch [5/10], Training Loss: 0.653, Validation Accuracy: 63.18%\n",
            "Epoch [6/10], Training Loss: 0.626, Validation Accuracy: 62.93%\n",
            "Epoch [7/10], Training Loss: 0.601, Validation Accuracy: 62.96%\n",
            "Epoch [8/10], Training Loss: 0.582, Validation Accuracy: 62.80%\n",
            "Epoch [9/10], Training Loss: 0.557, Validation Accuracy: 61.37%\n",
            "Epoch [10/10], Training Loss: 0.544, Validation Accuracy: 63.00%\n",
            "Epoch [1/10], Training Loss: 0.835, Validation Accuracy: 63.13%\n",
            "Epoch [2/10], Training Loss: 0.738, Validation Accuracy: 63.11%\n",
            "Epoch [3/10], Training Loss: 0.702, Validation Accuracy: 63.13%\n",
            "Epoch [4/10], Training Loss: 0.667, Validation Accuracy: 63.28%\n",
            "Epoch [5/10], Training Loss: 0.631, Validation Accuracy: 63.36%\n",
            "Epoch [6/10], Training Loss: 0.600, Validation Accuracy: 62.96%\n",
            "Epoch [7/10], Training Loss: 0.589, Validation Accuracy: 63.11%\n",
            "Epoch [8/10], Training Loss: 0.565, Validation Accuracy: 62.92%\n",
            "Epoch [9/10], Training Loss: 0.540, Validation Accuracy: 62.54%\n",
            "Epoch [10/10], Training Loss: 0.529, Validation Accuracy: 62.97%\n",
            "Epoch [1/10], Training Loss: 0.830, Validation Accuracy: 63.10%\n",
            "Epoch [2/10], Training Loss: 0.741, Validation Accuracy: 62.90%\n",
            "Epoch [3/10], Training Loss: 0.691, Validation Accuracy: 62.90%\n",
            "Epoch [4/10], Training Loss: 0.664, Validation Accuracy: 63.12%\n",
            "Epoch [5/10], Training Loss: 0.627, Validation Accuracy: 62.27%\n",
            "Epoch [6/10], Training Loss: 0.598, Validation Accuracy: 62.94%\n",
            "Epoch [7/10], Training Loss: 0.575, Validation Accuracy: 62.40%\n",
            "Epoch [8/10], Training Loss: 0.557, Validation Accuracy: 61.34%\n",
            "Epoch [9/10], Training Loss: 0.540, Validation Accuracy: 62.53%\n",
            "Epoch [10/10], Training Loss: 0.518, Validation Accuracy: 62.45%\n",
            "Epoch [1/10], Training Loss: 0.811, Validation Accuracy: 62.97%\n",
            "Epoch [2/10], Training Loss: 0.718, Validation Accuracy: 62.82%\n",
            "Epoch [3/10], Training Loss: 0.664, Validation Accuracy: 62.77%\n",
            "Epoch [4/10], Training Loss: 0.624, Validation Accuracy: 62.57%\n",
            "Epoch [5/10], Training Loss: 0.605, Validation Accuracy: 62.82%\n",
            "Epoch [6/10], Training Loss: 0.567, Validation Accuracy: 63.19%\n",
            "Epoch [7/10], Training Loss: 0.555, Validation Accuracy: 63.37%\n",
            "Epoch [8/10], Training Loss: 0.526, Validation Accuracy: 63.24%\n",
            "Epoch [9/10], Training Loss: 0.509, Validation Accuracy: 62.81%\n",
            "Epoch [10/10], Training Loss: 0.500, Validation Accuracy: 63.05%\n",
            "Epoch [1/10], Training Loss: 0.792, Validation Accuracy: 61.69%\n",
            "Epoch [2/10], Training Loss: 0.717, Validation Accuracy: 63.28%\n",
            "Epoch [3/10], Training Loss: 0.652, Validation Accuracy: 63.09%\n",
            "Epoch [4/10], Training Loss: 0.613, Validation Accuracy: 62.89%\n",
            "Epoch [5/10], Training Loss: 0.575, Validation Accuracy: 63.17%\n",
            "Epoch [6/10], Training Loss: 0.547, Validation Accuracy: 63.20%\n",
            "Epoch [7/10], Training Loss: 0.519, Validation Accuracy: 62.11%\n",
            "Epoch [8/10], Training Loss: 0.509, Validation Accuracy: 62.87%\n",
            "Epoch [9/10], Training Loss: 0.485, Validation Accuracy: 62.46%\n",
            "Epoch [10/10], Training Loss: 0.465, Validation Accuracy: 62.62%\n",
            "Epoch [1/10], Training Loss: 0.805, Validation Accuracy: 61.59%\n",
            "Epoch [2/10], Training Loss: 0.704, Validation Accuracy: 63.24%\n",
            "Epoch [3/10], Training Loss: 0.640, Validation Accuracy: 63.42%\n",
            "Epoch [4/10], Training Loss: 0.594, Validation Accuracy: 63.29%\n",
            "Epoch [5/10], Training Loss: 0.574, Validation Accuracy: 63.15%\n",
            "Epoch [6/10], Training Loss: 0.545, Validation Accuracy: 63.13%\n",
            "Epoch [7/10], Training Loss: 0.515, Validation Accuracy: 63.52%\n",
            "Epoch [8/10], Training Loss: 0.495, Validation Accuracy: 62.83%\n",
            "Epoch [9/10], Training Loss: 0.473, Validation Accuracy: 62.38%\n",
            "Epoch [10/10], Training Loss: 0.462, Validation Accuracy: 63.36%\n",
            "Epoch [1/10], Training Loss: 0.789, Validation Accuracy: 63.01%\n",
            "Epoch [2/10], Training Loss: 0.685, Validation Accuracy: 63.00%\n",
            "Epoch [3/10], Training Loss: 0.636, Validation Accuracy: 62.77%\n",
            "Epoch [4/10], Training Loss: 0.581, Validation Accuracy: 62.88%\n",
            "Epoch [5/10], Training Loss: 0.545, Validation Accuracy: 62.82%\n",
            "Epoch [6/10], Training Loss: 0.517, Validation Accuracy: 63.10%\n",
            "Epoch [7/10], Training Loss: 0.498, Validation Accuracy: 62.75%\n",
            "Epoch [8/10], Training Loss: 0.467, Validation Accuracy: 62.33%\n",
            "Epoch [9/10], Training Loss: 0.455, Validation Accuracy: 62.51%\n",
            "Epoch [10/10], Training Loss: 0.433, Validation Accuracy: 62.42%\n",
            "Epoch [1/10], Training Loss: 0.799, Validation Accuracy: 62.07%\n",
            "Epoch [2/10], Training Loss: 0.680, Validation Accuracy: 63.16%\n",
            "Epoch [3/10], Training Loss: 0.620, Validation Accuracy: 62.81%\n",
            "Epoch [4/10], Training Loss: 0.584, Validation Accuracy: 62.60%\n",
            "Epoch [5/10], Training Loss: 0.546, Validation Accuracy: 62.67%\n",
            "Epoch [6/10], Training Loss: 0.525, Validation Accuracy: 62.13%\n",
            "Epoch [7/10], Training Loss: 0.499, Validation Accuracy: 61.85%\n",
            "Epoch [8/10], Training Loss: 0.473, Validation Accuracy: 61.69%\n",
            "Epoch [9/10], Training Loss: 0.459, Validation Accuracy: 62.66%\n",
            "Epoch [10/10], Training Loss: 0.436, Validation Accuracy: 61.29%\n",
            "Epoch [1/10], Training Loss: 0.768, Validation Accuracy: 62.23%\n",
            "Epoch [2/10], Training Loss: 0.654, Validation Accuracy: 63.12%\n",
            "Epoch [3/10], Training Loss: 0.595, Validation Accuracy: 62.95%\n",
            "Epoch [4/10], Training Loss: 0.549, Validation Accuracy: 63.04%\n",
            "Epoch [5/10], Training Loss: 0.523, Validation Accuracy: 62.61%\n",
            "Epoch [6/10], Training Loss: 0.493, Validation Accuracy: 62.07%\n",
            "Epoch [7/10], Training Loss: 0.461, Validation Accuracy: 63.44%\n",
            "Epoch [8/10], Training Loss: 0.440, Validation Accuracy: 62.69%\n",
            "Epoch [9/10], Training Loss: 0.424, Validation Accuracy: 62.86%\n",
            "Epoch [10/10], Training Loss: 0.401, Validation Accuracy: 62.20%\n",
            "Confusion Matrix:\n",
            "[[672  13  65  28  34  11  11  12 102  52]\n",
            " [ 37 728  17  15   1   6   9  12  46 129]\n",
            " [ 66   7 581  55  62  89  66  48  14  12]\n",
            " [ 36   9 109 406  55 231  66  51  13  24]\n",
            " [ 42   6 118  68 458  79  89 117  17   6]\n",
            " [ 20   3  82 162  38 542  43  88   9  13]\n",
            " [ 15   5  84  69  38  50 707  16   7   9]\n",
            " [ 19   2  72  43  51  85  12 687  13  16]\n",
            " [ 99  43  20  13  13  12   9   7 735  49]\n",
            " [ 56  80  14  27  12  16   8  32  35 720]]\n",
            "Test Accuracy: 62.36%\n",
            "True Positives (TP): [672 728 581 406 458 542 707 687 735 720]\n",
            "False Positives (FP): [390 168 581 480 304 579 313 383 256 310]\n",
            "True Negatives (TN): [8610 8832 8419 8520 8696 8421 8687 8617 8744 8690]\n",
            "False Negatives (FN): [328 272 419 594 542 458 293 313 265 280]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.63276836 0.8125     0.5        0.45823928 0.60104987 0.48349688\n",
            " 0.69313725 0.64205607 0.74167508 0.69902913]\n",
            "Recall: [0.672 0.728 0.581 0.406 0.458 0.542 0.707 0.687 0.735 0.72 ]\n",
            "F1 Score: [0.65179437 0.76793249 0.53746531 0.43054083 0.51986379 0.51107968\n",
            " 0.7        0.66376812 0.73832245 0.70935961]\n",
            "CPU times: user 3h 17min 14s, sys: 1min 22s, total: 3h 18min 36s\n",
            "Wall time: 3h 34min 19s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 4):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=4):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_normal: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = ( augmented_data_normal + augmented_data_truncated) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=4)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"normal\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTQrJoVbqSE1eVrSAI9Eay",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}