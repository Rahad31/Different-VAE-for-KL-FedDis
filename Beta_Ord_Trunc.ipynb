{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahad31/Different-VAE-for-KL-FedDis/blob/main/Beta_Ord_Trunc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpJbi0RbDhy5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QXsfWnMDwza"
      },
      "source": [
        "Beta=.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Noujn05rDvDQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMFSzAJYkag9",
        "outputId": "f6bac336-d04c-49b3-d21f-8625cd3447e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 48.1MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Images per Class: [5996 5958 5954 5965 6011 6034 6086 5912 5996 6088]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1-1355293134.py:281: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Training Loss: 2.304, Validation Accuracy: 9.90%\n",
            "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 10.30%\n",
            "Epoch [3/10], Training Loss: 2.302, Validation Accuracy: 10.51%\n",
            "Epoch [4/10], Training Loss: 2.302, Validation Accuracy: 10.93%\n",
            "Epoch [5/10], Training Loss: 2.301, Validation Accuracy: 11.54%\n",
            "Epoch [6/10], Training Loss: 2.300, Validation Accuracy: 11.20%\n",
            "Epoch [7/10], Training Loss: 2.300, Validation Accuracy: 11.46%\n",
            "Epoch [8/10], Training Loss: 2.299, Validation Accuracy: 11.72%\n",
            "Epoch [9/10], Training Loss: 2.298, Validation Accuracy: 12.03%\n",
            "Epoch [10/10], Training Loss: 2.297, Validation Accuracy: 12.85%\n",
            "Epoch [1/10], Training Loss: 2.297, Validation Accuracy: 13.43%\n",
            "Epoch [2/10], Training Loss: 2.295, Validation Accuracy: 14.59%\n",
            "Epoch [3/10], Training Loss: 2.293, Validation Accuracy: 15.73%\n",
            "Epoch [4/10], Training Loss: 2.289, Validation Accuracy: 16.18%\n",
            "Epoch [5/10], Training Loss: 2.285, Validation Accuracy: 16.20%\n",
            "Epoch [6/10], Training Loss: 2.278, Validation Accuracy: 18.79%\n",
            "Epoch [7/10], Training Loss: 2.267, Validation Accuracy: 17.16%\n",
            "Epoch [8/10], Training Loss: 2.251, Validation Accuracy: 15.96%\n",
            "Epoch [9/10], Training Loss: 2.232, Validation Accuracy: 15.65%\n",
            "Epoch [10/10], Training Loss: 2.211, Validation Accuracy: 16.20%\n",
            "Epoch [1/10], Training Loss: 2.189, Validation Accuracy: 16.61%\n",
            "Epoch [2/10], Training Loss: 2.168, Validation Accuracy: 17.47%\n",
            "Epoch [3/10], Training Loss: 2.151, Validation Accuracy: 18.33%\n",
            "Epoch [4/10], Training Loss: 2.136, Validation Accuracy: 19.80%\n",
            "Epoch [5/10], Training Loss: 2.116, Validation Accuracy: 20.81%\n",
            "Epoch [6/10], Training Loss: 2.094, Validation Accuracy: 22.97%\n",
            "Epoch [7/10], Training Loss: 2.071, Validation Accuracy: 23.64%\n",
            "Epoch [8/10], Training Loss: 2.044, Validation Accuracy: 24.88%\n",
            "Epoch [9/10], Training Loss: 2.019, Validation Accuracy: 26.26%\n",
            "Epoch [10/10], Training Loss: 1.994, Validation Accuracy: 27.36%\n",
            "Epoch [1/10], Training Loss: 1.985, Validation Accuracy: 27.68%\n",
            "Epoch [2/10], Training Loss: 1.962, Validation Accuracy: 28.72%\n",
            "Epoch [3/10], Training Loss: 1.936, Validation Accuracy: 28.52%\n",
            "Epoch [4/10], Training Loss: 1.910, Validation Accuracy: 28.93%\n",
            "Epoch [5/10], Training Loss: 1.888, Validation Accuracy: 31.12%\n",
            "Epoch [6/10], Training Loss: 1.865, Validation Accuracy: 31.50%\n",
            "Epoch [7/10], Training Loss: 1.841, Validation Accuracy: 31.80%\n",
            "Epoch [8/10], Training Loss: 1.818, Validation Accuracy: 32.72%\n",
            "Epoch [9/10], Training Loss: 1.801, Validation Accuracy: 33.18%\n",
            "Epoch [10/10], Training Loss: 1.782, Validation Accuracy: 33.15%\n",
            "Epoch [1/10], Training Loss: 1.779, Validation Accuracy: 34.29%\n",
            "Epoch [2/10], Training Loss: 1.759, Validation Accuracy: 35.03%\n",
            "Epoch [3/10], Training Loss: 1.743, Validation Accuracy: 36.85%\n",
            "Epoch [4/10], Training Loss: 1.730, Validation Accuracy: 36.16%\n",
            "Epoch [5/10], Training Loss: 1.716, Validation Accuracy: 37.79%\n",
            "Epoch [6/10], Training Loss: 1.707, Validation Accuracy: 37.61%\n",
            "Epoch [7/10], Training Loss: 1.692, Validation Accuracy: 38.42%\n",
            "Epoch [8/10], Training Loss: 1.677, Validation Accuracy: 38.40%\n",
            "Epoch [9/10], Training Loss: 1.666, Validation Accuracy: 38.92%\n",
            "Epoch [10/10], Training Loss: 1.654, Validation Accuracy: 38.96%\n",
            "Epoch [1/10], Training Loss: 1.667, Validation Accuracy: 39.43%\n",
            "Epoch [2/10], Training Loss: 1.649, Validation Accuracy: 39.79%\n",
            "Epoch [3/10], Training Loss: 1.641, Validation Accuracy: 40.68%\n",
            "Epoch [4/10], Training Loss: 1.635, Validation Accuracy: 39.51%\n",
            "Epoch [5/10], Training Loss: 1.625, Validation Accuracy: 41.20%\n",
            "Epoch [6/10], Training Loss: 1.612, Validation Accuracy: 40.60%\n",
            "Epoch [7/10], Training Loss: 1.603, Validation Accuracy: 41.84%\n",
            "Epoch [8/10], Training Loss: 1.590, Validation Accuracy: 41.92%\n",
            "Epoch [9/10], Training Loss: 1.583, Validation Accuracy: 42.02%\n",
            "Epoch [10/10], Training Loss: 1.574, Validation Accuracy: 42.30%\n",
            "Epoch [1/10], Training Loss: 1.567, Validation Accuracy: 41.81%\n",
            "Epoch [2/10], Training Loss: 1.552, Validation Accuracy: 42.40%\n",
            "Epoch [3/10], Training Loss: 1.535, Validation Accuracy: 42.42%\n",
            "Epoch [4/10], Training Loss: 1.535, Validation Accuracy: 42.40%\n",
            "Epoch [5/10], Training Loss: 1.527, Validation Accuracy: 43.09%\n",
            "Epoch [6/10], Training Loss: 1.513, Validation Accuracy: 43.31%\n",
            "Epoch [7/10], Training Loss: 1.505, Validation Accuracy: 43.75%\n",
            "Epoch [8/10], Training Loss: 1.493, Validation Accuracy: 44.65%\n",
            "Epoch [9/10], Training Loss: 1.485, Validation Accuracy: 44.77%\n",
            "Epoch [10/10], Training Loss: 1.478, Validation Accuracy: 44.21%\n",
            "Epoch [1/10], Training Loss: 1.499, Validation Accuracy: 44.29%\n",
            "Epoch [2/10], Training Loss: 1.489, Validation Accuracy: 45.13%\n",
            "Epoch [3/10], Training Loss: 1.475, Validation Accuracy: 45.54%\n",
            "Epoch [4/10], Training Loss: 1.462, Validation Accuracy: 45.29%\n",
            "Epoch [5/10], Training Loss: 1.452, Validation Accuracy: 46.37%\n",
            "Epoch [6/10], Training Loss: 1.439, Validation Accuracy: 46.57%\n",
            "Epoch [7/10], Training Loss: 1.435, Validation Accuracy: 46.62%\n",
            "Epoch [8/10], Training Loss: 1.423, Validation Accuracy: 46.57%\n",
            "Epoch [9/10], Training Loss: 1.417, Validation Accuracy: 46.85%\n",
            "Epoch [10/10], Training Loss: 1.406, Validation Accuracy: 47.09%\n",
            "Epoch [1/10], Training Loss: 1.452, Validation Accuracy: 47.17%\n",
            "Epoch [2/10], Training Loss: 1.433, Validation Accuracy: 47.63%\n",
            "Epoch [3/10], Training Loss: 1.428, Validation Accuracy: 47.41%\n",
            "Epoch [4/10], Training Loss: 1.420, Validation Accuracy: 47.54%\n",
            "Epoch [5/10], Training Loss: 1.407, Validation Accuracy: 48.34%\n",
            "Epoch [6/10], Training Loss: 1.399, Validation Accuracy: 48.24%\n",
            "Epoch [7/10], Training Loss: 1.393, Validation Accuracy: 46.97%\n",
            "Epoch [8/10], Training Loss: 1.387, Validation Accuracy: 47.71%\n",
            "Epoch [9/10], Training Loss: 1.384, Validation Accuracy: 48.72%\n",
            "Epoch [10/10], Training Loss: 1.378, Validation Accuracy: 48.84%\n",
            "Epoch [1/10], Training Loss: 1.420, Validation Accuracy: 48.71%\n",
            "Epoch [2/10], Training Loss: 1.412, Validation Accuracy: 48.66%\n",
            "Epoch [3/10], Training Loss: 1.396, Validation Accuracy: 49.16%\n",
            "Epoch [4/10], Training Loss: 1.388, Validation Accuracy: 49.57%\n",
            "Epoch [5/10], Training Loss: 1.383, Validation Accuracy: 49.48%\n",
            "Epoch [6/10], Training Loss: 1.375, Validation Accuracy: 49.36%\n",
            "Epoch [7/10], Training Loss: 1.367, Validation Accuracy: 49.14%\n",
            "Epoch [8/10], Training Loss: 1.369, Validation Accuracy: 49.19%\n",
            "Epoch [9/10], Training Loss: 1.351, Validation Accuracy: 49.68%\n",
            "Epoch [10/10], Training Loss: 1.342, Validation Accuracy: 49.91%\n",
            "Epoch [1/10], Training Loss: 1.398, Validation Accuracy: 50.42%\n",
            "Epoch [2/10], Training Loss: 1.384, Validation Accuracy: 50.28%\n",
            "Epoch [3/10], Training Loss: 1.374, Validation Accuracy: 50.35%\n",
            "Epoch [4/10], Training Loss: 1.365, Validation Accuracy: 50.67%\n",
            "Epoch [5/10], Training Loss: 1.352, Validation Accuracy: 50.36%\n",
            "Epoch [6/10], Training Loss: 1.342, Validation Accuracy: 50.56%\n",
            "Epoch [7/10], Training Loss: 1.337, Validation Accuracy: 50.66%\n",
            "Epoch [8/10], Training Loss: 1.327, Validation Accuracy: 51.08%\n",
            "Epoch [9/10], Training Loss: 1.321, Validation Accuracy: 51.20%\n",
            "Epoch [10/10], Training Loss: 1.314, Validation Accuracy: 51.75%\n",
            "Epoch [1/10], Training Loss: 1.343, Validation Accuracy: 51.62%\n",
            "Epoch [2/10], Training Loss: 1.327, Validation Accuracy: 50.25%\n",
            "Epoch [3/10], Training Loss: 1.318, Validation Accuracy: 51.70%\n",
            "Epoch [4/10], Training Loss: 1.309, Validation Accuracy: 51.34%\n",
            "Epoch [5/10], Training Loss: 1.298, Validation Accuracy: 51.47%\n",
            "Epoch [6/10], Training Loss: 1.292, Validation Accuracy: 51.39%\n",
            "Epoch [7/10], Training Loss: 1.283, Validation Accuracy: 51.81%\n",
            "Epoch [8/10], Training Loss: 1.271, Validation Accuracy: 51.43%\n",
            "Epoch [9/10], Training Loss: 1.261, Validation Accuracy: 51.93%\n",
            "Epoch [10/10], Training Loss: 1.260, Validation Accuracy: 52.23%\n",
            "Epoch [1/10], Training Loss: 1.316, Validation Accuracy: 51.80%\n",
            "Epoch [2/10], Training Loss: 1.291, Validation Accuracy: 52.28%\n",
            "Epoch [3/10], Training Loss: 1.275, Validation Accuracy: 52.57%\n",
            "Epoch [4/10], Training Loss: 1.276, Validation Accuracy: 52.66%\n",
            "Epoch [5/10], Training Loss: 1.259, Validation Accuracy: 52.65%\n",
            "Epoch [6/10], Training Loss: 1.254, Validation Accuracy: 52.07%\n",
            "Epoch [7/10], Training Loss: 1.242, Validation Accuracy: 53.39%\n",
            "Epoch [8/10], Training Loss: 1.234, Validation Accuracy: 52.99%\n",
            "Epoch [9/10], Training Loss: 1.225, Validation Accuracy: 52.52%\n",
            "Epoch [10/10], Training Loss: 1.222, Validation Accuracy: 52.51%\n",
            "Epoch [1/10], Training Loss: 1.304, Validation Accuracy: 53.45%\n",
            "Epoch [2/10], Training Loss: 1.275, Validation Accuracy: 52.97%\n",
            "Epoch [3/10], Training Loss: 1.257, Validation Accuracy: 53.38%\n",
            "Epoch [4/10], Training Loss: 1.248, Validation Accuracy: 53.34%\n",
            "Epoch [5/10], Training Loss: 1.245, Validation Accuracy: 53.81%\n",
            "Epoch [6/10], Training Loss: 1.239, Validation Accuracy: 53.01%\n",
            "Epoch [7/10], Training Loss: 1.224, Validation Accuracy: 53.54%\n",
            "Epoch [8/10], Training Loss: 1.216, Validation Accuracy: 54.20%\n",
            "Epoch [9/10], Training Loss: 1.201, Validation Accuracy: 54.26%\n",
            "Epoch [10/10], Training Loss: 1.194, Validation Accuracy: 54.27%\n",
            "Epoch [1/10], Training Loss: 1.285, Validation Accuracy: 54.02%\n",
            "Epoch [2/10], Training Loss: 1.257, Validation Accuracy: 54.19%\n",
            "Epoch [3/10], Training Loss: 1.239, Validation Accuracy: 54.25%\n",
            "Epoch [4/10], Training Loss: 1.238, Validation Accuracy: 53.72%\n",
            "Epoch [5/10], Training Loss: 1.222, Validation Accuracy: 54.60%\n",
            "Epoch [6/10], Training Loss: 1.217, Validation Accuracy: 54.48%\n",
            "Epoch [7/10], Training Loss: 1.204, Validation Accuracy: 53.80%\n",
            "Epoch [8/10], Training Loss: 1.203, Validation Accuracy: 54.80%\n",
            "Epoch [9/10], Training Loss: 1.188, Validation Accuracy: 54.70%\n",
            "Epoch [10/10], Training Loss: 1.181, Validation Accuracy: 54.89%\n",
            "Epoch [1/10], Training Loss: 1.260, Validation Accuracy: 55.05%\n",
            "Epoch [2/10], Training Loss: 1.246, Validation Accuracy: 55.07%\n",
            "Epoch [3/10], Training Loss: 1.225, Validation Accuracy: 54.70%\n",
            "Epoch [4/10], Training Loss: 1.213, Validation Accuracy: 55.25%\n",
            "Epoch [5/10], Training Loss: 1.202, Validation Accuracy: 55.97%\n",
            "Epoch [6/10], Training Loss: 1.196, Validation Accuracy: 55.46%\n",
            "Epoch [7/10], Training Loss: 1.186, Validation Accuracy: 55.93%\n",
            "Epoch [8/10], Training Loss: 1.174, Validation Accuracy: 56.15%\n",
            "Epoch [9/10], Training Loss: 1.170, Validation Accuracy: 55.50%\n",
            "Epoch [10/10], Training Loss: 1.158, Validation Accuracy: 55.51%\n",
            "Epoch [1/10], Training Loss: 1.226, Validation Accuracy: 55.54%\n",
            "Epoch [2/10], Training Loss: 1.198, Validation Accuracy: 55.50%\n",
            "Epoch [3/10], Training Loss: 1.177, Validation Accuracy: 56.08%\n",
            "Epoch [4/10], Training Loss: 1.167, Validation Accuracy: 55.35%\n",
            "Epoch [5/10], Training Loss: 1.160, Validation Accuracy: 55.02%\n",
            "Epoch [6/10], Training Loss: 1.147, Validation Accuracy: 55.34%\n",
            "Epoch [7/10], Training Loss: 1.139, Validation Accuracy: 56.51%\n",
            "Epoch [8/10], Training Loss: 1.133, Validation Accuracy: 55.96%\n",
            "Epoch [9/10], Training Loss: 1.125, Validation Accuracy: 55.70%\n",
            "Epoch [10/10], Training Loss: 1.129, Validation Accuracy: 55.72%\n",
            "Epoch [1/10], Training Loss: 1.193, Validation Accuracy: 56.46%\n",
            "Epoch [2/10], Training Loss: 1.164, Validation Accuracy: 56.50%\n",
            "Epoch [3/10], Training Loss: 1.157, Validation Accuracy: 56.00%\n",
            "Epoch [4/10], Training Loss: 1.132, Validation Accuracy: 56.37%\n",
            "Epoch [5/10], Training Loss: 1.120, Validation Accuracy: 56.88%\n",
            "Epoch [6/10], Training Loss: 1.112, Validation Accuracy: 56.36%\n",
            "Epoch [7/10], Training Loss: 1.093, Validation Accuracy: 56.92%\n",
            "Epoch [8/10], Training Loss: 1.088, Validation Accuracy: 56.02%\n",
            "Epoch [9/10], Training Loss: 1.078, Validation Accuracy: 56.27%\n",
            "Epoch [10/10], Training Loss: 1.072, Validation Accuracy: 55.44%\n",
            "Epoch [1/10], Training Loss: 1.185, Validation Accuracy: 56.41%\n",
            "Epoch [2/10], Training Loss: 1.154, Validation Accuracy: 57.26%\n",
            "Epoch [3/10], Training Loss: 1.135, Validation Accuracy: 57.37%\n",
            "Epoch [4/10], Training Loss: 1.121, Validation Accuracy: 57.41%\n",
            "Epoch [5/10], Training Loss: 1.106, Validation Accuracy: 57.04%\n",
            "Epoch [6/10], Training Loss: 1.107, Validation Accuracy: 57.18%\n",
            "Epoch [7/10], Training Loss: 1.097, Validation Accuracy: 57.20%\n",
            "Epoch [8/10], Training Loss: 1.094, Validation Accuracy: 57.23%\n",
            "Epoch [9/10], Training Loss: 1.074, Validation Accuracy: 57.48%\n",
            "Epoch [10/10], Training Loss: 1.068, Validation Accuracy: 57.21%\n",
            "Epoch [1/10], Training Loss: 1.172, Validation Accuracy: 57.41%\n",
            "Epoch [2/10], Training Loss: 1.144, Validation Accuracy: 57.64%\n",
            "Epoch [3/10], Training Loss: 1.131, Validation Accuracy: 57.47%\n",
            "Epoch [4/10], Training Loss: 1.104, Validation Accuracy: 57.40%\n",
            "Epoch [5/10], Training Loss: 1.102, Validation Accuracy: 58.35%\n",
            "Epoch [6/10], Training Loss: 1.087, Validation Accuracy: 57.27%\n",
            "Epoch [7/10], Training Loss: 1.074, Validation Accuracy: 57.50%\n",
            "Epoch [8/10], Training Loss: 1.070, Validation Accuracy: 57.99%\n",
            "Epoch [9/10], Training Loss: 1.062, Validation Accuracy: 57.33%\n",
            "Epoch [10/10], Training Loss: 1.050, Validation Accuracy: 57.27%\n",
            "Epoch [1/10], Training Loss: 1.159, Validation Accuracy: 57.77%\n",
            "Epoch [2/10], Training Loss: 1.135, Validation Accuracy: 58.01%\n",
            "Epoch [3/10], Training Loss: 1.116, Validation Accuracy: 57.13%\n",
            "Epoch [4/10], Training Loss: 1.097, Validation Accuracy: 57.78%\n",
            "Epoch [5/10], Training Loss: 1.087, Validation Accuracy: 58.51%\n",
            "Epoch [6/10], Training Loss: 1.080, Validation Accuracy: 57.41%\n",
            "Epoch [7/10], Training Loss: 1.064, Validation Accuracy: 58.39%\n",
            "Epoch [8/10], Training Loss: 1.054, Validation Accuracy: 58.10%\n",
            "Epoch [9/10], Training Loss: 1.038, Validation Accuracy: 58.16%\n",
            "Epoch [10/10], Training Loss: 1.029, Validation Accuracy: 57.82%\n",
            "Epoch [1/10], Training Loss: 1.134, Validation Accuracy: 58.19%\n",
            "Epoch [2/10], Training Loss: 1.096, Validation Accuracy: 58.98%\n",
            "Epoch [3/10], Training Loss: 1.076, Validation Accuracy: 58.56%\n",
            "Epoch [4/10], Training Loss: 1.065, Validation Accuracy: 58.46%\n",
            "Epoch [5/10], Training Loss: 1.047, Validation Accuracy: 58.91%\n",
            "Epoch [6/10], Training Loss: 1.034, Validation Accuracy: 58.64%\n",
            "Epoch [7/10], Training Loss: 1.022, Validation Accuracy: 58.51%\n",
            "Epoch [8/10], Training Loss: 1.020, Validation Accuracy: 58.25%\n",
            "Epoch [9/10], Training Loss: 1.001, Validation Accuracy: 58.29%\n",
            "Epoch [10/10], Training Loss: 0.993, Validation Accuracy: 58.65%\n",
            "Epoch [1/10], Training Loss: 1.101, Validation Accuracy: 58.50%\n",
            "Epoch [2/10], Training Loss: 1.057, Validation Accuracy: 59.02%\n",
            "Epoch [3/10], Training Loss: 1.040, Validation Accuracy: 59.29%\n",
            "Epoch [4/10], Training Loss: 1.027, Validation Accuracy: 58.51%\n",
            "Epoch [5/10], Training Loss: 1.015, Validation Accuracy: 59.17%\n",
            "Epoch [6/10], Training Loss: 0.998, Validation Accuracy: 58.52%\n",
            "Epoch [7/10], Training Loss: 0.990, Validation Accuracy: 57.76%\n",
            "Epoch [8/10], Training Loss: 0.986, Validation Accuracy: 58.63%\n",
            "Epoch [9/10], Training Loss: 0.964, Validation Accuracy: 59.29%\n",
            "Epoch [10/10], Training Loss: 0.944, Validation Accuracy: 58.62%\n",
            "Epoch [1/10], Training Loss: 1.100, Validation Accuracy: 58.97%\n",
            "Epoch [2/10], Training Loss: 1.072, Validation Accuracy: 58.80%\n",
            "Epoch [3/10], Training Loss: 1.052, Validation Accuracy: 59.24%\n",
            "Epoch [4/10], Training Loss: 1.019, Validation Accuracy: 59.49%\n",
            "Epoch [5/10], Training Loss: 1.011, Validation Accuracy: 58.87%\n",
            "Epoch [6/10], Training Loss: 0.994, Validation Accuracy: 59.12%\n",
            "Epoch [7/10], Training Loss: 0.980, Validation Accuracy: 58.89%\n",
            "Epoch [8/10], Training Loss: 0.982, Validation Accuracy: 59.16%\n",
            "Epoch [9/10], Training Loss: 0.964, Validation Accuracy: 59.39%\n",
            "Epoch [10/10], Training Loss: 0.946, Validation Accuracy: 58.92%\n",
            "Epoch [1/10], Training Loss: 1.100, Validation Accuracy: 59.66%\n",
            "Epoch [2/10], Training Loss: 1.060, Validation Accuracy: 59.83%\n",
            "Epoch [3/10], Training Loss: 1.039, Validation Accuracy: 59.21%\n",
            "Epoch [4/10], Training Loss: 1.011, Validation Accuracy: 59.84%\n",
            "Epoch [5/10], Training Loss: 1.004, Validation Accuracy: 59.33%\n",
            "Epoch [6/10], Training Loss: 0.993, Validation Accuracy: 59.61%\n",
            "Epoch [7/10], Training Loss: 0.973, Validation Accuracy: 58.96%\n",
            "Epoch [8/10], Training Loss: 0.960, Validation Accuracy: 59.82%\n",
            "Epoch [9/10], Training Loss: 0.947, Validation Accuracy: 59.80%\n",
            "Epoch [10/10], Training Loss: 0.940, Validation Accuracy: 59.58%\n",
            "Epoch [1/10], Training Loss: 1.088, Validation Accuracy: 60.04%\n",
            "Epoch [2/10], Training Loss: 1.047, Validation Accuracy: 59.69%\n",
            "Epoch [3/10], Training Loss: 1.023, Validation Accuracy: 60.31%\n",
            "Epoch [4/10], Training Loss: 1.005, Validation Accuracy: 59.67%\n",
            "Epoch [5/10], Training Loss: 0.995, Validation Accuracy: 60.07%\n",
            "Epoch [6/10], Training Loss: 0.971, Validation Accuracy: 59.74%\n",
            "Epoch [7/10], Training Loss: 0.957, Validation Accuracy: 60.50%\n",
            "Epoch [8/10], Training Loss: 0.946, Validation Accuracy: 60.21%\n",
            "Epoch [9/10], Training Loss: 0.934, Validation Accuracy: 60.03%\n",
            "Epoch [10/10], Training Loss: 0.917, Validation Accuracy: 60.13%\n",
            "Epoch [1/10], Training Loss: 1.056, Validation Accuracy: 60.09%\n",
            "Epoch [2/10], Training Loss: 1.009, Validation Accuracy: 59.58%\n",
            "Epoch [3/10], Training Loss: 0.996, Validation Accuracy: 60.31%\n",
            "Epoch [4/10], Training Loss: 0.973, Validation Accuracy: 59.91%\n",
            "Epoch [5/10], Training Loss: 0.953, Validation Accuracy: 60.12%\n",
            "Epoch [6/10], Training Loss: 0.936, Validation Accuracy: 59.62%\n",
            "Epoch [7/10], Training Loss: 0.930, Validation Accuracy: 60.27%\n",
            "Epoch [8/10], Training Loss: 0.913, Validation Accuracy: 59.72%\n",
            "Epoch [9/10], Training Loss: 0.904, Validation Accuracy: 59.44%\n",
            "Epoch [10/10], Training Loss: 0.893, Validation Accuracy: 60.14%\n",
            "Epoch [1/10], Training Loss: 1.033, Validation Accuracy: 60.24%\n",
            "Epoch [2/10], Training Loss: 0.985, Validation Accuracy: 59.38%\n",
            "Epoch [3/10], Training Loss: 0.962, Validation Accuracy: 60.00%\n",
            "Epoch [4/10], Training Loss: 0.936, Validation Accuracy: 59.91%\n",
            "Epoch [5/10], Training Loss: 0.921, Validation Accuracy: 59.82%\n",
            "Epoch [6/10], Training Loss: 0.911, Validation Accuracy: 60.35%\n",
            "Epoch [7/10], Training Loss: 0.888, Validation Accuracy: 60.07%\n",
            "Epoch [8/10], Training Loss: 0.870, Validation Accuracy: 59.38%\n",
            "Epoch [9/10], Training Loss: 0.867, Validation Accuracy: 60.08%\n",
            "Epoch [10/10], Training Loss: 0.847, Validation Accuracy: 59.31%\n",
            "Epoch [1/10], Training Loss: 1.026, Validation Accuracy: 60.63%\n",
            "Epoch [2/10], Training Loss: 0.991, Validation Accuracy: 59.94%\n",
            "Epoch [3/10], Training Loss: 0.966, Validation Accuracy: 59.65%\n",
            "Epoch [4/10], Training Loss: 0.937, Validation Accuracy: 60.91%\n",
            "Epoch [5/10], Training Loss: 0.918, Validation Accuracy: 60.53%\n",
            "Epoch [6/10], Training Loss: 0.902, Validation Accuracy: 59.89%\n",
            "Epoch [7/10], Training Loss: 0.895, Validation Accuracy: 60.24%\n",
            "Epoch [8/10], Training Loss: 0.887, Validation Accuracy: 60.45%\n",
            "Epoch [9/10], Training Loss: 0.863, Validation Accuracy: 60.68%\n",
            "Epoch [10/10], Training Loss: 0.851, Validation Accuracy: 60.48%\n",
            "Epoch [1/10], Training Loss: 1.030, Validation Accuracy: 60.15%\n",
            "Epoch [2/10], Training Loss: 0.987, Validation Accuracy: 60.07%\n",
            "Epoch [3/10], Training Loss: 0.960, Validation Accuracy: 60.95%\n",
            "Epoch [4/10], Training Loss: 0.943, Validation Accuracy: 60.71%\n",
            "Epoch [5/10], Training Loss: 0.919, Validation Accuracy: 61.24%\n",
            "Epoch [6/10], Training Loss: 0.902, Validation Accuracy: 61.25%\n",
            "Epoch [7/10], Training Loss: 0.879, Validation Accuracy: 61.01%\n",
            "Epoch [8/10], Training Loss: 0.870, Validation Accuracy: 60.74%\n",
            "Epoch [9/10], Training Loss: 0.859, Validation Accuracy: 60.51%\n",
            "Epoch [10/10], Training Loss: 0.845, Validation Accuracy: 60.74%\n",
            "Epoch [1/10], Training Loss: 1.030, Validation Accuracy: 60.57%\n",
            "Epoch [2/10], Training Loss: 0.983, Validation Accuracy: 61.10%\n",
            "Epoch [3/10], Training Loss: 0.942, Validation Accuracy: 61.50%\n",
            "Epoch [4/10], Training Loss: 0.919, Validation Accuracy: 61.31%\n",
            "Epoch [5/10], Training Loss: 0.901, Validation Accuracy: 61.00%\n",
            "Epoch [6/10], Training Loss: 0.883, Validation Accuracy: 60.20%\n",
            "Epoch [7/10], Training Loss: 0.873, Validation Accuracy: 60.78%\n",
            "Epoch [8/10], Training Loss: 0.847, Validation Accuracy: 60.61%\n",
            "Epoch [9/10], Training Loss: 0.850, Validation Accuracy: 61.04%\n",
            "Epoch [10/10], Training Loss: 0.819, Validation Accuracy: 61.09%\n",
            "Epoch [1/10], Training Loss: 0.999, Validation Accuracy: 60.97%\n",
            "Epoch [2/10], Training Loss: 0.947, Validation Accuracy: 59.66%\n",
            "Epoch [3/10], Training Loss: 0.913, Validation Accuracy: 60.90%\n",
            "Epoch [4/10], Training Loss: 0.882, Validation Accuracy: 60.78%\n",
            "Epoch [5/10], Training Loss: 0.872, Validation Accuracy: 59.60%\n",
            "Epoch [6/10], Training Loss: 0.860, Validation Accuracy: 61.35%\n",
            "Epoch [7/10], Training Loss: 0.833, Validation Accuracy: 61.03%\n",
            "Epoch [8/10], Training Loss: 0.825, Validation Accuracy: 60.63%\n",
            "Epoch [9/10], Training Loss: 0.815, Validation Accuracy: 60.39%\n",
            "Epoch [10/10], Training Loss: 0.791, Validation Accuracy: 61.02%\n",
            "Epoch [1/10], Training Loss: 0.973, Validation Accuracy: 60.55%\n",
            "Epoch [2/10], Training Loss: 0.918, Validation Accuracy: 61.16%\n",
            "Epoch [3/10], Training Loss: 0.878, Validation Accuracy: 60.97%\n",
            "Epoch [4/10], Training Loss: 0.866, Validation Accuracy: 60.51%\n",
            "Epoch [5/10], Training Loss: 0.840, Validation Accuracy: 60.68%\n",
            "Epoch [6/10], Training Loss: 0.818, Validation Accuracy: 60.78%\n",
            "Epoch [7/10], Training Loss: 0.805, Validation Accuracy: 60.71%\n",
            "Epoch [8/10], Training Loss: 0.778, Validation Accuracy: 60.38%\n",
            "Epoch [9/10], Training Loss: 0.769, Validation Accuracy: 60.76%\n",
            "Epoch [10/10], Training Loss: 0.751, Validation Accuracy: 60.47%\n",
            "Epoch [1/10], Training Loss: 0.981, Validation Accuracy: 59.76%\n",
            "Epoch [2/10], Training Loss: 0.923, Validation Accuracy: 60.53%\n",
            "Epoch [3/10], Training Loss: 0.882, Validation Accuracy: 60.71%\n",
            "Epoch [4/10], Training Loss: 0.855, Validation Accuracy: 60.84%\n",
            "Epoch [5/10], Training Loss: 0.835, Validation Accuracy: 60.96%\n",
            "Epoch [6/10], Training Loss: 0.815, Validation Accuracy: 60.62%\n",
            "Epoch [7/10], Training Loss: 0.794, Validation Accuracy: 60.73%\n",
            "Epoch [8/10], Training Loss: 0.788, Validation Accuracy: 61.12%\n",
            "Epoch [9/10], Training Loss: 0.769, Validation Accuracy: 60.76%\n",
            "Epoch [10/10], Training Loss: 0.753, Validation Accuracy: 60.20%\n",
            "Epoch [1/10], Training Loss: 0.991, Validation Accuracy: 61.30%\n",
            "Epoch [2/10], Training Loss: 0.924, Validation Accuracy: 61.33%\n",
            "Epoch [3/10], Training Loss: 0.879, Validation Accuracy: 60.70%\n",
            "Epoch [4/10], Training Loss: 0.857, Validation Accuracy: 61.41%\n",
            "Epoch [5/10], Training Loss: 0.832, Validation Accuracy: 61.22%\n",
            "Epoch [6/10], Training Loss: 0.819, Validation Accuracy: 60.41%\n",
            "Epoch [7/10], Training Loss: 0.800, Validation Accuracy: 61.48%\n",
            "Epoch [8/10], Training Loss: 0.790, Validation Accuracy: 61.06%\n",
            "Epoch [9/10], Training Loss: 0.767, Validation Accuracy: 61.32%\n",
            "Epoch [10/10], Training Loss: 0.747, Validation Accuracy: 61.45%\n",
            "Epoch [1/10], Training Loss: 0.965, Validation Accuracy: 61.73%\n",
            "Epoch [2/10], Training Loss: 0.908, Validation Accuracy: 61.95%\n",
            "Epoch [3/10], Training Loss: 0.864, Validation Accuracy: 61.51%\n",
            "Epoch [4/10], Training Loss: 0.841, Validation Accuracy: 62.16%\n",
            "Epoch [5/10], Training Loss: 0.813, Validation Accuracy: 61.72%\n",
            "Epoch [6/10], Training Loss: 0.793, Validation Accuracy: 61.49%\n",
            "Epoch [7/10], Training Loss: 0.774, Validation Accuracy: 61.30%\n",
            "Epoch [8/10], Training Loss: 0.760, Validation Accuracy: 61.22%\n",
            "Epoch [9/10], Training Loss: 0.753, Validation Accuracy: 61.35%\n",
            "Epoch [10/10], Training Loss: 0.727, Validation Accuracy: 60.77%\n",
            "Epoch [1/10], Training Loss: 0.961, Validation Accuracy: 61.25%\n",
            "Epoch [2/10], Training Loss: 0.884, Validation Accuracy: 61.55%\n",
            "Epoch [3/10], Training Loss: 0.844, Validation Accuracy: 61.73%\n",
            "Epoch [4/10], Training Loss: 0.817, Validation Accuracy: 61.65%\n",
            "Epoch [5/10], Training Loss: 0.791, Validation Accuracy: 61.43%\n",
            "Epoch [6/10], Training Loss: 0.772, Validation Accuracy: 62.27%\n",
            "Epoch [7/10], Training Loss: 0.755, Validation Accuracy: 61.83%\n",
            "Epoch [8/10], Training Loss: 0.732, Validation Accuracy: 61.18%\n",
            "Epoch [9/10], Training Loss: 0.723, Validation Accuracy: 61.12%\n",
            "Epoch [10/10], Training Loss: 0.702, Validation Accuracy: 61.14%\n",
            "Epoch [1/10], Training Loss: 0.920, Validation Accuracy: 61.81%\n",
            "Epoch [2/10], Training Loss: 0.847, Validation Accuracy: 61.39%\n",
            "Epoch [3/10], Training Loss: 0.805, Validation Accuracy: 61.48%\n",
            "Epoch [4/10], Training Loss: 0.778, Validation Accuracy: 61.49%\n",
            "Epoch [5/10], Training Loss: 0.763, Validation Accuracy: 61.36%\n",
            "Epoch [6/10], Training Loss: 0.731, Validation Accuracy: 60.77%\n",
            "Epoch [7/10], Training Loss: 0.713, Validation Accuracy: 61.27%\n",
            "Epoch [8/10], Training Loss: 0.691, Validation Accuracy: 61.57%\n",
            "Epoch [9/10], Training Loss: 0.670, Validation Accuracy: 61.63%\n",
            "Epoch [10/10], Training Loss: 0.655, Validation Accuracy: 60.48%\n",
            "Epoch [1/10], Training Loss: 0.919, Validation Accuracy: 61.39%\n",
            "Epoch [2/10], Training Loss: 0.848, Validation Accuracy: 60.84%\n",
            "Epoch [3/10], Training Loss: 0.814, Validation Accuracy: 61.51%\n",
            "Epoch [4/10], Training Loss: 0.777, Validation Accuracy: 61.45%\n",
            "Epoch [5/10], Training Loss: 0.753, Validation Accuracy: 60.87%\n",
            "Epoch [6/10], Training Loss: 0.738, Validation Accuracy: 61.02%\n",
            "Epoch [7/10], Training Loss: 0.707, Validation Accuracy: 60.72%\n",
            "Epoch [8/10], Training Loss: 0.691, Validation Accuracy: 61.16%\n",
            "Epoch [9/10], Training Loss: 0.678, Validation Accuracy: 61.40%\n",
            "Epoch [10/10], Training Loss: 0.659, Validation Accuracy: 61.04%\n",
            "Epoch [1/10], Training Loss: 0.939, Validation Accuracy: 61.46%\n",
            "Epoch [2/10], Training Loss: 0.868, Validation Accuracy: 61.85%\n",
            "Epoch [3/10], Training Loss: 0.822, Validation Accuracy: 61.67%\n",
            "Epoch [4/10], Training Loss: 0.788, Validation Accuracy: 61.62%\n",
            "Epoch [5/10], Training Loss: 0.761, Validation Accuracy: 61.60%\n",
            "Epoch [6/10], Training Loss: 0.738, Validation Accuracy: 61.05%\n",
            "Epoch [7/10], Training Loss: 0.718, Validation Accuracy: 61.64%\n",
            "Epoch [8/10], Training Loss: 0.694, Validation Accuracy: 61.07%\n",
            "Epoch [9/10], Training Loss: 0.686, Validation Accuracy: 61.71%\n",
            "Epoch [10/10], Training Loss: 0.662, Validation Accuracy: 60.71%\n",
            "Epoch [1/10], Training Loss: 0.933, Validation Accuracy: 60.73%\n",
            "Epoch [2/10], Training Loss: 0.852, Validation Accuracy: 61.52%\n",
            "Epoch [3/10], Training Loss: 0.806, Validation Accuracy: 62.23%\n",
            "Epoch [4/10], Training Loss: 0.772, Validation Accuracy: 61.88%\n",
            "Epoch [5/10], Training Loss: 0.735, Validation Accuracy: 61.41%\n",
            "Epoch [6/10], Training Loss: 0.711, Validation Accuracy: 62.08%\n",
            "Epoch [7/10], Training Loss: 0.696, Validation Accuracy: 61.85%\n",
            "Epoch [8/10], Training Loss: 0.669, Validation Accuracy: 62.11%\n",
            "Epoch [9/10], Training Loss: 0.653, Validation Accuracy: 61.70%\n",
            "Epoch [10/10], Training Loss: 0.633, Validation Accuracy: 61.42%\n",
            "Epoch [1/10], Training Loss: 0.907, Validation Accuracy: 61.94%\n",
            "Epoch [2/10], Training Loss: 0.825, Validation Accuracy: 61.69%\n",
            "Epoch [3/10], Training Loss: 0.777, Validation Accuracy: 61.13%\n",
            "Epoch [4/10], Training Loss: 0.746, Validation Accuracy: 62.49%\n",
            "Epoch [5/10], Training Loss: 0.718, Validation Accuracy: 61.30%\n",
            "Epoch [6/10], Training Loss: 0.708, Validation Accuracy: 61.39%\n",
            "Epoch [7/10], Training Loss: 0.684, Validation Accuracy: 61.55%\n",
            "Epoch [8/10], Training Loss: 0.658, Validation Accuracy: 61.55%\n",
            "Epoch [9/10], Training Loss: 0.640, Validation Accuracy: 61.47%\n",
            "Epoch [10/10], Training Loss: 0.627, Validation Accuracy: 60.20%\n",
            "Epoch [1/10], Training Loss: 0.882, Validation Accuracy: 61.06%\n",
            "Epoch [2/10], Training Loss: 0.788, Validation Accuracy: 62.08%\n",
            "Epoch [3/10], Training Loss: 0.734, Validation Accuracy: 61.27%\n",
            "Epoch [4/10], Training Loss: 0.706, Validation Accuracy: 61.82%\n",
            "Epoch [5/10], Training Loss: 0.680, Validation Accuracy: 61.75%\n",
            "Epoch [6/10], Training Loss: 0.648, Validation Accuracy: 61.27%\n",
            "Epoch [7/10], Training Loss: 0.634, Validation Accuracy: 60.95%\n",
            "Epoch [8/10], Training Loss: 0.605, Validation Accuracy: 61.21%\n",
            "Epoch [9/10], Training Loss: 0.584, Validation Accuracy: 61.07%\n",
            "Epoch [10/10], Training Loss: 0.566, Validation Accuracy: 61.07%\n",
            "Epoch [1/10], Training Loss: 0.886, Validation Accuracy: 61.06%\n",
            "Epoch [2/10], Training Loss: 0.791, Validation Accuracy: 60.45%\n",
            "Epoch [3/10], Training Loss: 0.743, Validation Accuracy: 61.35%\n",
            "Epoch [4/10], Training Loss: 0.702, Validation Accuracy: 61.73%\n",
            "Epoch [5/10], Training Loss: 0.664, Validation Accuracy: 61.79%\n",
            "Epoch [6/10], Training Loss: 0.647, Validation Accuracy: 61.39%\n",
            "Epoch [7/10], Training Loss: 0.626, Validation Accuracy: 61.43%\n",
            "Epoch [8/10], Training Loss: 0.602, Validation Accuracy: 60.90%\n",
            "Epoch [9/10], Training Loss: 0.580, Validation Accuracy: 61.53%\n",
            "Epoch [10/10], Training Loss: 0.560, Validation Accuracy: 61.21%\n",
            "Epoch [1/10], Training Loss: 0.898, Validation Accuracy: 60.55%\n",
            "Epoch [2/10], Training Loss: 0.801, Validation Accuracy: 61.82%\n",
            "Epoch [3/10], Training Loss: 0.749, Validation Accuracy: 61.99%\n",
            "Epoch [4/10], Training Loss: 0.710, Validation Accuracy: 62.52%\n",
            "Epoch [5/10], Training Loss: 0.678, Validation Accuracy: 61.06%\n",
            "Epoch [6/10], Training Loss: 0.648, Validation Accuracy: 61.15%\n",
            "Epoch [7/10], Training Loss: 0.643, Validation Accuracy: 61.65%\n",
            "Epoch [8/10], Training Loss: 0.614, Validation Accuracy: 61.52%\n",
            "Epoch [9/10], Training Loss: 0.591, Validation Accuracy: 61.15%\n",
            "Epoch [10/10], Training Loss: 0.582, Validation Accuracy: 61.44%\n",
            "Epoch [1/10], Training Loss: 0.881, Validation Accuracy: 60.73%\n",
            "Epoch [2/10], Training Loss: 0.785, Validation Accuracy: 62.11%\n",
            "Epoch [3/10], Training Loss: 0.720, Validation Accuracy: 62.07%\n",
            "Epoch [4/10], Training Loss: 0.697, Validation Accuracy: 61.95%\n",
            "Epoch [5/10], Training Loss: 0.660, Validation Accuracy: 62.07%\n",
            "Epoch [6/10], Training Loss: 0.632, Validation Accuracy: 61.06%\n",
            "Epoch [7/10], Training Loss: 0.612, Validation Accuracy: 61.36%\n",
            "Epoch [8/10], Training Loss: 0.584, Validation Accuracy: 61.72%\n",
            "Epoch [9/10], Training Loss: 0.564, Validation Accuracy: 61.83%\n",
            "Epoch [10/10], Training Loss: 0.555, Validation Accuracy: 62.00%\n",
            "Epoch [1/10], Training Loss: 0.855, Validation Accuracy: 61.09%\n",
            "Epoch [2/10], Training Loss: 0.765, Validation Accuracy: 61.09%\n",
            "Epoch [3/10], Training Loss: 0.721, Validation Accuracy: 62.20%\n",
            "Epoch [4/10], Training Loss: 0.668, Validation Accuracy: 61.36%\n",
            "Epoch [5/10], Training Loss: 0.644, Validation Accuracy: 61.31%\n",
            "Epoch [6/10], Training Loss: 0.625, Validation Accuracy: 61.82%\n",
            "Epoch [7/10], Training Loss: 0.589, Validation Accuracy: 61.47%\n",
            "Epoch [8/10], Training Loss: 0.573, Validation Accuracy: 61.30%\n",
            "Epoch [9/10], Training Loss: 0.544, Validation Accuracy: 61.05%\n",
            "Epoch [10/10], Training Loss: 0.541, Validation Accuracy: 61.03%\n",
            "Epoch [1/10], Training Loss: 0.849, Validation Accuracy: 61.23%\n",
            "Epoch [2/10], Training Loss: 0.732, Validation Accuracy: 61.42%\n",
            "Epoch [3/10], Training Loss: 0.671, Validation Accuracy: 61.76%\n",
            "Epoch [4/10], Training Loss: 0.632, Validation Accuracy: 61.55%\n",
            "Epoch [5/10], Training Loss: 0.597, Validation Accuracy: 61.37%\n",
            "Epoch [6/10], Training Loss: 0.565, Validation Accuracy: 61.18%\n",
            "Epoch [7/10], Training Loss: 0.540, Validation Accuracy: 61.24%\n",
            "Epoch [8/10], Training Loss: 0.521, Validation Accuracy: 60.95%\n",
            "Epoch [9/10], Training Loss: 0.500, Validation Accuracy: 61.07%\n",
            "Epoch [10/10], Training Loss: 0.486, Validation Accuracy: 60.19%\n",
            "Epoch [1/10], Training Loss: 0.857, Validation Accuracy: 60.58%\n",
            "Epoch [2/10], Training Loss: 0.730, Validation Accuracy: 61.07%\n",
            "Epoch [3/10], Training Loss: 0.666, Validation Accuracy: 61.79%\n",
            "Epoch [4/10], Training Loss: 0.618, Validation Accuracy: 61.77%\n",
            "Epoch [5/10], Training Loss: 0.585, Validation Accuracy: 61.20%\n",
            "Epoch [6/10], Training Loss: 0.569, Validation Accuracy: 61.01%\n",
            "Epoch [7/10], Training Loss: 0.537, Validation Accuracy: 60.53%\n",
            "Epoch [8/10], Training Loss: 0.517, Validation Accuracy: 61.02%\n",
            "Epoch [9/10], Training Loss: 0.495, Validation Accuracy: 60.96%\n",
            "Epoch [10/10], Training Loss: 0.467, Validation Accuracy: 60.86%\n",
            "Epoch [1/10], Training Loss: 0.862, Validation Accuracy: 60.98%\n",
            "Epoch [2/10], Training Loss: 0.741, Validation Accuracy: 61.11%\n",
            "Epoch [3/10], Training Loss: 0.689, Validation Accuracy: 61.53%\n",
            "Epoch [4/10], Training Loss: 0.638, Validation Accuracy: 61.47%\n",
            "Epoch [5/10], Training Loss: 0.621, Validation Accuracy: 61.56%\n",
            "Epoch [6/10], Training Loss: 0.581, Validation Accuracy: 61.24%\n",
            "Epoch [7/10], Training Loss: 0.548, Validation Accuracy: 61.01%\n",
            "Epoch [8/10], Training Loss: 0.534, Validation Accuracy: 60.89%\n",
            "Epoch [9/10], Training Loss: 0.510, Validation Accuracy: 61.35%\n",
            "Epoch [10/10], Training Loss: 0.494, Validation Accuracy: 60.93%\n",
            "Confusion Matrix:\n",
            "[[698  17  41  10  29  11  18  10 104  62]\n",
            " [ 49 615  15  11   8  15  13   4  65 205]\n",
            " [ 78   8 471  47 130  92  85  46  26  17]\n",
            " [ 21   8  82 331 105 213 107  53  35  45]\n",
            " [ 23   6  65  47 593  50  88  85  27  16]\n",
            " [ 18   8  68 126  72 545  54  72  17  20]\n",
            " [ 10   7  47  54  75  32 731  14  10  20]\n",
            " [ 20   9  34  36  95  89  18 657   4  38]\n",
            " [ 94  35  14   9  10   9  10   8 751  60]\n",
            " [ 58  57  16  19  21  12  18  32  56 711]]\n",
            "Test Accuracy: 61.03%\n",
            "True Positives (TP): [698 615 471 331 593 545 731 657 751 711]\n",
            "False Positives (FP): [371 155 382 359 545 523 411 324 344 483]\n",
            "True Negatives (TN): [8629 8845 8618 8641 8455 8477 8589 8676 8656 8517]\n",
            "False Negatives (FN): [302 385 529 669 407 455 269 343 249 289]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.65294668 0.7987013  0.55216882 0.47971014 0.52108963 0.51029963\n",
            " 0.64010508 0.66972477 0.68584475 0.59547739]\n",
            "Recall: [0.698 0.615 0.471 0.331 0.593 0.545 0.731 0.657 0.751 0.711]\n",
            "F1 Score: [0.67472209 0.69491525 0.50836481 0.39171598 0.55472404 0.5270793\n",
            " 0.68253968 0.66330136 0.71694511 0.64813127]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 0.1):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=0.1):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_normal: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = ( augmented_data_normal + augmented_data_truncated) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=0.1)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"normal\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iUz3Q723B3z"
      },
      "source": [
        "Beta =.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbKkmn683E5V",
        "outputId": "c4235139-2a59-4e49-ac2e-68442b8049f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Images per Class: [5936 6016 6061 5973 5968 5981 5979 6046 6013 6027]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<timed exec>:281: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Training Loss: 2.308, Validation Accuracy: 10.06%\n",
            "Epoch [2/10], Training Loss: 2.307, Validation Accuracy: 10.06%\n",
            "Epoch [3/10], Training Loss: 2.305, Validation Accuracy: 10.06%\n",
            "Epoch [4/10], Training Loss: 2.304, Validation Accuracy: 10.05%\n",
            "Epoch [5/10], Training Loss: 2.303, Validation Accuracy: 10.03%\n",
            "Epoch [6/10], Training Loss: 2.302, Validation Accuracy: 10.31%\n",
            "Epoch [7/10], Training Loss: 2.301, Validation Accuracy: 10.93%\n",
            "Epoch [8/10], Training Loss: 2.301, Validation Accuracy: 11.08%\n",
            "Epoch [9/10], Training Loss: 2.300, Validation Accuracy: 10.64%\n",
            "Epoch [10/10], Training Loss: 2.299, Validation Accuracy: 10.64%\n",
            "Epoch [1/10], Training Loss: 2.300, Validation Accuracy: 10.82%\n",
            "Epoch [2/10], Training Loss: 2.299, Validation Accuracy: 10.83%\n",
            "Epoch [3/10], Training Loss: 2.298, Validation Accuracy: 11.23%\n",
            "Epoch [4/10], Training Loss: 2.297, Validation Accuracy: 14.84%\n",
            "Epoch [5/10], Training Loss: 2.295, Validation Accuracy: 17.06%\n",
            "Epoch [6/10], Training Loss: 2.293, Validation Accuracy: 17.39%\n",
            "Epoch [7/10], Training Loss: 2.291, Validation Accuracy: 16.72%\n",
            "Epoch [8/10], Training Loss: 2.288, Validation Accuracy: 16.11%\n",
            "Epoch [9/10], Training Loss: 2.283, Validation Accuracy: 15.50%\n",
            "Epoch [10/10], Training Loss: 2.276, Validation Accuracy: 15.32%\n",
            "Epoch [1/10], Training Loss: 2.268, Validation Accuracy: 15.98%\n",
            "Epoch [2/10], Training Loss: 2.252, Validation Accuracy: 16.26%\n",
            "Epoch [3/10], Training Loss: 2.227, Validation Accuracy: 16.58%\n",
            "Epoch [4/10], Training Loss: 2.195, Validation Accuracy: 18.40%\n",
            "Epoch [5/10], Training Loss: 2.164, Validation Accuracy: 20.55%\n",
            "Epoch [6/10], Training Loss: 2.143, Validation Accuracy: 22.32%\n",
            "Epoch [7/10], Training Loss: 2.128, Validation Accuracy: 23.06%\n",
            "Epoch [8/10], Training Loss: 2.116, Validation Accuracy: 23.70%\n",
            "Epoch [9/10], Training Loss: 2.107, Validation Accuracy: 24.39%\n",
            "Epoch [10/10], Training Loss: 2.095, Validation Accuracy: 25.41%\n",
            "Epoch [1/10], Training Loss: 2.091, Validation Accuracy: 25.56%\n",
            "Epoch [2/10], Training Loss: 2.078, Validation Accuracy: 26.63%\n",
            "Epoch [3/10], Training Loss: 2.063, Validation Accuracy: 27.41%\n",
            "Epoch [4/10], Training Loss: 2.044, Validation Accuracy: 27.61%\n",
            "Epoch [5/10], Training Loss: 2.026, Validation Accuracy: 27.72%\n",
            "Epoch [6/10], Training Loss: 1.999, Validation Accuracy: 29.00%\n",
            "Epoch [7/10], Training Loss: 1.969, Validation Accuracy: 29.80%\n",
            "Epoch [8/10], Training Loss: 1.944, Validation Accuracy: 31.33%\n",
            "Epoch [9/10], Training Loss: 1.920, Validation Accuracy: 31.23%\n",
            "Epoch [10/10], Training Loss: 1.903, Validation Accuracy: 31.18%\n",
            "Epoch [1/10], Training Loss: 1.908, Validation Accuracy: 32.51%\n",
            "Epoch [2/10], Training Loss: 1.891, Validation Accuracy: 32.91%\n",
            "Epoch [3/10], Training Loss: 1.873, Validation Accuracy: 33.38%\n",
            "Epoch [4/10], Training Loss: 1.859, Validation Accuracy: 34.21%\n",
            "Epoch [5/10], Training Loss: 1.841, Validation Accuracy: 34.48%\n",
            "Epoch [6/10], Training Loss: 1.833, Validation Accuracy: 34.66%\n",
            "Epoch [7/10], Training Loss: 1.812, Validation Accuracy: 35.43%\n",
            "Epoch [8/10], Training Loss: 1.792, Validation Accuracy: 35.42%\n",
            "Epoch [9/10], Training Loss: 1.781, Validation Accuracy: 35.63%\n",
            "Epoch [10/10], Training Loss: 1.762, Validation Accuracy: 36.92%\n",
            "Epoch [1/10], Training Loss: 1.779, Validation Accuracy: 36.69%\n",
            "Epoch [2/10], Training Loss: 1.759, Validation Accuracy: 37.04%\n",
            "Epoch [3/10], Training Loss: 1.738, Validation Accuracy: 37.58%\n",
            "Epoch [4/10], Training Loss: 1.724, Validation Accuracy: 37.43%\n",
            "Epoch [5/10], Training Loss: 1.717, Validation Accuracy: 36.52%\n",
            "Epoch [6/10], Training Loss: 1.712, Validation Accuracy: 37.93%\n",
            "Epoch [7/10], Training Loss: 1.691, Validation Accuracy: 38.90%\n",
            "Epoch [8/10], Training Loss: 1.682, Validation Accuracy: 38.86%\n",
            "Epoch [9/10], Training Loss: 1.673, Validation Accuracy: 39.19%\n",
            "Epoch [10/10], Training Loss: 1.667, Validation Accuracy: 39.81%\n",
            "Epoch [1/10], Training Loss: 1.653, Validation Accuracy: 38.27%\n",
            "Epoch [2/10], Training Loss: 1.644, Validation Accuracy: 39.52%\n",
            "Epoch [3/10], Training Loss: 1.619, Validation Accuracy: 40.24%\n",
            "Epoch [4/10], Training Loss: 1.612, Validation Accuracy: 40.72%\n",
            "Epoch [5/10], Training Loss: 1.607, Validation Accuracy: 39.83%\n",
            "Epoch [6/10], Training Loss: 1.590, Validation Accuracy: 41.13%\n",
            "Epoch [7/10], Training Loss: 1.577, Validation Accuracy: 41.87%\n",
            "Epoch [8/10], Training Loss: 1.562, Validation Accuracy: 41.75%\n",
            "Epoch [9/10], Training Loss: 1.558, Validation Accuracy: 41.10%\n",
            "Epoch [10/10], Training Loss: 1.546, Validation Accuracy: 42.65%\n",
            "Epoch [1/10], Training Loss: 1.575, Validation Accuracy: 42.87%\n",
            "Epoch [2/10], Training Loss: 1.560, Validation Accuracy: 41.60%\n",
            "Epoch [3/10], Training Loss: 1.551, Validation Accuracy: 43.60%\n",
            "Epoch [4/10], Training Loss: 1.538, Validation Accuracy: 43.70%\n",
            "Epoch [5/10], Training Loss: 1.527, Validation Accuracy: 44.37%\n",
            "Epoch [6/10], Training Loss: 1.520, Validation Accuracy: 44.37%\n",
            "Epoch [7/10], Training Loss: 1.512, Validation Accuracy: 44.10%\n",
            "Epoch [8/10], Training Loss: 1.506, Validation Accuracy: 43.48%\n",
            "Epoch [9/10], Training Loss: 1.494, Validation Accuracy: 45.11%\n",
            "Epoch [10/10], Training Loss: 1.482, Validation Accuracy: 45.56%\n",
            "Epoch [1/10], Training Loss: 1.500, Validation Accuracy: 44.66%\n",
            "Epoch [2/10], Training Loss: 1.480, Validation Accuracy: 45.51%\n",
            "Epoch [3/10], Training Loss: 1.478, Validation Accuracy: 45.13%\n",
            "Epoch [4/10], Training Loss: 1.455, Validation Accuracy: 46.26%\n",
            "Epoch [5/10], Training Loss: 1.454, Validation Accuracy: 46.15%\n",
            "Epoch [6/10], Training Loss: 1.435, Validation Accuracy: 46.50%\n",
            "Epoch [7/10], Training Loss: 1.431, Validation Accuracy: 47.00%\n",
            "Epoch [8/10], Training Loss: 1.422, Validation Accuracy: 46.77%\n",
            "Epoch [9/10], Training Loss: 1.412, Validation Accuracy: 47.42%\n",
            "Epoch [10/10], Training Loss: 1.401, Validation Accuracy: 47.76%\n",
            "Epoch [1/10], Training Loss: 1.455, Validation Accuracy: 47.52%\n",
            "Epoch [2/10], Training Loss: 1.433, Validation Accuracy: 47.52%\n",
            "Epoch [3/10], Training Loss: 1.427, Validation Accuracy: 46.02%\n",
            "Epoch [4/10], Training Loss: 1.421, Validation Accuracy: 47.38%\n",
            "Epoch [5/10], Training Loss: 1.405, Validation Accuracy: 48.07%\n",
            "Epoch [6/10], Training Loss: 1.396, Validation Accuracy: 47.45%\n",
            "Epoch [7/10], Training Loss: 1.391, Validation Accuracy: 47.97%\n",
            "Epoch [8/10], Training Loss: 1.383, Validation Accuracy: 48.20%\n",
            "Epoch [9/10], Training Loss: 1.374, Validation Accuracy: 47.63%\n",
            "Epoch [10/10], Training Loss: 1.368, Validation Accuracy: 48.27%\n",
            "Epoch [1/10], Training Loss: 1.428, Validation Accuracy: 48.66%\n",
            "Epoch [2/10], Training Loss: 1.404, Validation Accuracy: 48.57%\n",
            "Epoch [3/10], Training Loss: 1.388, Validation Accuracy: 49.22%\n",
            "Epoch [4/10], Training Loss: 1.386, Validation Accuracy: 49.98%\n",
            "Epoch [5/10], Training Loss: 1.377, Validation Accuracy: 49.83%\n",
            "Epoch [6/10], Training Loss: 1.359, Validation Accuracy: 49.74%\n",
            "Epoch [7/10], Training Loss: 1.350, Validation Accuracy: 49.92%\n",
            "Epoch [8/10], Training Loss: 1.351, Validation Accuracy: 50.03%\n",
            "Epoch [9/10], Training Loss: 1.340, Validation Accuracy: 49.16%\n",
            "Epoch [10/10], Training Loss: 1.329, Validation Accuracy: 49.99%\n",
            "Epoch [1/10], Training Loss: 1.361, Validation Accuracy: 50.72%\n",
            "Epoch [2/10], Training Loss: 1.342, Validation Accuracy: 50.68%\n",
            "Epoch [3/10], Training Loss: 1.326, Validation Accuracy: 50.34%\n",
            "Epoch [4/10], Training Loss: 1.322, Validation Accuracy: 50.97%\n",
            "Epoch [5/10], Training Loss: 1.309, Validation Accuracy: 50.73%\n",
            "Epoch [6/10], Training Loss: 1.301, Validation Accuracy: 51.09%\n",
            "Epoch [7/10], Training Loss: 1.296, Validation Accuracy: 51.37%\n",
            "Epoch [8/10], Training Loss: 1.277, Validation Accuracy: 51.14%\n",
            "Epoch [9/10], Training Loss: 1.272, Validation Accuracy: 51.63%\n",
            "Epoch [10/10], Training Loss: 1.263, Validation Accuracy: 51.12%\n",
            "Epoch [1/10], Training Loss: 1.337, Validation Accuracy: 51.65%\n",
            "Epoch [2/10], Training Loss: 1.315, Validation Accuracy: 52.34%\n",
            "Epoch [3/10], Training Loss: 1.302, Validation Accuracy: 52.66%\n",
            "Epoch [4/10], Training Loss: 1.286, Validation Accuracy: 52.24%\n",
            "Epoch [5/10], Training Loss: 1.280, Validation Accuracy: 51.89%\n",
            "Epoch [6/10], Training Loss: 1.271, Validation Accuracy: 52.82%\n",
            "Epoch [7/10], Training Loss: 1.257, Validation Accuracy: 52.88%\n",
            "Epoch [8/10], Training Loss: 1.258, Validation Accuracy: 52.26%\n",
            "Epoch [9/10], Training Loss: 1.245, Validation Accuracy: 52.28%\n",
            "Epoch [10/10], Training Loss: 1.242, Validation Accuracy: 53.53%\n",
            "Epoch [1/10], Training Loss: 1.284, Validation Accuracy: 53.21%\n",
            "Epoch [2/10], Training Loss: 1.257, Validation Accuracy: 53.78%\n",
            "Epoch [3/10], Training Loss: 1.234, Validation Accuracy: 53.65%\n",
            "Epoch [4/10], Training Loss: 1.224, Validation Accuracy: 52.95%\n",
            "Epoch [5/10], Training Loss: 1.226, Validation Accuracy: 54.28%\n",
            "Epoch [6/10], Training Loss: 1.218, Validation Accuracy: 54.11%\n",
            "Epoch [7/10], Training Loss: 1.201, Validation Accuracy: 53.86%\n",
            "Epoch [8/10], Training Loss: 1.196, Validation Accuracy: 53.61%\n",
            "Epoch [9/10], Training Loss: 1.190, Validation Accuracy: 53.03%\n",
            "Epoch [10/10], Training Loss: 1.178, Validation Accuracy: 54.10%\n",
            "Epoch [1/10], Training Loss: 1.272, Validation Accuracy: 53.84%\n",
            "Epoch [2/10], Training Loss: 1.246, Validation Accuracy: 54.76%\n",
            "Epoch [3/10], Training Loss: 1.231, Validation Accuracy: 54.57%\n",
            "Epoch [4/10], Training Loss: 1.223, Validation Accuracy: 54.26%\n",
            "Epoch [5/10], Training Loss: 1.209, Validation Accuracy: 54.79%\n",
            "Epoch [6/10], Training Loss: 1.204, Validation Accuracy: 54.96%\n",
            "Epoch [7/10], Training Loss: 1.190, Validation Accuracy: 54.49%\n",
            "Epoch [8/10], Training Loss: 1.184, Validation Accuracy: 54.74%\n",
            "Epoch [9/10], Training Loss: 1.183, Validation Accuracy: 54.16%\n",
            "Epoch [10/10], Training Loss: 1.174, Validation Accuracy: 55.14%\n",
            "Epoch [1/10], Training Loss: 1.239, Validation Accuracy: 54.76%\n",
            "Epoch [2/10], Training Loss: 1.220, Validation Accuracy: 54.51%\n",
            "Epoch [3/10], Training Loss: 1.197, Validation Accuracy: 54.36%\n",
            "Epoch [4/10], Training Loss: 1.192, Validation Accuracy: 55.54%\n",
            "Epoch [5/10], Training Loss: 1.179, Validation Accuracy: 55.51%\n",
            "Epoch [6/10], Training Loss: 1.174, Validation Accuracy: 53.24%\n",
            "Epoch [7/10], Training Loss: 1.164, Validation Accuracy: 55.42%\n",
            "Epoch [8/10], Training Loss: 1.147, Validation Accuracy: 54.41%\n",
            "Epoch [9/10], Training Loss: 1.137, Validation Accuracy: 55.67%\n",
            "Epoch [10/10], Training Loss: 1.133, Validation Accuracy: 55.18%\n",
            "Epoch [1/10], Training Loss: 1.207, Validation Accuracy: 55.84%\n",
            "Epoch [2/10], Training Loss: 1.176, Validation Accuracy: 56.07%\n",
            "Epoch [3/10], Training Loss: 1.162, Validation Accuracy: 55.94%\n",
            "Epoch [4/10], Training Loss: 1.140, Validation Accuracy: 55.99%\n",
            "Epoch [5/10], Training Loss: 1.132, Validation Accuracy: 56.66%\n",
            "Epoch [6/10], Training Loss: 1.119, Validation Accuracy: 55.99%\n",
            "Epoch [7/10], Training Loss: 1.109, Validation Accuracy: 55.78%\n",
            "Epoch [8/10], Training Loss: 1.102, Validation Accuracy: 56.57%\n",
            "Epoch [9/10], Training Loss: 1.097, Validation Accuracy: 56.55%\n",
            "Epoch [10/10], Training Loss: 1.082, Validation Accuracy: 56.66%\n",
            "Epoch [1/10], Training Loss: 1.194, Validation Accuracy: 55.55%\n",
            "Epoch [2/10], Training Loss: 1.158, Validation Accuracy: 57.23%\n",
            "Epoch [3/10], Training Loss: 1.141, Validation Accuracy: 57.50%\n",
            "Epoch [4/10], Training Loss: 1.124, Validation Accuracy: 57.09%\n",
            "Epoch [5/10], Training Loss: 1.107, Validation Accuracy: 56.86%\n",
            "Epoch [6/10], Training Loss: 1.099, Validation Accuracy: 57.64%\n",
            "Epoch [7/10], Training Loss: 1.090, Validation Accuracy: 57.79%\n",
            "Epoch [8/10], Training Loss: 1.080, Validation Accuracy: 57.02%\n",
            "Epoch [9/10], Training Loss: 1.074, Validation Accuracy: 56.92%\n",
            "Epoch [10/10], Training Loss: 1.059, Validation Accuracy: 58.12%\n",
            "Epoch [1/10], Training Loss: 1.158, Validation Accuracy: 58.02%\n",
            "Epoch [2/10], Training Loss: 1.126, Validation Accuracy: 57.98%\n",
            "Epoch [3/10], Training Loss: 1.108, Validation Accuracy: 57.90%\n",
            "Epoch [4/10], Training Loss: 1.102, Validation Accuracy: 58.50%\n",
            "Epoch [5/10], Training Loss: 1.077, Validation Accuracy: 58.07%\n",
            "Epoch [6/10], Training Loss: 1.063, Validation Accuracy: 57.98%\n",
            "Epoch [7/10], Training Loss: 1.050, Validation Accuracy: 57.40%\n",
            "Epoch [8/10], Training Loss: 1.048, Validation Accuracy: 57.82%\n",
            "Epoch [9/10], Training Loss: 1.037, Validation Accuracy: 57.19%\n",
            "Epoch [10/10], Training Loss: 1.022, Validation Accuracy: 57.12%\n",
            "Epoch [1/10], Training Loss: 1.156, Validation Accuracy: 57.00%\n",
            "Epoch [2/10], Training Loss: 1.123, Validation Accuracy: 58.98%\n",
            "Epoch [3/10], Training Loss: 1.099, Validation Accuracy: 57.78%\n",
            "Epoch [4/10], Training Loss: 1.085, Validation Accuracy: 58.77%\n",
            "Epoch [5/10], Training Loss: 1.074, Validation Accuracy: 58.49%\n",
            "Epoch [6/10], Training Loss: 1.064, Validation Accuracy: 58.64%\n",
            "Epoch [7/10], Training Loss: 1.048, Validation Accuracy: 58.42%\n",
            "Epoch [8/10], Training Loss: 1.040, Validation Accuracy: 58.72%\n",
            "Epoch [9/10], Training Loss: 1.026, Validation Accuracy: 58.38%\n",
            "Epoch [10/10], Training Loss: 1.012, Validation Accuracy: 58.90%\n",
            "Epoch [1/10], Training Loss: 1.130, Validation Accuracy: 58.13%\n",
            "Epoch [2/10], Training Loss: 1.090, Validation Accuracy: 58.81%\n",
            "Epoch [3/10], Training Loss: 1.076, Validation Accuracy: 59.09%\n",
            "Epoch [4/10], Training Loss: 1.051, Validation Accuracy: 59.09%\n",
            "Epoch [5/10], Training Loss: 1.039, Validation Accuracy: 59.36%\n",
            "Epoch [6/10], Training Loss: 1.023, Validation Accuracy: 58.93%\n",
            "Epoch [7/10], Training Loss: 1.011, Validation Accuracy: 58.40%\n",
            "Epoch [8/10], Training Loss: 1.003, Validation Accuracy: 58.73%\n",
            "Epoch [9/10], Training Loss: 0.999, Validation Accuracy: 59.48%\n",
            "Epoch [10/10], Training Loss: 0.983, Validation Accuracy: 59.74%\n",
            "Epoch [1/10], Training Loss: 1.106, Validation Accuracy: 58.23%\n",
            "Epoch [2/10], Training Loss: 1.065, Validation Accuracy: 59.02%\n",
            "Epoch [3/10], Training Loss: 1.048, Validation Accuracy: 58.58%\n",
            "Epoch [4/10], Training Loss: 1.027, Validation Accuracy: 60.08%\n",
            "Epoch [5/10], Training Loss: 1.012, Validation Accuracy: 59.53%\n",
            "Epoch [6/10], Training Loss: 0.997, Validation Accuracy: 59.38%\n",
            "Epoch [7/10], Training Loss: 0.992, Validation Accuracy: 59.29%\n",
            "Epoch [8/10], Training Loss: 0.978, Validation Accuracy: 59.27%\n",
            "Epoch [9/10], Training Loss: 0.965, Validation Accuracy: 58.91%\n",
            "Epoch [10/10], Training Loss: 0.958, Validation Accuracy: 60.00%\n",
            "Epoch [1/10], Training Loss: 1.075, Validation Accuracy: 59.61%\n",
            "Epoch [2/10], Training Loss: 1.039, Validation Accuracy: 60.07%\n",
            "Epoch [3/10], Training Loss: 1.015, Validation Accuracy: 59.72%\n",
            "Epoch [4/10], Training Loss: 0.997, Validation Accuracy: 60.32%\n",
            "Epoch [5/10], Training Loss: 0.980, Validation Accuracy: 59.80%\n",
            "Epoch [6/10], Training Loss: 0.965, Validation Accuracy: 60.12%\n",
            "Epoch [7/10], Training Loss: 0.950, Validation Accuracy: 60.04%\n",
            "Epoch [8/10], Training Loss: 0.941, Validation Accuracy: 59.64%\n",
            "Epoch [9/10], Training Loss: 0.933, Validation Accuracy: 60.21%\n",
            "Epoch [10/10], Training Loss: 0.923, Validation Accuracy: 60.32%\n",
            "Epoch [1/10], Training Loss: 1.050, Validation Accuracy: 60.73%\n",
            "Epoch [2/10], Training Loss: 1.016, Validation Accuracy: 60.79%\n",
            "Epoch [3/10], Training Loss: 0.994, Validation Accuracy: 60.43%\n",
            "Epoch [4/10], Training Loss: 0.974, Validation Accuracy: 59.88%\n",
            "Epoch [5/10], Training Loss: 0.964, Validation Accuracy: 60.17%\n",
            "Epoch [6/10], Training Loss: 0.947, Validation Accuracy: 60.52%\n",
            "Epoch [7/10], Training Loss: 0.931, Validation Accuracy: 60.44%\n",
            "Epoch [8/10], Training Loss: 0.919, Validation Accuracy: 60.15%\n",
            "Epoch [9/10], Training Loss: 0.909, Validation Accuracy: 60.37%\n",
            "Epoch [10/10], Training Loss: 0.901, Validation Accuracy: 60.55%\n",
            "Epoch [1/10], Training Loss: 1.065, Validation Accuracy: 60.85%\n",
            "Epoch [2/10], Training Loss: 1.025, Validation Accuracy: 60.39%\n",
            "Epoch [3/10], Training Loss: 0.993, Validation Accuracy: 61.24%\n",
            "Epoch [4/10], Training Loss: 0.978, Validation Accuracy: 60.74%\n",
            "Epoch [5/10], Training Loss: 0.965, Validation Accuracy: 60.73%\n",
            "Epoch [6/10], Training Loss: 0.943, Validation Accuracy: 60.34%\n",
            "Epoch [7/10], Training Loss: 0.925, Validation Accuracy: 61.39%\n",
            "Epoch [8/10], Training Loss: 0.914, Validation Accuracy: 60.39%\n",
            "Epoch [9/10], Training Loss: 0.909, Validation Accuracy: 60.64%\n",
            "Epoch [10/10], Training Loss: 0.891, Validation Accuracy: 61.08%\n",
            "Epoch [1/10], Training Loss: 1.045, Validation Accuracy: 61.42%\n",
            "Epoch [2/10], Training Loss: 0.997, Validation Accuracy: 60.88%\n",
            "Epoch [3/10], Training Loss: 0.968, Validation Accuracy: 60.81%\n",
            "Epoch [4/10], Training Loss: 0.952, Validation Accuracy: 60.88%\n",
            "Epoch [5/10], Training Loss: 0.939, Validation Accuracy: 60.51%\n",
            "Epoch [6/10], Training Loss: 0.915, Validation Accuracy: 60.86%\n",
            "Epoch [7/10], Training Loss: 0.904, Validation Accuracy: 60.70%\n",
            "Epoch [8/10], Training Loss: 0.893, Validation Accuracy: 61.28%\n",
            "Epoch [9/10], Training Loss: 0.873, Validation Accuracy: 60.80%\n",
            "Epoch [10/10], Training Loss: 0.872, Validation Accuracy: 59.44%\n",
            "Epoch [1/10], Training Loss: 1.035, Validation Accuracy: 61.09%\n",
            "Epoch [2/10], Training Loss: 0.979, Validation Accuracy: 60.60%\n",
            "Epoch [3/10], Training Loss: 0.955, Validation Accuracy: 60.80%\n",
            "Epoch [4/10], Training Loss: 0.932, Validation Accuracy: 61.05%\n",
            "Epoch [5/10], Training Loss: 0.914, Validation Accuracy: 61.15%\n",
            "Epoch [6/10], Training Loss: 0.896, Validation Accuracy: 61.20%\n",
            "Epoch [7/10], Training Loss: 0.889, Validation Accuracy: 61.59%\n",
            "Epoch [8/10], Training Loss: 0.864, Validation Accuracy: 60.96%\n",
            "Epoch [9/10], Training Loss: 0.849, Validation Accuracy: 61.16%\n",
            "Epoch [10/10], Training Loss: 0.844, Validation Accuracy: 61.13%\n",
            "Epoch [1/10], Training Loss: 0.988, Validation Accuracy: 61.24%\n",
            "Epoch [2/10], Training Loss: 0.949, Validation Accuracy: 61.95%\n",
            "Epoch [3/10], Training Loss: 0.923, Validation Accuracy: 62.03%\n",
            "Epoch [4/10], Training Loss: 0.900, Validation Accuracy: 62.57%\n",
            "Epoch [5/10], Training Loss: 0.874, Validation Accuracy: 62.40%\n",
            "Epoch [6/10], Training Loss: 0.856, Validation Accuracy: 62.33%\n",
            "Epoch [7/10], Training Loss: 0.850, Validation Accuracy: 62.06%\n",
            "Epoch [8/10], Training Loss: 0.836, Validation Accuracy: 62.62%\n",
            "Epoch [9/10], Training Loss: 0.829, Validation Accuracy: 62.18%\n",
            "Epoch [10/10], Training Loss: 0.808, Validation Accuracy: 61.74%\n",
            "Epoch [1/10], Training Loss: 0.980, Validation Accuracy: 61.57%\n",
            "Epoch [2/10], Training Loss: 0.932, Validation Accuracy: 62.36%\n",
            "Epoch [3/10], Training Loss: 0.906, Validation Accuracy: 62.14%\n",
            "Epoch [4/10], Training Loss: 0.883, Validation Accuracy: 62.47%\n",
            "Epoch [5/10], Training Loss: 0.859, Validation Accuracy: 61.79%\n",
            "Epoch [6/10], Training Loss: 0.851, Validation Accuracy: 61.99%\n",
            "Epoch [7/10], Training Loss: 0.832, Validation Accuracy: 62.08%\n",
            "Epoch [8/10], Training Loss: 0.817, Validation Accuracy: 62.17%\n",
            "Epoch [9/10], Training Loss: 0.799, Validation Accuracy: 61.71%\n",
            "Epoch [10/10], Training Loss: 0.787, Validation Accuracy: 61.71%\n",
            "Epoch [1/10], Training Loss: 0.994, Validation Accuracy: 61.46%\n",
            "Epoch [2/10], Training Loss: 0.934, Validation Accuracy: 61.17%\n",
            "Epoch [3/10], Training Loss: 0.905, Validation Accuracy: 62.15%\n",
            "Epoch [4/10], Training Loss: 0.875, Validation Accuracy: 62.20%\n",
            "Epoch [5/10], Training Loss: 0.861, Validation Accuracy: 61.85%\n",
            "Epoch [6/10], Training Loss: 0.850, Validation Accuracy: 62.31%\n",
            "Epoch [7/10], Training Loss: 0.828, Validation Accuracy: 61.87%\n",
            "Epoch [8/10], Training Loss: 0.804, Validation Accuracy: 62.74%\n",
            "Epoch [9/10], Training Loss: 0.798, Validation Accuracy: 61.92%\n",
            "Epoch [10/10], Training Loss: 0.779, Validation Accuracy: 61.48%\n",
            "Epoch [1/10], Training Loss: 0.972, Validation Accuracy: 61.85%\n",
            "Epoch [2/10], Training Loss: 0.910, Validation Accuracy: 61.94%\n",
            "Epoch [3/10], Training Loss: 0.884, Validation Accuracy: 61.73%\n",
            "Epoch [4/10], Training Loss: 0.867, Validation Accuracy: 62.61%\n",
            "Epoch [5/10], Training Loss: 0.837, Validation Accuracy: 62.22%\n",
            "Epoch [6/10], Training Loss: 0.819, Validation Accuracy: 62.17%\n",
            "Epoch [7/10], Training Loss: 0.804, Validation Accuracy: 62.41%\n",
            "Epoch [8/10], Training Loss: 0.787, Validation Accuracy: 61.05%\n",
            "Epoch [9/10], Training Loss: 0.783, Validation Accuracy: 62.44%\n",
            "Epoch [10/10], Training Loss: 0.753, Validation Accuracy: 61.80%\n",
            "Epoch [1/10], Training Loss: 0.952, Validation Accuracy: 62.54%\n",
            "Epoch [2/10], Training Loss: 0.899, Validation Accuracy: 62.24%\n",
            "Epoch [3/10], Training Loss: 0.863, Validation Accuracy: 62.31%\n",
            "Epoch [4/10], Training Loss: 0.843, Validation Accuracy: 62.83%\n",
            "Epoch [5/10], Training Loss: 0.822, Validation Accuracy: 62.10%\n",
            "Epoch [6/10], Training Loss: 0.814, Validation Accuracy: 62.96%\n",
            "Epoch [7/10], Training Loss: 0.785, Validation Accuracy: 62.19%\n",
            "Epoch [8/10], Training Loss: 0.777, Validation Accuracy: 62.27%\n",
            "Epoch [9/10], Training Loss: 0.751, Validation Accuracy: 62.28%\n",
            "Epoch [10/10], Training Loss: 0.734, Validation Accuracy: 61.74%\n",
            "Epoch [1/10], Training Loss: 0.938, Validation Accuracy: 61.79%\n",
            "Epoch [2/10], Training Loss: 0.880, Validation Accuracy: 63.21%\n",
            "Epoch [3/10], Training Loss: 0.835, Validation Accuracy: 63.33%\n",
            "Epoch [4/10], Training Loss: 0.812, Validation Accuracy: 63.17%\n",
            "Epoch [5/10], Training Loss: 0.795, Validation Accuracy: 62.98%\n",
            "Epoch [6/10], Training Loss: 0.779, Validation Accuracy: 63.27%\n",
            "Epoch [7/10], Training Loss: 0.754, Validation Accuracy: 62.54%\n",
            "Epoch [8/10], Training Loss: 0.733, Validation Accuracy: 62.89%\n",
            "Epoch [9/10], Training Loss: 0.727, Validation Accuracy: 63.09%\n",
            "Epoch [10/10], Training Loss: 0.711, Validation Accuracy: 63.34%\n",
            "Epoch [1/10], Training Loss: 0.913, Validation Accuracy: 62.75%\n",
            "Epoch [2/10], Training Loss: 0.863, Validation Accuracy: 63.34%\n",
            "Epoch [3/10], Training Loss: 0.824, Validation Accuracy: 61.56%\n",
            "Epoch [4/10], Training Loss: 0.791, Validation Accuracy: 63.47%\n",
            "Epoch [5/10], Training Loss: 0.777, Validation Accuracy: 63.16%\n",
            "Epoch [6/10], Training Loss: 0.754, Validation Accuracy: 63.31%\n",
            "Epoch [7/10], Training Loss: 0.735, Validation Accuracy: 63.06%\n",
            "Epoch [8/10], Training Loss: 0.717, Validation Accuracy: 62.82%\n",
            "Epoch [9/10], Training Loss: 0.706, Validation Accuracy: 62.95%\n",
            "Epoch [10/10], Training Loss: 0.685, Validation Accuracy: 62.74%\n",
            "Epoch [1/10], Training Loss: 0.913, Validation Accuracy: 62.90%\n",
            "Epoch [2/10], Training Loss: 0.846, Validation Accuracy: 62.83%\n",
            "Epoch [3/10], Training Loss: 0.811, Validation Accuracy: 63.20%\n",
            "Epoch [4/10], Training Loss: 0.779, Validation Accuracy: 62.53%\n",
            "Epoch [5/10], Training Loss: 0.763, Validation Accuracy: 62.41%\n",
            "Epoch [6/10], Training Loss: 0.746, Validation Accuracy: 63.05%\n",
            "Epoch [7/10], Training Loss: 0.731, Validation Accuracy: 61.45%\n",
            "Epoch [8/10], Training Loss: 0.707, Validation Accuracy: 62.48%\n",
            "Epoch [9/10], Training Loss: 0.692, Validation Accuracy: 62.50%\n",
            "Epoch [10/10], Training Loss: 0.672, Validation Accuracy: 62.62%\n",
            "Epoch [1/10], Training Loss: 0.913, Validation Accuracy: 62.42%\n",
            "Epoch [2/10], Training Loss: 0.846, Validation Accuracy: 62.93%\n",
            "Epoch [3/10], Training Loss: 0.820, Validation Accuracy: 62.96%\n",
            "Epoch [4/10], Training Loss: 0.778, Validation Accuracy: 62.91%\n",
            "Epoch [5/10], Training Loss: 0.759, Validation Accuracy: 62.91%\n",
            "Epoch [6/10], Training Loss: 0.727, Validation Accuracy: 62.56%\n",
            "Epoch [7/10], Training Loss: 0.713, Validation Accuracy: 62.97%\n",
            "Epoch [8/10], Training Loss: 0.702, Validation Accuracy: 62.34%\n",
            "Epoch [9/10], Training Loss: 0.677, Validation Accuracy: 62.19%\n",
            "Epoch [10/10], Training Loss: 0.663, Validation Accuracy: 62.80%\n",
            "Epoch [1/10], Training Loss: 0.894, Validation Accuracy: 62.84%\n",
            "Epoch [2/10], Training Loss: 0.828, Validation Accuracy: 63.04%\n",
            "Epoch [3/10], Training Loss: 0.786, Validation Accuracy: 63.47%\n",
            "Epoch [4/10], Training Loss: 0.754, Validation Accuracy: 62.89%\n",
            "Epoch [5/10], Training Loss: 0.734, Validation Accuracy: 63.02%\n",
            "Epoch [6/10], Training Loss: 0.712, Validation Accuracy: 63.04%\n",
            "Epoch [7/10], Training Loss: 0.690, Validation Accuracy: 62.60%\n",
            "Epoch [8/10], Training Loss: 0.673, Validation Accuracy: 62.06%\n",
            "Epoch [9/10], Training Loss: 0.656, Validation Accuracy: 62.04%\n",
            "Epoch [10/10], Training Loss: 0.636, Validation Accuracy: 62.25%\n",
            "Epoch [1/10], Training Loss: 0.877, Validation Accuracy: 63.25%\n",
            "Epoch [2/10], Training Loss: 0.801, Validation Accuracy: 63.82%\n",
            "Epoch [3/10], Training Loss: 0.771, Validation Accuracy: 63.37%\n",
            "Epoch [4/10], Training Loss: 0.737, Validation Accuracy: 63.77%\n",
            "Epoch [5/10], Training Loss: 0.708, Validation Accuracy: 63.20%\n",
            "Epoch [6/10], Training Loss: 0.684, Validation Accuracy: 63.56%\n",
            "Epoch [7/10], Training Loss: 0.660, Validation Accuracy: 64.21%\n",
            "Epoch [8/10], Training Loss: 0.640, Validation Accuracy: 62.89%\n",
            "Epoch [9/10], Training Loss: 0.633, Validation Accuracy: 63.41%\n",
            "Epoch [10/10], Training Loss: 0.608, Validation Accuracy: 63.18%\n",
            "Epoch [1/10], Training Loss: 0.857, Validation Accuracy: 62.79%\n",
            "Epoch [2/10], Training Loss: 0.783, Validation Accuracy: 63.35%\n",
            "Epoch [3/10], Training Loss: 0.741, Validation Accuracy: 63.55%\n",
            "Epoch [4/10], Training Loss: 0.709, Validation Accuracy: 63.09%\n",
            "Epoch [5/10], Training Loss: 0.695, Validation Accuracy: 63.15%\n",
            "Epoch [6/10], Training Loss: 0.660, Validation Accuracy: 63.03%\n",
            "Epoch [7/10], Training Loss: 0.641, Validation Accuracy: 63.22%\n",
            "Epoch [8/10], Training Loss: 0.618, Validation Accuracy: 62.86%\n",
            "Epoch [9/10], Training Loss: 0.602, Validation Accuracy: 62.80%\n",
            "Epoch [10/10], Training Loss: 0.591, Validation Accuracy: 63.05%\n",
            "Epoch [1/10], Training Loss: 0.863, Validation Accuracy: 62.96%\n",
            "Epoch [2/10], Training Loss: 0.784, Validation Accuracy: 62.40%\n",
            "Epoch [3/10], Training Loss: 0.739, Validation Accuracy: 62.89%\n",
            "Epoch [4/10], Training Loss: 0.706, Validation Accuracy: 63.21%\n",
            "Epoch [5/10], Training Loss: 0.678, Validation Accuracy: 62.95%\n",
            "Epoch [6/10], Training Loss: 0.660, Validation Accuracy: 63.12%\n",
            "Epoch [7/10], Training Loss: 0.630, Validation Accuracy: 62.27%\n",
            "Epoch [8/10], Training Loss: 0.625, Validation Accuracy: 62.31%\n",
            "Epoch [9/10], Training Loss: 0.598, Validation Accuracy: 62.46%\n",
            "Epoch [10/10], Training Loss: 0.587, Validation Accuracy: 62.94%\n",
            "Epoch [1/10], Training Loss: 0.855, Validation Accuracy: 62.16%\n",
            "Epoch [2/10], Training Loss: 0.771, Validation Accuracy: 62.38%\n",
            "Epoch [3/10], Training Loss: 0.725, Validation Accuracy: 62.32%\n",
            "Epoch [4/10], Training Loss: 0.701, Validation Accuracy: 62.67%\n",
            "Epoch [5/10], Training Loss: 0.665, Validation Accuracy: 62.92%\n",
            "Epoch [6/10], Training Loss: 0.641, Validation Accuracy: 63.08%\n",
            "Epoch [7/10], Training Loss: 0.624, Validation Accuracy: 62.55%\n",
            "Epoch [8/10], Training Loss: 0.606, Validation Accuracy: 62.39%\n",
            "Epoch [9/10], Training Loss: 0.595, Validation Accuracy: 62.75%\n",
            "Epoch [10/10], Training Loss: 0.567, Validation Accuracy: 62.76%\n",
            "Epoch [1/10], Training Loss: 0.848, Validation Accuracy: 62.22%\n",
            "Epoch [2/10], Training Loss: 0.761, Validation Accuracy: 63.41%\n",
            "Epoch [3/10], Training Loss: 0.701, Validation Accuracy: 62.57%\n",
            "Epoch [4/10], Training Loss: 0.683, Validation Accuracy: 62.72%\n",
            "Epoch [5/10], Training Loss: 0.648, Validation Accuracy: 62.64%\n",
            "Epoch [6/10], Training Loss: 0.618, Validation Accuracy: 62.52%\n",
            "Epoch [7/10], Training Loss: 0.606, Validation Accuracy: 61.68%\n",
            "Epoch [8/10], Training Loss: 0.587, Validation Accuracy: 62.20%\n",
            "Epoch [9/10], Training Loss: 0.564, Validation Accuracy: 62.02%\n",
            "Epoch [10/10], Training Loss: 0.544, Validation Accuracy: 62.78%\n",
            "Epoch [1/10], Training Loss: 0.831, Validation Accuracy: 62.28%\n",
            "Epoch [2/10], Training Loss: 0.736, Validation Accuracy: 63.68%\n",
            "Epoch [3/10], Training Loss: 0.691, Validation Accuracy: 63.26%\n",
            "Epoch [4/10], Training Loss: 0.652, Validation Accuracy: 62.42%\n",
            "Epoch [5/10], Training Loss: 0.636, Validation Accuracy: 63.62%\n",
            "Epoch [6/10], Training Loss: 0.612, Validation Accuracy: 63.72%\n",
            "Epoch [7/10], Training Loss: 0.584, Validation Accuracy: 63.54%\n",
            "Epoch [8/10], Training Loss: 0.553, Validation Accuracy: 63.75%\n",
            "Epoch [9/10], Training Loss: 0.541, Validation Accuracy: 63.86%\n",
            "Epoch [10/10], Training Loss: 0.517, Validation Accuracy: 63.65%\n",
            "Epoch [1/10], Training Loss: 0.812, Validation Accuracy: 63.43%\n",
            "Epoch [2/10], Training Loss: 0.722, Validation Accuracy: 62.98%\n",
            "Epoch [3/10], Training Loss: 0.670, Validation Accuracy: 62.39%\n",
            "Epoch [4/10], Training Loss: 0.631, Validation Accuracy: 63.41%\n",
            "Epoch [5/10], Training Loss: 0.601, Validation Accuracy: 63.28%\n",
            "Epoch [6/10], Training Loss: 0.581, Validation Accuracy: 63.32%\n",
            "Epoch [7/10], Training Loss: 0.560, Validation Accuracy: 62.90%\n",
            "Epoch [8/10], Training Loss: 0.539, Validation Accuracy: 62.71%\n",
            "Epoch [9/10], Training Loss: 0.515, Validation Accuracy: 62.84%\n",
            "Epoch [10/10], Training Loss: 0.492, Validation Accuracy: 62.60%\n",
            "Epoch [1/10], Training Loss: 0.812, Validation Accuracy: 63.21%\n",
            "Epoch [2/10], Training Loss: 0.710, Validation Accuracy: 63.39%\n",
            "Epoch [3/10], Training Loss: 0.673, Validation Accuracy: 62.60%\n",
            "Epoch [4/10], Training Loss: 0.635, Validation Accuracy: 63.07%\n",
            "Epoch [5/10], Training Loss: 0.593, Validation Accuracy: 62.64%\n",
            "Epoch [6/10], Training Loss: 0.576, Validation Accuracy: 62.98%\n",
            "Epoch [7/10], Training Loss: 0.547, Validation Accuracy: 62.79%\n",
            "Epoch [8/10], Training Loss: 0.527, Validation Accuracy: 62.72%\n",
            "Epoch [9/10], Training Loss: 0.512, Validation Accuracy: 62.38%\n",
            "Epoch [10/10], Training Loss: 0.492, Validation Accuracy: 62.76%\n",
            "Epoch [1/10], Training Loss: 0.813, Validation Accuracy: 62.41%\n",
            "Epoch [2/10], Training Loss: 0.711, Validation Accuracy: 62.81%\n",
            "Epoch [3/10], Training Loss: 0.659, Validation Accuracy: 62.83%\n",
            "Epoch [4/10], Training Loss: 0.622, Validation Accuracy: 61.81%\n",
            "Epoch [5/10], Training Loss: 0.594, Validation Accuracy: 62.93%\n",
            "Epoch [6/10], Training Loss: 0.570, Validation Accuracy: 62.40%\n",
            "Epoch [7/10], Training Loss: 0.540, Validation Accuracy: 62.93%\n",
            "Epoch [8/10], Training Loss: 0.519, Validation Accuracy: 62.82%\n",
            "Epoch [9/10], Training Loss: 0.500, Validation Accuracy: 62.22%\n",
            "Epoch [10/10], Training Loss: 0.484, Validation Accuracy: 62.58%\n",
            "Epoch [1/10], Training Loss: 0.806, Validation Accuracy: 62.42%\n",
            "Epoch [2/10], Training Loss: 0.701, Validation Accuracy: 62.21%\n",
            "Epoch [3/10], Training Loss: 0.646, Validation Accuracy: 63.09%\n",
            "Epoch [4/10], Training Loss: 0.599, Validation Accuracy: 62.76%\n",
            "Epoch [5/10], Training Loss: 0.563, Validation Accuracy: 62.37%\n",
            "Epoch [6/10], Training Loss: 0.546, Validation Accuracy: 63.00%\n",
            "Epoch [7/10], Training Loss: 0.514, Validation Accuracy: 61.95%\n",
            "Epoch [8/10], Training Loss: 0.489, Validation Accuracy: 62.74%\n",
            "Epoch [9/10], Training Loss: 0.476, Validation Accuracy: 62.01%\n",
            "Epoch [10/10], Training Loss: 0.452, Validation Accuracy: 62.55%\n",
            "Epoch [1/10], Training Loss: 0.785, Validation Accuracy: 62.76%\n",
            "Epoch [2/10], Training Loss: 0.679, Validation Accuracy: 62.89%\n",
            "Epoch [3/10], Training Loss: 0.616, Validation Accuracy: 63.67%\n",
            "Epoch [4/10], Training Loss: 0.574, Validation Accuracy: 63.56%\n",
            "Epoch [5/10], Training Loss: 0.548, Validation Accuracy: 62.54%\n",
            "Epoch [6/10], Training Loss: 0.522, Validation Accuracy: 63.36%\n",
            "Epoch [7/10], Training Loss: 0.488, Validation Accuracy: 63.33%\n",
            "Epoch [8/10], Training Loss: 0.475, Validation Accuracy: 63.58%\n",
            "Epoch [9/10], Training Loss: 0.465, Validation Accuracy: 62.77%\n",
            "Epoch [10/10], Training Loss: 0.428, Validation Accuracy: 63.09%\n",
            "Epoch [1/10], Training Loss: 0.765, Validation Accuracy: 62.65%\n",
            "Epoch [2/10], Training Loss: 0.663, Validation Accuracy: 63.36%\n",
            "Epoch [3/10], Training Loss: 0.598, Validation Accuracy: 62.96%\n",
            "Epoch [4/10], Training Loss: 0.564, Validation Accuracy: 62.80%\n",
            "Epoch [5/10], Training Loss: 0.523, Validation Accuracy: 62.14%\n",
            "Epoch [6/10], Training Loss: 0.493, Validation Accuracy: 63.05%\n",
            "Epoch [7/10], Training Loss: 0.472, Validation Accuracy: 62.61%\n",
            "Epoch [8/10], Training Loss: 0.459, Validation Accuracy: 62.67%\n",
            "Epoch [9/10], Training Loss: 0.434, Validation Accuracy: 62.14%\n",
            "Epoch [10/10], Training Loss: 0.418, Validation Accuracy: 62.34%\n",
            "Epoch [1/10], Training Loss: 0.790, Validation Accuracy: 62.16%\n",
            "Epoch [2/10], Training Loss: 0.648, Validation Accuracy: 62.52%\n",
            "Epoch [3/10], Training Loss: 0.591, Validation Accuracy: 63.49%\n",
            "Epoch [4/10], Training Loss: 0.545, Validation Accuracy: 62.51%\n",
            "Epoch [5/10], Training Loss: 0.512, Validation Accuracy: 62.24%\n",
            "Epoch [6/10], Training Loss: 0.490, Validation Accuracy: 62.13%\n",
            "Epoch [7/10], Training Loss: 0.459, Validation Accuracy: 62.72%\n",
            "Epoch [8/10], Training Loss: 0.445, Validation Accuracy: 62.43%\n",
            "Epoch [9/10], Training Loss: 0.422, Validation Accuracy: 62.43%\n",
            "Epoch [10/10], Training Loss: 0.397, Validation Accuracy: 61.95%\n",
            "Confusion Matrix:\n",
            "[[650  33 101  34  28   5  15   8  84  42]\n",
            " [ 23 766  14  16   5   6   6   7  45 112]\n",
            " [ 45  10 543 105  92  53  75  41  20  16]\n",
            " [ 13  14 114 496  64 125  83  45  24  22]\n",
            " [ 23   2 115  99 537  39  63  97  22   3]\n",
            " [ 15   5  86 245  60 458  35  79   9   8]\n",
            " [ 12   7  60  93  68  23 698  22  11   6]\n",
            " [ 19   8  68  64  75  66  10 670   2  18]\n",
            " [103  52  23  34  15   4   7   7 725  30]\n",
            " [ 54 124  16  38  15   8   9  32  55 649]]\n",
            "Test Accuracy: 61.92%\n",
            "True Positives (TP): [650 766 543 496 537 458 698 670 725 649]\n",
            "False Positives (FP): [307 255 597 728 422 329 303 338 272 257]\n",
            "True Negatives (TN): [8693 8745 8403 8272 8578 8671 8697 8662 8728 8743]\n",
            "False Negatives (FN): [350 234 457 504 463 542 302 330 275 351]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.67920585 0.75024486 0.47631579 0.40522876 0.55995829 0.5819568\n",
            " 0.6973027  0.66468254 0.72718154 0.71633554]\n",
            "Recall: [0.65  0.766 0.543 0.496 0.537 0.458 0.698 0.67  0.725 0.649]\n",
            "F1 Score: [0.66428206 0.75804057 0.50747664 0.44604317 0.5482389  0.51259093\n",
            " 0.69765117 0.66733068 0.72608913 0.68100735]\n",
            "CPU times: user 3h 2min 12s, sys: 1min 17s, total: 3h 3min 30s\n",
            "Wall time: 3h 18min 10s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 0.5):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=0.5):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_normal: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = ( augmented_data_normal + augmented_data_truncated) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=0.5)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"normal\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs-pp8pRSXy-"
      },
      "source": [
        "Beta=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-D7fSt4SZbw",
        "outputId": "be52663b-8bdf-4ba9-88a6-7daab38c0277"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 61.2MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Images per Class: [5960 6073 5991 6056 6048 5929 6006 5973 5940 6024]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<timed exec>:281: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Training Loss: 2.304, Validation Accuracy: 7.90%\n",
            "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 9.24%\n",
            "Epoch [3/10], Training Loss: 2.303, Validation Accuracy: 10.46%\n",
            "Epoch [4/10], Training Loss: 2.302, Validation Accuracy: 11.80%\n",
            "Epoch [5/10], Training Loss: 2.302, Validation Accuracy: 12.91%\n",
            "Epoch [6/10], Training Loss: 2.301, Validation Accuracy: 13.52%\n",
            "Epoch [7/10], Training Loss: 2.300, Validation Accuracy: 13.52%\n",
            "Epoch [8/10], Training Loss: 2.299, Validation Accuracy: 14.20%\n",
            "Epoch [9/10], Training Loss: 2.298, Validation Accuracy: 14.56%\n",
            "Epoch [10/10], Training Loss: 2.297, Validation Accuracy: 14.87%\n",
            "Epoch [1/10], Training Loss: 2.297, Validation Accuracy: 16.06%\n",
            "Epoch [2/10], Training Loss: 2.295, Validation Accuracy: 16.74%\n",
            "Epoch [3/10], Training Loss: 2.292, Validation Accuracy: 17.41%\n",
            "Epoch [4/10], Training Loss: 2.289, Validation Accuracy: 17.58%\n",
            "Epoch [5/10], Training Loss: 2.284, Validation Accuracy: 16.10%\n",
            "Epoch [6/10], Training Loss: 2.276, Validation Accuracy: 15.82%\n",
            "Epoch [7/10], Training Loss: 2.264, Validation Accuracy: 15.54%\n",
            "Epoch [8/10], Training Loss: 2.246, Validation Accuracy: 17.51%\n",
            "Epoch [9/10], Training Loss: 2.220, Validation Accuracy: 18.58%\n",
            "Epoch [10/10], Training Loss: 2.188, Validation Accuracy: 19.59%\n",
            "Epoch [1/10], Training Loss: 2.147, Validation Accuracy: 20.97%\n",
            "Epoch [2/10], Training Loss: 2.111, Validation Accuracy: 23.73%\n",
            "Epoch [3/10], Training Loss: 2.084, Validation Accuracy: 23.70%\n",
            "Epoch [4/10], Training Loss: 2.061, Validation Accuracy: 24.81%\n",
            "Epoch [5/10], Training Loss: 2.043, Validation Accuracy: 25.87%\n",
            "Epoch [6/10], Training Loss: 2.027, Validation Accuracy: 25.86%\n",
            "Epoch [7/10], Training Loss: 2.012, Validation Accuracy: 27.09%\n",
            "Epoch [8/10], Training Loss: 2.000, Validation Accuracy: 27.11%\n",
            "Epoch [9/10], Training Loss: 1.986, Validation Accuracy: 27.26%\n",
            "Epoch [10/10], Training Loss: 1.973, Validation Accuracy: 27.70%\n",
            "Epoch [1/10], Training Loss: 1.963, Validation Accuracy: 27.94%\n",
            "Epoch [2/10], Training Loss: 1.951, Validation Accuracy: 28.84%\n",
            "Epoch [3/10], Training Loss: 1.940, Validation Accuracy: 29.20%\n",
            "Epoch [4/10], Training Loss: 1.931, Validation Accuracy: 29.16%\n",
            "Epoch [5/10], Training Loss: 1.917, Validation Accuracy: 29.85%\n",
            "Epoch [6/10], Training Loss: 1.907, Validation Accuracy: 30.17%\n",
            "Epoch [7/10], Training Loss: 1.898, Validation Accuracy: 30.95%\n",
            "Epoch [8/10], Training Loss: 1.885, Validation Accuracy: 31.86%\n",
            "Epoch [9/10], Training Loss: 1.870, Validation Accuracy: 31.89%\n",
            "Epoch [10/10], Training Loss: 1.861, Validation Accuracy: 32.13%\n",
            "Epoch [1/10], Training Loss: 1.860, Validation Accuracy: 32.09%\n",
            "Epoch [2/10], Training Loss: 1.847, Validation Accuracy: 33.81%\n",
            "Epoch [3/10], Training Loss: 1.829, Validation Accuracy: 34.18%\n",
            "Epoch [4/10], Training Loss: 1.811, Validation Accuracy: 35.09%\n",
            "Epoch [5/10], Training Loss: 1.794, Validation Accuracy: 35.47%\n",
            "Epoch [6/10], Training Loss: 1.779, Validation Accuracy: 36.31%\n",
            "Epoch [7/10], Training Loss: 1.759, Validation Accuracy: 35.98%\n",
            "Epoch [8/10], Training Loss: 1.743, Validation Accuracy: 36.60%\n",
            "Epoch [9/10], Training Loss: 1.727, Validation Accuracy: 37.28%\n",
            "Epoch [10/10], Training Loss: 1.709, Validation Accuracy: 37.75%\n",
            "Epoch [1/10], Training Loss: 1.706, Validation Accuracy: 38.60%\n",
            "Epoch [2/10], Training Loss: 1.686, Validation Accuracy: 39.51%\n",
            "Epoch [3/10], Training Loss: 1.669, Validation Accuracy: 39.88%\n",
            "Epoch [4/10], Training Loss: 1.650, Validation Accuracy: 39.64%\n",
            "Epoch [5/10], Training Loss: 1.637, Validation Accuracy: 39.27%\n",
            "Epoch [6/10], Training Loss: 1.620, Validation Accuracy: 40.43%\n",
            "Epoch [7/10], Training Loss: 1.599, Validation Accuracy: 41.41%\n",
            "Epoch [8/10], Training Loss: 1.591, Validation Accuracy: 42.01%\n",
            "Epoch [9/10], Training Loss: 1.572, Validation Accuracy: 41.79%\n",
            "Epoch [10/10], Training Loss: 1.561, Validation Accuracy: 42.63%\n",
            "Epoch [1/10], Training Loss: 1.603, Validation Accuracy: 42.76%\n",
            "Epoch [2/10], Training Loss: 1.587, Validation Accuracy: 43.57%\n",
            "Epoch [3/10], Training Loss: 1.566, Validation Accuracy: 44.00%\n",
            "Epoch [4/10], Training Loss: 1.550, Validation Accuracy: 44.06%\n",
            "Epoch [5/10], Training Loss: 1.539, Validation Accuracy: 44.80%\n",
            "Epoch [6/10], Training Loss: 1.528, Validation Accuracy: 45.11%\n",
            "Epoch [7/10], Training Loss: 1.520, Validation Accuracy: 45.55%\n",
            "Epoch [8/10], Training Loss: 1.504, Validation Accuracy: 45.22%\n",
            "Epoch [9/10], Training Loss: 1.497, Validation Accuracy: 44.95%\n",
            "Epoch [10/10], Training Loss: 1.494, Validation Accuracy: 46.34%\n",
            "Epoch [1/10], Training Loss: 1.508, Validation Accuracy: 46.42%\n",
            "Epoch [2/10], Training Loss: 1.487, Validation Accuracy: 46.55%\n",
            "Epoch [3/10], Training Loss: 1.473, Validation Accuracy: 46.97%\n",
            "Epoch [4/10], Training Loss: 1.465, Validation Accuracy: 47.06%\n",
            "Epoch [5/10], Training Loss: 1.455, Validation Accuracy: 46.95%\n",
            "Epoch [6/10], Training Loss: 1.445, Validation Accuracy: 47.35%\n",
            "Epoch [7/10], Training Loss: 1.438, Validation Accuracy: 47.82%\n",
            "Epoch [8/10], Training Loss: 1.428, Validation Accuracy: 48.10%\n",
            "Epoch [9/10], Training Loss: 1.414, Validation Accuracy: 47.42%\n",
            "Epoch [10/10], Training Loss: 1.411, Validation Accuracy: 47.34%\n",
            "Epoch [1/10], Training Loss: 1.435, Validation Accuracy: 48.39%\n",
            "Epoch [2/10], Training Loss: 1.418, Validation Accuracy: 48.62%\n",
            "Epoch [3/10], Training Loss: 1.404, Validation Accuracy: 47.90%\n",
            "Epoch [4/10], Training Loss: 1.398, Validation Accuracy: 48.83%\n",
            "Epoch [5/10], Training Loss: 1.384, Validation Accuracy: 48.69%\n",
            "Epoch [6/10], Training Loss: 1.385, Validation Accuracy: 48.64%\n",
            "Epoch [7/10], Training Loss: 1.368, Validation Accuracy: 50.30%\n",
            "Epoch [8/10], Training Loss: 1.363, Validation Accuracy: 49.22%\n",
            "Epoch [9/10], Training Loss: 1.352, Validation Accuracy: 49.30%\n",
            "Epoch [10/10], Training Loss: 1.355, Validation Accuracy: 49.48%\n",
            "Epoch [1/10], Training Loss: 1.406, Validation Accuracy: 49.91%\n",
            "Epoch [2/10], Training Loss: 1.387, Validation Accuracy: 50.00%\n",
            "Epoch [3/10], Training Loss: 1.376, Validation Accuracy: 50.67%\n",
            "Epoch [4/10], Training Loss: 1.364, Validation Accuracy: 50.95%\n",
            "Epoch [5/10], Training Loss: 1.352, Validation Accuracy: 50.75%\n",
            "Epoch [6/10], Training Loss: 1.349, Validation Accuracy: 49.75%\n",
            "Epoch [7/10], Training Loss: 1.342, Validation Accuracy: 50.91%\n",
            "Epoch [8/10], Training Loss: 1.327, Validation Accuracy: 50.73%\n",
            "Epoch [9/10], Training Loss: 1.322, Validation Accuracy: 50.49%\n",
            "Epoch [10/10], Training Loss: 1.321, Validation Accuracy: 50.76%\n",
            "Epoch [1/10], Training Loss: 1.356, Validation Accuracy: 51.86%\n",
            "Epoch [2/10], Training Loss: 1.327, Validation Accuracy: 51.14%\n",
            "Epoch [3/10], Training Loss: 1.320, Validation Accuracy: 51.69%\n",
            "Epoch [4/10], Training Loss: 1.305, Validation Accuracy: 51.82%\n",
            "Epoch [5/10], Training Loss: 1.297, Validation Accuracy: 51.79%\n",
            "Epoch [6/10], Training Loss: 1.281, Validation Accuracy: 52.88%\n",
            "Epoch [7/10], Training Loss: 1.276, Validation Accuracy: 52.08%\n",
            "Epoch [8/10], Training Loss: 1.268, Validation Accuracy: 52.55%\n",
            "Epoch [9/10], Training Loss: 1.255, Validation Accuracy: 52.61%\n",
            "Epoch [10/10], Training Loss: 1.248, Validation Accuracy: 52.26%\n",
            "Epoch [1/10], Training Loss: 1.344, Validation Accuracy: 52.72%\n",
            "Epoch [2/10], Training Loss: 1.321, Validation Accuracy: 52.77%\n",
            "Epoch [3/10], Training Loss: 1.307, Validation Accuracy: 52.69%\n",
            "Epoch [4/10], Training Loss: 1.296, Validation Accuracy: 52.34%\n",
            "Epoch [5/10], Training Loss: 1.300, Validation Accuracy: 53.09%\n",
            "Epoch [6/10], Training Loss: 1.276, Validation Accuracy: 52.88%\n",
            "Epoch [7/10], Training Loss: 1.265, Validation Accuracy: 52.87%\n",
            "Epoch [8/10], Training Loss: 1.258, Validation Accuracy: 53.04%\n",
            "Epoch [9/10], Training Loss: 1.252, Validation Accuracy: 52.33%\n",
            "Epoch [10/10], Training Loss: 1.242, Validation Accuracy: 54.13%\n",
            "Epoch [1/10], Training Loss: 1.292, Validation Accuracy: 53.53%\n",
            "Epoch [2/10], Training Loss: 1.268, Validation Accuracy: 54.04%\n",
            "Epoch [3/10], Training Loss: 1.262, Validation Accuracy: 53.71%\n",
            "Epoch [4/10], Training Loss: 1.247, Validation Accuracy: 54.18%\n",
            "Epoch [5/10], Training Loss: 1.238, Validation Accuracy: 54.01%\n",
            "Epoch [6/10], Training Loss: 1.230, Validation Accuracy: 54.17%\n",
            "Epoch [7/10], Training Loss: 1.216, Validation Accuracy: 54.84%\n",
            "Epoch [8/10], Training Loss: 1.209, Validation Accuracy: 54.53%\n",
            "Epoch [9/10], Training Loss: 1.202, Validation Accuracy: 54.36%\n",
            "Epoch [10/10], Training Loss: 1.199, Validation Accuracy: 54.61%\n",
            "Epoch [1/10], Training Loss: 1.268, Validation Accuracy: 53.68%\n",
            "Epoch [2/10], Training Loss: 1.243, Validation Accuracy: 54.15%\n",
            "Epoch [3/10], Training Loss: 1.229, Validation Accuracy: 54.76%\n",
            "Epoch [4/10], Training Loss: 1.222, Validation Accuracy: 55.36%\n",
            "Epoch [5/10], Training Loss: 1.199, Validation Accuracy: 55.39%\n",
            "Epoch [6/10], Training Loss: 1.194, Validation Accuracy: 54.63%\n",
            "Epoch [7/10], Training Loss: 1.187, Validation Accuracy: 55.16%\n",
            "Epoch [8/10], Training Loss: 1.178, Validation Accuracy: 54.37%\n",
            "Epoch [9/10], Training Loss: 1.171, Validation Accuracy: 55.13%\n",
            "Epoch [10/10], Training Loss: 1.158, Validation Accuracy: 54.96%\n",
            "Epoch [1/10], Training Loss: 1.250, Validation Accuracy: 55.70%\n",
            "Epoch [2/10], Training Loss: 1.216, Validation Accuracy: 55.98%\n",
            "Epoch [3/10], Training Loss: 1.202, Validation Accuracy: 56.00%\n",
            "Epoch [4/10], Training Loss: 1.197, Validation Accuracy: 55.87%\n",
            "Epoch [5/10], Training Loss: 1.175, Validation Accuracy: 56.25%\n",
            "Epoch [6/10], Training Loss: 1.173, Validation Accuracy: 55.67%\n",
            "Epoch [7/10], Training Loss: 1.162, Validation Accuracy: 55.87%\n",
            "Epoch [8/10], Training Loss: 1.149, Validation Accuracy: 55.77%\n",
            "Epoch [9/10], Training Loss: 1.138, Validation Accuracy: 55.04%\n",
            "Epoch [10/10], Training Loss: 1.134, Validation Accuracy: 56.10%\n",
            "Epoch [1/10], Training Loss: 1.200, Validation Accuracy: 56.12%\n",
            "Epoch [2/10], Training Loss: 1.169, Validation Accuracy: 56.62%\n",
            "Epoch [3/10], Training Loss: 1.160, Validation Accuracy: 55.96%\n",
            "Epoch [4/10], Training Loss: 1.144, Validation Accuracy: 57.23%\n",
            "Epoch [5/10], Training Loss: 1.133, Validation Accuracy: 56.58%\n",
            "Epoch [6/10], Training Loss: 1.120, Validation Accuracy: 56.76%\n",
            "Epoch [7/10], Training Loss: 1.106, Validation Accuracy: 56.32%\n",
            "Epoch [8/10], Training Loss: 1.105, Validation Accuracy: 56.49%\n",
            "Epoch [9/10], Training Loss: 1.094, Validation Accuracy: 57.53%\n",
            "Epoch [10/10], Training Loss: 1.077, Validation Accuracy: 56.98%\n",
            "Epoch [1/10], Training Loss: 1.209, Validation Accuracy: 56.81%\n",
            "Epoch [2/10], Training Loss: 1.184, Validation Accuracy: 56.94%\n",
            "Epoch [3/10], Training Loss: 1.163, Validation Accuracy: 56.16%\n",
            "Epoch [4/10], Training Loss: 1.148, Validation Accuracy: 57.16%\n",
            "Epoch [5/10], Training Loss: 1.136, Validation Accuracy: 57.42%\n",
            "Epoch [6/10], Training Loss: 1.119, Validation Accuracy: 56.96%\n",
            "Epoch [7/10], Training Loss: 1.114, Validation Accuracy: 56.93%\n",
            "Epoch [8/10], Training Loss: 1.094, Validation Accuracy: 56.88%\n",
            "Epoch [9/10], Training Loss: 1.094, Validation Accuracy: 56.98%\n",
            "Epoch [10/10], Training Loss: 1.082, Validation Accuracy: 57.45%\n",
            "Epoch [1/10], Training Loss: 1.180, Validation Accuracy: 57.76%\n",
            "Epoch [2/10], Training Loss: 1.147, Validation Accuracy: 57.32%\n",
            "Epoch [3/10], Training Loss: 1.126, Validation Accuracy: 58.19%\n",
            "Epoch [4/10], Training Loss: 1.111, Validation Accuracy: 58.32%\n",
            "Epoch [5/10], Training Loss: 1.092, Validation Accuracy: 57.53%\n",
            "Epoch [6/10], Training Loss: 1.089, Validation Accuracy: 58.00%\n",
            "Epoch [7/10], Training Loss: 1.072, Validation Accuracy: 57.80%\n",
            "Epoch [8/10], Training Loss: 1.059, Validation Accuracy: 58.22%\n",
            "Epoch [9/10], Training Loss: 1.055, Validation Accuracy: 57.91%\n",
            "Epoch [10/10], Training Loss: 1.038, Validation Accuracy: 58.12%\n",
            "Epoch [1/10], Training Loss: 1.160, Validation Accuracy: 57.92%\n",
            "Epoch [2/10], Training Loss: 1.129, Validation Accuracy: 57.50%\n",
            "Epoch [3/10], Training Loss: 1.100, Validation Accuracy: 58.75%\n",
            "Epoch [4/10], Training Loss: 1.087, Validation Accuracy: 58.43%\n",
            "Epoch [5/10], Training Loss: 1.080, Validation Accuracy: 57.81%\n",
            "Epoch [6/10], Training Loss: 1.061, Validation Accuracy: 57.67%\n",
            "Epoch [7/10], Training Loss: 1.054, Validation Accuracy: 58.70%\n",
            "Epoch [8/10], Training Loss: 1.029, Validation Accuracy: 58.21%\n",
            "Epoch [9/10], Training Loss: 1.021, Validation Accuracy: 57.69%\n",
            "Epoch [10/10], Training Loss: 1.017, Validation Accuracy: 58.22%\n",
            "Epoch [1/10], Training Loss: 1.145, Validation Accuracy: 57.54%\n",
            "Epoch [2/10], Training Loss: 1.106, Validation Accuracy: 58.76%\n",
            "Epoch [3/10], Training Loss: 1.082, Validation Accuracy: 58.66%\n",
            "Epoch [4/10], Training Loss: 1.068, Validation Accuracy: 58.71%\n",
            "Epoch [5/10], Training Loss: 1.054, Validation Accuracy: 59.01%\n",
            "Epoch [6/10], Training Loss: 1.053, Validation Accuracy: 59.05%\n",
            "Epoch [7/10], Training Loss: 1.027, Validation Accuracy: 58.63%\n",
            "Epoch [8/10], Training Loss: 1.009, Validation Accuracy: 58.84%\n",
            "Epoch [9/10], Training Loss: 1.003, Validation Accuracy: 58.51%\n",
            "Epoch [10/10], Training Loss: 0.996, Validation Accuracy: 58.81%\n",
            "Epoch [1/10], Training Loss: 1.096, Validation Accuracy: 59.40%\n",
            "Epoch [2/10], Training Loss: 1.067, Validation Accuracy: 58.72%\n",
            "Epoch [3/10], Training Loss: 1.042, Validation Accuracy: 59.60%\n",
            "Epoch [4/10], Training Loss: 1.029, Validation Accuracy: 59.05%\n",
            "Epoch [5/10], Training Loss: 1.013, Validation Accuracy: 59.32%\n",
            "Epoch [6/10], Training Loss: 1.005, Validation Accuracy: 58.70%\n",
            "Epoch [7/10], Training Loss: 0.983, Validation Accuracy: 59.19%\n",
            "Epoch [8/10], Training Loss: 0.981, Validation Accuracy: 58.39%\n",
            "Epoch [9/10], Training Loss: 0.966, Validation Accuracy: 59.41%\n",
            "Epoch [10/10], Training Loss: 0.956, Validation Accuracy: 58.75%\n",
            "Epoch [1/10], Training Loss: 1.126, Validation Accuracy: 59.59%\n",
            "Epoch [2/10], Training Loss: 1.083, Validation Accuracy: 59.98%\n",
            "Epoch [3/10], Training Loss: 1.052, Validation Accuracy: 58.78%\n",
            "Epoch [4/10], Training Loss: 1.041, Validation Accuracy: 59.01%\n",
            "Epoch [5/10], Training Loss: 1.021, Validation Accuracy: 58.32%\n",
            "Epoch [6/10], Training Loss: 1.014, Validation Accuracy: 59.99%\n",
            "Epoch [7/10], Training Loss: 0.994, Validation Accuracy: 59.31%\n",
            "Epoch [8/10], Training Loss: 0.985, Validation Accuracy: 59.28%\n",
            "Epoch [9/10], Training Loss: 0.982, Validation Accuracy: 59.25%\n",
            "Epoch [10/10], Training Loss: 0.959, Validation Accuracy: 59.71%\n",
            "Epoch [1/10], Training Loss: 1.097, Validation Accuracy: 60.32%\n",
            "Epoch [2/10], Training Loss: 1.049, Validation Accuracy: 59.79%\n",
            "Epoch [3/10], Training Loss: 1.031, Validation Accuracy: 60.07%\n",
            "Epoch [4/10], Training Loss: 1.006, Validation Accuracy: 59.95%\n",
            "Epoch [5/10], Training Loss: 0.986, Validation Accuracy: 60.09%\n",
            "Epoch [6/10], Training Loss: 0.974, Validation Accuracy: 59.99%\n",
            "Epoch [7/10], Training Loss: 0.958, Validation Accuracy: 59.57%\n",
            "Epoch [8/10], Training Loss: 0.944, Validation Accuracy: 60.27%\n",
            "Epoch [9/10], Training Loss: 0.932, Validation Accuracy: 60.01%\n",
            "Epoch [10/10], Training Loss: 0.929, Validation Accuracy: 59.89%\n",
            "Epoch [1/10], Training Loss: 1.069, Validation Accuracy: 60.59%\n",
            "Epoch [2/10], Training Loss: 1.024, Validation Accuracy: 60.32%\n",
            "Epoch [3/10], Training Loss: 1.010, Validation Accuracy: 60.08%\n",
            "Epoch [4/10], Training Loss: 0.981, Validation Accuracy: 60.32%\n",
            "Epoch [5/10], Training Loss: 0.971, Validation Accuracy: 60.51%\n",
            "Epoch [6/10], Training Loss: 0.954, Validation Accuracy: 60.61%\n",
            "Epoch [7/10], Training Loss: 0.939, Validation Accuracy: 60.16%\n",
            "Epoch [8/10], Training Loss: 0.925, Validation Accuracy: 60.54%\n",
            "Epoch [9/10], Training Loss: 0.915, Validation Accuracy: 60.11%\n",
            "Epoch [10/10], Training Loss: 0.904, Validation Accuracy: 60.47%\n",
            "Epoch [1/10], Training Loss: 1.062, Validation Accuracy: 59.69%\n",
            "Epoch [2/10], Training Loss: 1.023, Validation Accuracy: 60.48%\n",
            "Epoch [3/10], Training Loss: 0.991, Validation Accuracy: 59.51%\n",
            "Epoch [4/10], Training Loss: 0.969, Validation Accuracy: 61.22%\n",
            "Epoch [5/10], Training Loss: 0.954, Validation Accuracy: 59.08%\n",
            "Epoch [6/10], Training Loss: 0.938, Validation Accuracy: 59.92%\n",
            "Epoch [7/10], Training Loss: 0.930, Validation Accuracy: 59.29%\n",
            "Epoch [8/10], Training Loss: 0.919, Validation Accuracy: 58.62%\n",
            "Epoch [9/10], Training Loss: 0.904, Validation Accuracy: 60.53%\n",
            "Epoch [10/10], Training Loss: 0.889, Validation Accuracy: 60.30%\n",
            "Epoch [1/10], Training Loss: 1.034, Validation Accuracy: 60.22%\n",
            "Epoch [2/10], Training Loss: 0.978, Validation Accuracy: 60.66%\n",
            "Epoch [3/10], Training Loss: 0.959, Validation Accuracy: 60.62%\n",
            "Epoch [4/10], Training Loss: 0.930, Validation Accuracy: 60.80%\n",
            "Epoch [5/10], Training Loss: 0.915, Validation Accuracy: 60.70%\n",
            "Epoch [6/10], Training Loss: 0.896, Validation Accuracy: 60.90%\n",
            "Epoch [7/10], Training Loss: 0.882, Validation Accuracy: 60.89%\n",
            "Epoch [8/10], Training Loss: 0.870, Validation Accuracy: 60.44%\n",
            "Epoch [9/10], Training Loss: 0.852, Validation Accuracy: 60.82%\n",
            "Epoch [10/10], Training Loss: 0.839, Validation Accuracy: 60.64%\n",
            "Epoch [1/10], Training Loss: 1.047, Validation Accuracy: 60.59%\n",
            "Epoch [2/10], Training Loss: 0.999, Validation Accuracy: 60.91%\n",
            "Epoch [3/10], Training Loss: 0.975, Validation Accuracy: 60.96%\n",
            "Epoch [4/10], Training Loss: 0.945, Validation Accuracy: 61.18%\n",
            "Epoch [5/10], Training Loss: 0.928, Validation Accuracy: 61.17%\n",
            "Epoch [6/10], Training Loss: 0.904, Validation Accuracy: 61.02%\n",
            "Epoch [7/10], Training Loss: 0.898, Validation Accuracy: 61.12%\n",
            "Epoch [8/10], Training Loss: 0.880, Validation Accuracy: 61.46%\n",
            "Epoch [9/10], Training Loss: 0.867, Validation Accuracy: 60.46%\n",
            "Epoch [10/10], Training Loss: 0.849, Validation Accuracy: 60.65%\n",
            "Epoch [1/10], Training Loss: 1.021, Validation Accuracy: 61.17%\n",
            "Epoch [2/10], Training Loss: 0.968, Validation Accuracy: 60.27%\n",
            "Epoch [3/10], Training Loss: 0.935, Validation Accuracy: 61.57%\n",
            "Epoch [4/10], Training Loss: 0.912, Validation Accuracy: 61.20%\n",
            "Epoch [5/10], Training Loss: 0.887, Validation Accuracy: 61.53%\n",
            "Epoch [6/10], Training Loss: 0.872, Validation Accuracy: 60.41%\n",
            "Epoch [7/10], Training Loss: 0.856, Validation Accuracy: 60.98%\n",
            "Epoch [8/10], Training Loss: 0.847, Validation Accuracy: 60.61%\n",
            "Epoch [9/10], Training Loss: 0.829, Validation Accuracy: 61.02%\n",
            "Epoch [10/10], Training Loss: 0.823, Validation Accuracy: 60.57%\n",
            "Epoch [1/10], Training Loss: 1.005, Validation Accuracy: 61.05%\n",
            "Epoch [2/10], Training Loss: 0.951, Validation Accuracy: 61.67%\n",
            "Epoch [3/10], Training Loss: 0.913, Validation Accuracy: 61.55%\n",
            "Epoch [4/10], Training Loss: 0.899, Validation Accuracy: 61.86%\n",
            "Epoch [5/10], Training Loss: 0.878, Validation Accuracy: 60.68%\n",
            "Epoch [6/10], Training Loss: 0.854, Validation Accuracy: 61.42%\n",
            "Epoch [7/10], Training Loss: 0.839, Validation Accuracy: 61.61%\n",
            "Epoch [8/10], Training Loss: 0.819, Validation Accuracy: 61.29%\n",
            "Epoch [9/10], Training Loss: 0.812, Validation Accuracy: 60.46%\n",
            "Epoch [10/10], Training Loss: 0.807, Validation Accuracy: 61.11%\n",
            "Epoch [1/10], Training Loss: 1.001, Validation Accuracy: 61.24%\n",
            "Epoch [2/10], Training Loss: 0.943, Validation Accuracy: 61.66%\n",
            "Epoch [3/10], Training Loss: 0.914, Validation Accuracy: 61.73%\n",
            "Epoch [4/10], Training Loss: 0.882, Validation Accuracy: 61.19%\n",
            "Epoch [5/10], Training Loss: 0.859, Validation Accuracy: 61.16%\n",
            "Epoch [6/10], Training Loss: 0.845, Validation Accuracy: 61.15%\n",
            "Epoch [7/10], Training Loss: 0.831, Validation Accuracy: 61.37%\n",
            "Epoch [8/10], Training Loss: 0.805, Validation Accuracy: 61.22%\n",
            "Epoch [9/10], Training Loss: 0.797, Validation Accuracy: 61.23%\n",
            "Epoch [10/10], Training Loss: 0.786, Validation Accuracy: 61.41%\n",
            "Epoch [1/10], Training Loss: 0.964, Validation Accuracy: 61.24%\n",
            "Epoch [2/10], Training Loss: 0.908, Validation Accuracy: 60.32%\n",
            "Epoch [3/10], Training Loss: 0.870, Validation Accuracy: 61.02%\n",
            "Epoch [4/10], Training Loss: 0.849, Validation Accuracy: 61.54%\n",
            "Epoch [5/10], Training Loss: 0.818, Validation Accuracy: 61.28%\n",
            "Epoch [6/10], Training Loss: 0.803, Validation Accuracy: 61.71%\n",
            "Epoch [7/10], Training Loss: 0.791, Validation Accuracy: 61.64%\n",
            "Epoch [8/10], Training Loss: 0.763, Validation Accuracy: 61.10%\n",
            "Epoch [9/10], Training Loss: 0.749, Validation Accuracy: 61.13%\n",
            "Epoch [10/10], Training Loss: 0.734, Validation Accuracy: 60.53%\n",
            "Epoch [1/10], Training Loss: 1.001, Validation Accuracy: 61.69%\n",
            "Epoch [2/10], Training Loss: 0.931, Validation Accuracy: 61.80%\n",
            "Epoch [3/10], Training Loss: 0.888, Validation Accuracy: 61.22%\n",
            "Epoch [4/10], Training Loss: 0.863, Validation Accuracy: 61.78%\n",
            "Epoch [5/10], Training Loss: 0.837, Validation Accuracy: 61.23%\n",
            "Epoch [6/10], Training Loss: 0.826, Validation Accuracy: 61.08%\n",
            "Epoch [7/10], Training Loss: 0.800, Validation Accuracy: 61.40%\n",
            "Epoch [8/10], Training Loss: 0.786, Validation Accuracy: 61.51%\n",
            "Epoch [9/10], Training Loss: 0.764, Validation Accuracy: 61.22%\n",
            "Epoch [10/10], Training Loss: 0.753, Validation Accuracy: 61.52%\n",
            "Epoch [1/10], Training Loss: 0.963, Validation Accuracy: 61.64%\n",
            "Epoch [2/10], Training Loss: 0.896, Validation Accuracy: 60.43%\n",
            "Epoch [3/10], Training Loss: 0.876, Validation Accuracy: 61.03%\n",
            "Epoch [4/10], Training Loss: 0.824, Validation Accuracy: 62.11%\n",
            "Epoch [5/10], Training Loss: 0.795, Validation Accuracy: 61.83%\n",
            "Epoch [6/10], Training Loss: 0.787, Validation Accuracy: 61.92%\n",
            "Epoch [7/10], Training Loss: 0.755, Validation Accuracy: 62.16%\n",
            "Epoch [8/10], Training Loss: 0.737, Validation Accuracy: 61.22%\n",
            "Epoch [9/10], Training Loss: 0.727, Validation Accuracy: 60.90%\n",
            "Epoch [10/10], Training Loss: 0.710, Validation Accuracy: 60.92%\n",
            "Epoch [1/10], Training Loss: 0.956, Validation Accuracy: 62.07%\n",
            "Epoch [2/10], Training Loss: 0.884, Validation Accuracy: 62.32%\n",
            "Epoch [3/10], Training Loss: 0.847, Validation Accuracy: 62.12%\n",
            "Epoch [4/10], Training Loss: 0.812, Validation Accuracy: 62.59%\n",
            "Epoch [5/10], Training Loss: 0.783, Validation Accuracy: 61.80%\n",
            "Epoch [6/10], Training Loss: 0.775, Validation Accuracy: 62.00%\n",
            "Epoch [7/10], Training Loss: 0.747, Validation Accuracy: 61.83%\n",
            "Epoch [8/10], Training Loss: 0.734, Validation Accuracy: 61.44%\n",
            "Epoch [9/10], Training Loss: 0.711, Validation Accuracy: 61.85%\n",
            "Epoch [10/10], Training Loss: 0.698, Validation Accuracy: 61.48%\n",
            "Epoch [1/10], Training Loss: 0.942, Validation Accuracy: 62.02%\n",
            "Epoch [2/10], Training Loss: 0.867, Validation Accuracy: 61.98%\n",
            "Epoch [3/10], Training Loss: 0.829, Validation Accuracy: 61.13%\n",
            "Epoch [4/10], Training Loss: 0.796, Validation Accuracy: 62.26%\n",
            "Epoch [5/10], Training Loss: 0.777, Validation Accuracy: 62.42%\n",
            "Epoch [6/10], Training Loss: 0.748, Validation Accuracy: 61.48%\n",
            "Epoch [7/10], Training Loss: 0.730, Validation Accuracy: 61.92%\n",
            "Epoch [8/10], Training Loss: 0.713, Validation Accuracy: 61.39%\n",
            "Epoch [9/10], Training Loss: 0.697, Validation Accuracy: 61.08%\n",
            "Epoch [10/10], Training Loss: 0.682, Validation Accuracy: 61.25%\n",
            "Epoch [1/10], Training Loss: 0.912, Validation Accuracy: 60.13%\n",
            "Epoch [2/10], Training Loss: 0.843, Validation Accuracy: 61.76%\n",
            "Epoch [3/10], Training Loss: 0.792, Validation Accuracy: 62.01%\n",
            "Epoch [4/10], Training Loss: 0.761, Validation Accuracy: 61.58%\n",
            "Epoch [5/10], Training Loss: 0.729, Validation Accuracy: 61.77%\n",
            "Epoch [6/10], Training Loss: 0.709, Validation Accuracy: 61.73%\n",
            "Epoch [7/10], Training Loss: 0.694, Validation Accuracy: 61.37%\n",
            "Epoch [8/10], Training Loss: 0.672, Validation Accuracy: 61.72%\n",
            "Epoch [9/10], Training Loss: 0.651, Validation Accuracy: 61.29%\n",
            "Epoch [10/10], Training Loss: 0.635, Validation Accuracy: 61.50%\n",
            "Epoch [1/10], Training Loss: 0.939, Validation Accuracy: 61.32%\n",
            "Epoch [2/10], Training Loss: 0.867, Validation Accuracy: 61.27%\n",
            "Epoch [3/10], Training Loss: 0.818, Validation Accuracy: 61.63%\n",
            "Epoch [4/10], Training Loss: 0.786, Validation Accuracy: 61.78%\n",
            "Epoch [5/10], Training Loss: 0.751, Validation Accuracy: 61.67%\n",
            "Epoch [6/10], Training Loss: 0.725, Validation Accuracy: 62.05%\n",
            "Epoch [7/10], Training Loss: 0.710, Validation Accuracy: 61.63%\n",
            "Epoch [8/10], Training Loss: 0.688, Validation Accuracy: 61.70%\n",
            "Epoch [9/10], Training Loss: 0.667, Validation Accuracy: 61.35%\n",
            "Epoch [10/10], Training Loss: 0.653, Validation Accuracy: 61.29%\n",
            "Epoch [1/10], Training Loss: 0.895, Validation Accuracy: 61.23%\n",
            "Epoch [2/10], Training Loss: 0.822, Validation Accuracy: 61.27%\n",
            "Epoch [3/10], Training Loss: 0.779, Validation Accuracy: 61.29%\n",
            "Epoch [4/10], Training Loss: 0.751, Validation Accuracy: 61.65%\n",
            "Epoch [5/10], Training Loss: 0.710, Validation Accuracy: 61.98%\n",
            "Epoch [6/10], Training Loss: 0.684, Validation Accuracy: 61.91%\n",
            "Epoch [7/10], Training Loss: 0.665, Validation Accuracy: 61.17%\n",
            "Epoch [8/10], Training Loss: 0.649, Validation Accuracy: 61.61%\n",
            "Epoch [9/10], Training Loss: 0.635, Validation Accuracy: 61.29%\n",
            "Epoch [10/10], Training Loss: 0.609, Validation Accuracy: 60.85%\n",
            "Epoch [1/10], Training Loss: 0.902, Validation Accuracy: 61.53%\n",
            "Epoch [2/10], Training Loss: 0.819, Validation Accuracy: 61.46%\n",
            "Epoch [3/10], Training Loss: 0.778, Validation Accuracy: 62.18%\n",
            "Epoch [4/10], Training Loss: 0.739, Validation Accuracy: 62.61%\n",
            "Epoch [5/10], Training Loss: 0.702, Validation Accuracy: 62.48%\n",
            "Epoch [6/10], Training Loss: 0.679, Validation Accuracy: 62.28%\n",
            "Epoch [7/10], Training Loss: 0.660, Validation Accuracy: 61.97%\n",
            "Epoch [8/10], Training Loss: 0.634, Validation Accuracy: 61.66%\n",
            "Epoch [9/10], Training Loss: 0.612, Validation Accuracy: 62.05%\n",
            "Epoch [10/10], Training Loss: 0.608, Validation Accuracy: 61.98%\n",
            "Epoch [1/10], Training Loss: 0.902, Validation Accuracy: 61.89%\n",
            "Epoch [2/10], Training Loss: 0.816, Validation Accuracy: 61.94%\n",
            "Epoch [3/10], Training Loss: 0.757, Validation Accuracy: 61.62%\n",
            "Epoch [4/10], Training Loss: 0.719, Validation Accuracy: 62.28%\n",
            "Epoch [5/10], Training Loss: 0.691, Validation Accuracy: 61.51%\n",
            "Epoch [6/10], Training Loss: 0.661, Validation Accuracy: 61.57%\n",
            "Epoch [7/10], Training Loss: 0.640, Validation Accuracy: 61.87%\n",
            "Epoch [8/10], Training Loss: 0.620, Validation Accuracy: 61.49%\n",
            "Epoch [9/10], Training Loss: 0.599, Validation Accuracy: 61.67%\n",
            "Epoch [10/10], Training Loss: 0.591, Validation Accuracy: 61.14%\n",
            "Epoch [1/10], Training Loss: 0.875, Validation Accuracy: 61.66%\n",
            "Epoch [2/10], Training Loss: 0.773, Validation Accuracy: 61.76%\n",
            "Epoch [3/10], Training Loss: 0.714, Validation Accuracy: 61.74%\n",
            "Epoch [4/10], Training Loss: 0.689, Validation Accuracy: 61.75%\n",
            "Epoch [5/10], Training Loss: 0.656, Validation Accuracy: 62.06%\n",
            "Epoch [6/10], Training Loss: 0.633, Validation Accuracy: 62.02%\n",
            "Epoch [7/10], Training Loss: 0.597, Validation Accuracy: 61.71%\n",
            "Epoch [8/10], Training Loss: 0.575, Validation Accuracy: 60.86%\n",
            "Epoch [9/10], Training Loss: 0.557, Validation Accuracy: 61.51%\n",
            "Epoch [10/10], Training Loss: 0.533, Validation Accuracy: 61.39%\n",
            "Epoch [1/10], Training Loss: 0.907, Validation Accuracy: 59.82%\n",
            "Epoch [2/10], Training Loss: 0.806, Validation Accuracy: 61.54%\n",
            "Epoch [3/10], Training Loss: 0.743, Validation Accuracy: 61.09%\n",
            "Epoch [4/10], Training Loss: 0.711, Validation Accuracy: 61.74%\n",
            "Epoch [5/10], Training Loss: 0.670, Validation Accuracy: 61.69%\n",
            "Epoch [6/10], Training Loss: 0.643, Validation Accuracy: 61.20%\n",
            "Epoch [7/10], Training Loss: 0.623, Validation Accuracy: 61.92%\n",
            "Epoch [8/10], Training Loss: 0.599, Validation Accuracy: 60.79%\n",
            "Epoch [9/10], Training Loss: 0.578, Validation Accuracy: 61.61%\n",
            "Epoch [10/10], Training Loss: 0.560, Validation Accuracy: 61.00%\n",
            "Epoch [1/10], Training Loss: 0.874, Validation Accuracy: 60.88%\n",
            "Epoch [2/10], Training Loss: 0.768, Validation Accuracy: 61.33%\n",
            "Epoch [3/10], Training Loss: 0.720, Validation Accuracy: 61.46%\n",
            "Epoch [4/10], Training Loss: 0.664, Validation Accuracy: 61.42%\n",
            "Epoch [5/10], Training Loss: 0.633, Validation Accuracy: 61.67%\n",
            "Epoch [6/10], Training Loss: 0.613, Validation Accuracy: 61.68%\n",
            "Epoch [7/10], Training Loss: 0.576, Validation Accuracy: 61.51%\n",
            "Epoch [8/10], Training Loss: 0.559, Validation Accuracy: 61.12%\n",
            "Epoch [9/10], Training Loss: 0.535, Validation Accuracy: 61.05%\n",
            "Epoch [10/10], Training Loss: 0.526, Validation Accuracy: 61.31%\n",
            "Epoch [1/10], Training Loss: 0.864, Validation Accuracy: 61.33%\n",
            "Epoch [2/10], Training Loss: 0.755, Validation Accuracy: 61.71%\n",
            "Epoch [3/10], Training Loss: 0.689, Validation Accuracy: 62.18%\n",
            "Epoch [4/10], Training Loss: 0.651, Validation Accuracy: 61.48%\n",
            "Epoch [5/10], Training Loss: 0.619, Validation Accuracy: 62.29%\n",
            "Epoch [6/10], Training Loss: 0.595, Validation Accuracy: 61.52%\n",
            "Epoch [7/10], Training Loss: 0.563, Validation Accuracy: 60.91%\n",
            "Epoch [8/10], Training Loss: 0.540, Validation Accuracy: 61.88%\n",
            "Epoch [9/10], Training Loss: 0.518, Validation Accuracy: 61.71%\n",
            "Epoch [10/10], Training Loss: 0.506, Validation Accuracy: 61.35%\n",
            "Epoch [1/10], Training Loss: 0.865, Validation Accuracy: 61.83%\n",
            "Epoch [2/10], Training Loss: 0.745, Validation Accuracy: 61.94%\n",
            "Epoch [3/10], Training Loss: 0.687, Validation Accuracy: 62.21%\n",
            "Epoch [4/10], Training Loss: 0.647, Validation Accuracy: 61.83%\n",
            "Epoch [5/10], Training Loss: 0.609, Validation Accuracy: 62.76%\n",
            "Epoch [6/10], Training Loss: 0.580, Validation Accuracy: 61.83%\n",
            "Epoch [7/10], Training Loss: 0.562, Validation Accuracy: 61.82%\n",
            "Epoch [8/10], Training Loss: 0.536, Validation Accuracy: 61.38%\n",
            "Epoch [9/10], Training Loss: 0.513, Validation Accuracy: 61.62%\n",
            "Epoch [10/10], Training Loss: 0.491, Validation Accuracy: 61.45%\n",
            "Epoch [1/10], Training Loss: 0.834, Validation Accuracy: 61.13%\n",
            "Epoch [2/10], Training Loss: 0.704, Validation Accuracy: 61.95%\n",
            "Epoch [3/10], Training Loss: 0.645, Validation Accuracy: 61.69%\n",
            "Epoch [4/10], Training Loss: 0.611, Validation Accuracy: 61.48%\n",
            "Epoch [5/10], Training Loss: 0.571, Validation Accuracy: 61.74%\n",
            "Epoch [6/10], Training Loss: 0.537, Validation Accuracy: 61.16%\n",
            "Epoch [7/10], Training Loss: 0.507, Validation Accuracy: 61.65%\n",
            "Epoch [8/10], Training Loss: 0.491, Validation Accuracy: 61.74%\n",
            "Epoch [9/10], Training Loss: 0.463, Validation Accuracy: 61.22%\n",
            "Epoch [10/10], Training Loss: 0.445, Validation Accuracy: 61.74%\n",
            "Epoch [1/10], Training Loss: 0.859, Validation Accuracy: 61.44%\n",
            "Epoch [2/10], Training Loss: 0.733, Validation Accuracy: 61.56%\n",
            "Epoch [3/10], Training Loss: 0.660, Validation Accuracy: 61.14%\n",
            "Epoch [4/10], Training Loss: 0.629, Validation Accuracy: 61.23%\n",
            "Epoch [5/10], Training Loss: 0.597, Validation Accuracy: 61.39%\n",
            "Epoch [6/10], Training Loss: 0.558, Validation Accuracy: 61.47%\n",
            "Epoch [7/10], Training Loss: 0.524, Validation Accuracy: 61.49%\n",
            "Epoch [8/10], Training Loss: 0.511, Validation Accuracy: 60.82%\n",
            "Epoch [9/10], Training Loss: 0.481, Validation Accuracy: 60.83%\n",
            "Epoch [10/10], Training Loss: 0.474, Validation Accuracy: 60.92%\n",
            "Epoch [1/10], Training Loss: 0.841, Validation Accuracy: 61.18%\n",
            "Epoch [2/10], Training Loss: 0.705, Validation Accuracy: 60.84%\n",
            "Epoch [3/10], Training Loss: 0.639, Validation Accuracy: 61.46%\n",
            "Epoch [4/10], Training Loss: 0.593, Validation Accuracy: 61.70%\n",
            "Epoch [5/10], Training Loss: 0.552, Validation Accuracy: 60.99%\n",
            "Epoch [6/10], Training Loss: 0.522, Validation Accuracy: 61.36%\n",
            "Epoch [7/10], Training Loss: 0.495, Validation Accuracy: 61.64%\n",
            "Epoch [8/10], Training Loss: 0.470, Validation Accuracy: 61.35%\n",
            "Epoch [9/10], Training Loss: 0.452, Validation Accuracy: 60.77%\n",
            "Epoch [10/10], Training Loss: 0.434, Validation Accuracy: 61.38%\n",
            "Epoch [1/10], Training Loss: 0.829, Validation Accuracy: 60.76%\n",
            "Epoch [2/10], Training Loss: 0.697, Validation Accuracy: 61.86%\n",
            "Epoch [3/10], Training Loss: 0.620, Validation Accuracy: 60.47%\n",
            "Epoch [4/10], Training Loss: 0.579, Validation Accuracy: 60.61%\n",
            "Epoch [5/10], Training Loss: 0.534, Validation Accuracy: 61.23%\n",
            "Epoch [6/10], Training Loss: 0.512, Validation Accuracy: 61.50%\n",
            "Epoch [7/10], Training Loss: 0.480, Validation Accuracy: 61.52%\n",
            "Epoch [8/10], Training Loss: 0.455, Validation Accuracy: 61.18%\n",
            "Epoch [9/10], Training Loss: 0.435, Validation Accuracy: 60.64%\n",
            "Epoch [10/10], Training Loss: 0.418, Validation Accuracy: 61.62%\n",
            "Epoch [1/10], Training Loss: 0.846, Validation Accuracy: 61.45%\n",
            "Epoch [2/10], Training Loss: 0.701, Validation Accuracy: 61.39%\n",
            "Epoch [3/10], Training Loss: 0.617, Validation Accuracy: 61.25%\n",
            "Epoch [4/10], Training Loss: 0.578, Validation Accuracy: 61.05%\n",
            "Epoch [5/10], Training Loss: 0.534, Validation Accuracy: 61.97%\n",
            "Epoch [6/10], Training Loss: 0.500, Validation Accuracy: 61.27%\n",
            "Epoch [7/10], Training Loss: 0.483, Validation Accuracy: 61.37%\n",
            "Epoch [8/10], Training Loss: 0.456, Validation Accuracy: 61.22%\n",
            "Epoch [9/10], Training Loss: 0.430, Validation Accuracy: 61.40%\n",
            "Epoch [10/10], Training Loss: 0.409, Validation Accuracy: 61.34%\n",
            "Confusion Matrix:\n",
            "[[647  37  51  23  33  14  17  17 106  55]\n",
            " [ 31 726   7  11   6  13  11   8  43 144]\n",
            " [ 68  14 448  97 131 108  61  34  24  15]\n",
            " [ 33  13  68 386  85 232  75  43  27  38]\n",
            " [ 34  12  71  66 565  72  58  94  12  16]\n",
            " [ 23   9  47 182  71 542  40  58  13  15]\n",
            " [  7  14  56  81  82  46 683  14   8   9]\n",
            " [ 16   9  35  44  75  92  16 674   8  31]\n",
            " [ 82  59  18   9   9  16  10   5 745  47]\n",
            " [ 38 108  10  29  17  17   9  25  45 702]]\n",
            "Test Accuracy: 61.18%\n",
            "True Positives (TP): [647 726 448 386 565 542 683 674 745 702]\n",
            "False Positives (FP): [332 275 363 542 509 610 297 298 286 370]\n",
            "True Negatives (TN): [8668 8725 8637 8458 8491 8390 8703 8702 8714 8630]\n",
            "False Negatives (FN): [353 274 552 614 435 458 317 326 255 298]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.66087845 0.72527473 0.55240444 0.41594828 0.52607076 0.47048611\n",
            " 0.69693878 0.69341564 0.72259942 0.65485075]\n",
            "Recall: [0.647 0.726 0.448 0.386 0.565 0.542 0.683 0.674 0.745 0.702]\n",
            "F1 Score: [0.65386559 0.72563718 0.49475428 0.40041494 0.54484089 0.50371747\n",
            " 0.68989899 0.68356998 0.73362875 0.67760618]\n",
            "CPU times: user 3h 35min 8s, sys: 1min 25s, total: 3h 36min 34s\n",
            "Wall time: 3h 53min 40s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 1):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=1):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_normal: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = ( augmented_data_normal + augmented_data_truncated) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=1)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"normal\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4XCk9OeiUWK"
      },
      "source": [
        "Beta=2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XYytIYhiWPf",
        "outputId": "3734d163-fc04-42c3-ca52-294e8b50baaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 37.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Images per Class: [6070 6042 5865 5890 6177 6038 5914 6044 5905 6055]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<timed exec>:281: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Training Loss: 2.305, Validation Accuracy: 10.26%\n",
            "Epoch [2/10], Training Loss: 2.304, Validation Accuracy: 10.25%\n",
            "Epoch [3/10], Training Loss: 2.303, Validation Accuracy: 10.25%\n",
            "Epoch [4/10], Training Loss: 2.302, Validation Accuracy: 10.23%\n",
            "Epoch [5/10], Training Loss: 2.301, Validation Accuracy: 10.24%\n",
            "Epoch [6/10], Training Loss: 2.300, Validation Accuracy: 10.23%\n",
            "Epoch [7/10], Training Loss: 2.299, Validation Accuracy: 10.66%\n",
            "Epoch [8/10], Training Loss: 2.298, Validation Accuracy: 11.77%\n",
            "Epoch [9/10], Training Loss: 2.297, Validation Accuracy: 13.73%\n",
            "Epoch [10/10], Training Loss: 2.295, Validation Accuracy: 15.61%\n",
            "Epoch [1/10], Training Loss: 2.294, Validation Accuracy: 17.64%\n",
            "Epoch [2/10], Training Loss: 2.292, Validation Accuracy: 19.96%\n",
            "Epoch [3/10], Training Loss: 2.288, Validation Accuracy: 20.70%\n",
            "Epoch [4/10], Training Loss: 2.283, Validation Accuracy: 21.61%\n",
            "Epoch [5/10], Training Loss: 2.275, Validation Accuracy: 21.98%\n",
            "Epoch [6/10], Training Loss: 2.263, Validation Accuracy: 21.83%\n",
            "Epoch [7/10], Training Loss: 2.243, Validation Accuracy: 21.83%\n",
            "Epoch [8/10], Training Loss: 2.215, Validation Accuracy: 21.90%\n",
            "Epoch [9/10], Training Loss: 2.179, Validation Accuracy: 22.72%\n",
            "Epoch [10/10], Training Loss: 2.142, Validation Accuracy: 23.50%\n",
            "Epoch [1/10], Training Loss: 2.117, Validation Accuracy: 23.64%\n",
            "Epoch [2/10], Training Loss: 2.092, Validation Accuracy: 24.05%\n",
            "Epoch [3/10], Training Loss: 2.071, Validation Accuracy: 25.30%\n",
            "Epoch [4/10], Training Loss: 2.052, Validation Accuracy: 25.80%\n",
            "Epoch [5/10], Training Loss: 2.032, Validation Accuracy: 26.63%\n",
            "Epoch [6/10], Training Loss: 2.012, Validation Accuracy: 27.24%\n",
            "Epoch [7/10], Training Loss: 1.991, Validation Accuracy: 28.00%\n",
            "Epoch [8/10], Training Loss: 1.967, Validation Accuracy: 29.12%\n",
            "Epoch [9/10], Training Loss: 1.946, Validation Accuracy: 30.03%\n",
            "Epoch [10/10], Training Loss: 1.927, Validation Accuracy: 30.80%\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int, beta: float = 2):\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar , beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=2):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_normal: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = ( augmented_data_normal + augmented_data_truncated) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "           # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10, beta=2)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"normal\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNf7x1H4LDuO0I0o77j3hu0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}